
PCIe SSD 最早是 Fusion-IO 推出来的, 以**闪存卡**的形式被互联网公司和数据中心广泛使用. 闪存卡一般作为**数据缓存**来使用, 如果要在服务器中集成更多的 PCIe SSD, 闪存卡的形式就有局限了. 闪存卡有以下缺点:

* 插在服务器主板的 **PCIe 插槽**上, **数量有限**.
* 通过 PCIe 插槽供电, **单卡容量受到限制**.
* 在 PCIe 插槽上, 容易出现由于**散热不良**导致宕机的问题.
* 不能热插拔. 如果发现 PCIe 闪存卡有故障, 必须要停止服务, 关闭服务器, 打开机箱, 拔出闪存卡. 这对有成百上千台服务器的数据中心来说, 管理成本非常高.

所以, 如图 5-67 所示, PCIe SSD 推出了新的硬件形式 `SFF-8639`, 又称 `U.2`. `U.2 PCIe SSD`类似于传统的盘位式 SATA、SAS 硬盘, 可以直接从服务器前面板热插拔.

热插拔示意图(本图来源于 Oracle NVMe SSD 热插拔说明):



当服务器有很多个可以热插拔的 U.2 SSD 之后, 存储密度大为提升, 更重要的是, U.2 SSD 不只可以用作数据缓存, 关键数据也可以放在其中. 通过多个 U.2 SSD 组成 RAID 阵列, 当某个 U.2 SSD 故障之后, 可以通过前面板显示灯确定故障 SSD 盘位, 予以更换. 同时, 不会造成服务器停止服务或者数据丢失.

目前有很多服务器厂商都发布了有很多 U.2 SSD 盘位的服务器, 有的是少数 U.2SSD 和多数 SATA HDD 混合, 有的甚至是 24 个纯 U.2 SSD 盘位. 配备了高密度 SSD 的服务器对数据中心来说, 可以大幅减少传统服务器的数量, 因为很多企业应用对存储容量要求并不高, 传统机械硬盘阵列的容量很大, 却处于浪费状态. 企业对硬盘带宽的要求更高, 一台 SSD 阵列服务器能够支持的用户数是 HDD 阵列服务器的好几倍, 功耗和制冷成本却少了好几倍. 目前, 房租和土地成本越来越高, 能够在有限的数据中心空间中为大量用户提供服务, 对电信、视频网站、互联网公司等企业来说非常重要. 所以可以预期, 随着闪存价格的逐年下降, 配备 SSD 阵列的服务器会越来越普及.

我们来看看 PCIe SSD 热插拔的技术实现. 传统 SATA、SAS 硬盘通过 **HBA** 和主机通信, 所以也是通过 HBA 来管理热插拔. 但是, PCIe SSD 直接连到 CPU 的 **PCIe 控制器**, 热插拔需要**驱动直接管理**. 根据 Memblaze 公司公众号的介绍, 一般热插拔 PCIe SSD 需要几方面的支持:

* PCIe SSD: 一方面需要硬件支持, 避免 SSD 在插盘过程中产生电流波峰导致器件损坏; 另一方面, 控制器要能自动检测到拔盘操作, 避免数据因掉电而丢失.
* 服务器背板 PCIe SSD 插槽: 需要通过服务器厂家了解是否支持 U.2 SSD 热插拔.
* 操作系统: 要确定**热插拔**是**操作系统**还是 **BIOS** 处理的, 也需要咨询服务器主板厂家来确定.
* PCIe SSD 驱动: 不管是 Linux 内核自带的 NVMe 驱动, 还是厂家提供的驱动, 都需要在各种使用环境中做过大量热插拔稳定性测试, 避免在实际操作中因为驱动问题导致系统崩溃.

拔出 PCIe SSD 的基本流程如下:

1. 配置应用程序, 停止所有对目标 SSD 的访问. 如果某个程序打开了该 SSD 中的某个目录, 也需要退出;
2. umount 目标 SSD 上的所有文件系统;
3. 有些 SSD 厂家会要求卸载 SSD 驱动程序, 从系统中删除已注册的块设备和 disk;
4. 拔出 SSD.