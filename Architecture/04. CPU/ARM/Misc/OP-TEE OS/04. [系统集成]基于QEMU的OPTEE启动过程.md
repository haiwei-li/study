
基于 QEMU 的 OPTEE 启动过程

OPTEE 的整个工程编译出来的结果在 out 目录:

![](https://raw.githubusercontent.com/carloscn/images/main/typora20221003123730.png)

我们可以看到各个二进制文件都是谁生成的:

* linux & rootfs:

  * zImage

  * Image

  * rootfs

* ATF:

  * bl1.bin

  * bl2.bin

  * bl31.bin

* OPTEE:

  * bl32.bin (tee-header_v2.bin)

  * bl32_extra2.bin (tee-pageable_v2.bin)

  * bl32_extra1.bin (tee-pager_v2.bin)

对于这些启动文件的打包, 每一个芯片厂都采用了不同的方式:

# 1. bios 阶段

## QEMU 方式

我们从 OPTEE-OS 的 `run-only` 启动 QEMU 的过程就能看到, QEMU 运行 OPTEE 需要哪些依赖.

```makefile
run-only:
	ln -sf $(ROOT)/out-br/images/rootfs.cpio.gz $(BINARIES_PATH)/
	$(call check-terminal)
	$(call run-help)
	$(call launch-terminal,54320,"Normal World")
	$(call launch-terminal,54321,"Secure World")
	$(call wait-for-ports,54320,54321)
	cd $(BINARIES_PATH) && $(QEMU_BUILD)/aarch64-softmmu/qemu-system-aarch64 \
		-nographic \
		-serial tcp:localhost:54320 -serial tcp:localhost:54321 \
		-smp $(QEMU_SMP) \
		-s -S -machine virt,secure=on,mte=$(QEMU_MTE),gic-version=$(QEMU_GIC_VERSION),virtualization=$(QEMU_VIRT) \
		-cpu $(QEMU_CPU) \
		-d unimp -semihosting-config enable=on,target=native \
		-m $(QEMU_MEM) \
		-bios bl1.bin		\
		-initrd rootfs.cpio.gz \
		-kernel Image -no-acpi \
		-append 'console=ttyAMA0,38400 keep_bootcon root=/dev/vda2 $(QEMU_KERNEL_BOOTARGS)' \
		$(QEMU_XEN) \
		$(QEMU_EXTRA_ARGS)
```

我们可以根据 ATF 的启动流程分析. qemu 先加载 bl1.bin, 也就是根据 `-bios` 指定的参数, 启动过如图:

![](https://raw.githubusercontent.com/carloscn/images/main/typora20221003125326.png)

接着就开始启动 Linux 内核:

![](https://raw.githubusercontent.com/carloscn/images/main/typora20221003125558.png)

接着 Linux 内核挂载 rootfs.

## NXP 方式

例如 nxp 使用 mkimage 的方式把所有的 tee.bin, blx.bin, linux image 等等这些都包装成一个文件:

```Makefile
mkimage: u-boot tfa ddr-firmware
	ln -sf $(ROOT)/optee_os/out/arm/core/tee-raw.bin \
		$(MKIMAGE_PATH)/iMX8M/tee.bin
	ln -sf $(ROOT)/trusted-firmware-a/build/imx8mq/release/bl31.bin \
		$(MKIMAGE_PATH)/iMX8M/
	ln -sf $(LPDDR_BIN_PATH)/lpddr4_pmu_train_*.bin $(MKIMAGE_PATH)/iMX8M/
	ln -sf $(U-BOOT_PATH)/u-boot-nodtb.bin $(MKIMAGE_PATH)/iMX8M/
	ln -sf $(U-BOOT_PATH)/spl/u-boot-spl.bin $(MKIMAGE_PATH)/iMX8M/
	ln -sf $(U-BOOT_PATH)/arch/arm/dts/imx8mq-evk.dtb \
		$(MKIMAGE_PATH)/iMX8M/fsl-imx8mq-evk.dtb
	ln -sf $(U-BOOT_PATH)/tools/mkimage $(MKIMAGE_PATH)/iMX8M/mkimage_uboot
	$(MAKE) -C $(MKIMAGE_PATH) SOC=iMX8M flash_spl_uboot
```

还有在 ARMv7 的一些架构里面, 也是把这些二进制文件都打包成一个集合 bios.bin, 然后在入口函数中, 对这些二进制文件进行解析和重定位及跳转. 这部分处理是非常灵活的.

## ARMv7 方式

使用 QEMU 运行 OP-TEE 时首先加载的是编译生成的 bios.bin 镜像文件, 而 bios.bin 镜像文件的入口函数是在 `bios_qemu_tz_arm/bios/entry.S` 文件中定义的, 该文件的入口函数为 `_start`, 该文件的主要内容如下:

https://github.com/jenswi-linaro/bios_qemu_tz_arm

```assembly
#include <platform_config.h>

#include <asm.S>
#include <arm32.h>
#include <arm32_macros.S>

.section .text.boot
FUNC _start , :
	b	reset
	b	.	/* Undef */
	b	.	/* Syscall */
	b	.	/* Prefetch abort */
	b	.	/* Data abort */
	b	.	/* Reserved */
	b	.	/* IRQ */
	b	.	/* FIQ */
END_FUNC _start

/*
 * Binary is linked against BIOS_RAM_START, but starts to execute from
 * address 0. The branching etc before relocation works because the
 * assembly code is only using relative addressing.
 */
LOCAL_FUNC reset , :
	read_sctlr r0
	orr	r0, r0, #SCTLR_A
	write_sctlr r0

	/* Setup vector */
	adr	r0, _start
	write_vbar r0

	/* Relocate bios to RAM */
	mov	r0, #0
	ldr	r1, =__text_start
	ldr	r2, =__data_end
	sub	r2, r2, r1
	bl	copy_blob

	/* Jump to new location in RAM */
	ldr	ip, =new_loc
	bx	ip
new_loc:

	/* Setup vector again, now to the new location */
	adr	r0, _start
	write_vbar r0

	/* Zero bss */
	ldr	r0, =__bss_start
	ldr	r1, =__bss_end
	sub	r1, r1, r0
	bl	zero_mem

	/* Setup stack */
	ldr	ip, =main_stack_top;
	ldr	sp, [ip]

	push	{r0, r1, r2}
	mov	r0, sp
	ldr	ip, =main_init_sec
	blx	ip
	pop	{r0, r1, r2}
	mov	ip, r0	/* entry address */
	mov	r0, r1	/* argument (address of pagable part if != 0) */
	blx	ip

	/*
	 * Setup stack again as we're in non-secure mode now and have
	 * new registers.
	 */
	ldr	ip, =main_stack_top;
	ldr	sp, [ip]

	ldr	ip, =main_init_ns
	bx	ip
END_FUNC reset

LOCAL_FUNC copy_blob , :
	ldrb	r4, [r0], #1
	strb	r4, [r1], #1
	subs	r2, r2, #1
	bne	copy_blob
	bx	lr
END_FUNC copy_blob

LOCAL_FUNC zero_mem , :
	cmp	r1, #0
	bxeq	lr
	mov	r4, #0
	strb	r4, [r0], #1
	sub	r1, r1, #1
	b	zero_mem
END_FUNC zero_mem
```

`main_init_sec` 函数用来将 Linux 内核镜像, OP- TEE OS 镜像, rootfs 镜像文件加载到 RAM 的对应位 置, 并且解析出 OP-TEE OS 的入口地址, Linux 内核的加载地址, rootfs 在 RAM 中的地址和其他相关 信息. main_init_sec 函数执行完成后会返回 OP-TEE OS 的入口地址以及设备树 (device tree,DT) 的地 址, 然后在汇编代码中通过调用 blx 指令进入 OP- TEE OS 的启动.

OP-TEE 启动完成后会重新进入 entry.S 文件中继续执行, 最终执行 main_init_ns 函数来启动 Linux 内核, 在 `main_init_sec` 函数中会设定 Linux 内核的入口函数地址, DT 的相关信息, `main_init_ns` 函数会使用这些信息来开始 Linux 内核的加载.

上述两个函数都定义在 `bios_qemu_tz_arm/bios/main.c` 文件中. 将各种镜像 文件复制到 RAM 的操作都是通过解析 bios.bin 镜像的对应 section 来实现的, 通过寻找特定的 section 来确定各镜像文件在 bios.bin 文件中的位置.

启动过程中 entry.S 文件通过汇编调用 main_init_sec 函数将 optee-os 镜像, Linux 镜像和 rootfs 加载到 RAM 中, 并定位 DT 的地址信息, 以备 Linux 和 OP-TEE 启动使用, 这些操作是由 main_init_sec 函数进行的, 该函数定义在 bios_qemu_tz_arm/bios/main.c 文件中, 其内容如下:

```Cpp
void main_init_sec(struct sec_entry_arg *arg)
{
    void *fdt;
    int r;
    // 定义 OP-TEE OS 镜像文件存放的起始地址
    const uint8_t *sblob_start = &__linker_secure_blob_start;
    // 定义 OP-TEE OS 镜像文件存放的末端地址
    const uint8_t *sblob_end = &__linker_secure_blob_end;
    struct optee_header hdr; // 存放 OP-TEE OS image 头的信息
    size_t pg_part_size; //OP-TEE OS image 除去初始化头部信息的大小
    uint32_t pg_part_dst; //OP-TEE OS image 除去初始化头部信息后在 RAM 中的起始地址

    msg_init(); // 初始化 uart

    /* 加载 device tree 信息. 在 qemu 工程中, 并没有将 device tree 信息编译到 Bios.bin 中, 而默认存放在 DTB_START 地址中 */
    fdt = open_fdt(DTB_START, &__linker_nsec_dtb_start, &__linker_nsec_dtb_end);
    r = fdt_pack(fdt);
    CHECK(r < 0);

    /* 判定 OP-TEE OS image 的大小是否大于 image header 的大小 */
    CHECK(((intptr_t)sblob_end - (intptr_t)sblob_start) <
    (ssize_t)sizeof(hdr));

    /* 将 OP-TEE OS image header 信息复制到 hdr 变量中 */
    copy_bios_image("secure header", (uint32_t)&hdr, sblob_start,
    sblob_start + sizeof(hdr));

    /* 校验 OP-TEE OS image header 中的 magic 和版本信息是否合法 */
    CHECK(hdr.magic != OPTEE_MAGIC || hdr.version != OPTEE_VERSION);msg("found secure header\n");
    sblob_start += sizeof(hdr); // 将 sblob_start 的值后移到除去 image header 的位置
    CHECK(hdr.init_load_addr_hi != 0); // 检查 OP-TEE OS 的初始化加载地址是否为零

    /* 获取 OP-TEE OS 除去 image header 和 ini 操作部分代码后的大小 */
    pg_part_size = sblob_end - sblob_start - hdr.init_size;

    /* 确定存放 OP-TEE OS 除去 image header 和 init 操作部分代码后存放在 RAM 中的地址 */
    pg_part_dst = (size_t)TZ_RES_MEM_START + TZ_RES_MEM_SIZE - pg_part_size;

    /* 将存放 OP-TEE OS 除去 image header 和 init 操作部分后的内容复制到 RAM 中 */
    copy_bios_image("secure paged part",
    pg_part_dst, sblob_start + hdr.init_size, sblob_end);
    sblob_end -= pg_part_size; // 重新计算 sblo_end 的地址, 剔除 page part

    // 将 pg_part_dst 赋值给 arg 中的 paged_part 以备跳转执行 OP-TEE OS 使用
    arg->paged_part = pg_part_dst;

    // 将 hdr.init_load_addr_lo 赋值给 arg 中的 entry, 该地址为 op-TEE OS 的入口地址
    arg->entry = hdr.init_load_addr_lo;
    /* 将 OP-TEE OS 的实际 image 复制到起始地址为 hdr.init_load_addr_l 的 RAM 地址中 */
    copy_bios_image("secure blob", hdr.init_load_addr_lo, sblob_start, sblob_end);

    // 复制 kernel image,rootfs 到 RAM, 并复制 device tree 到对应地址, 以备被 kernel 使用
    copy_ns_images();

    /* 将 device tree 的地址赋值给 arg->fdt 变量, 以备 OP-TEE OS 启动使用 */
    arg->fdt = dtb_addr;
    msg("Initializing secure world\n");
}
```

main_init_sec 函数执行后将会返回一个 `sec_entry_arg` 的变量, 该变量包含启动 OP-TEE OS 的入口地址, DT 的地址以及 `paged_table` 的地址. `sec_entry_arg` 变量将会被 entry.S 文件用来启动 OP- TEE OS,entry.S 会将 OP-TEE OS 的入口地址保存在 r0 寄存器中, 而 `paged_table` 部分的起始地址会被保 存在 r1 寄存器中, 将 r0 赋值给 ip, 最终 entry.S 文件 通过执行 blx ip 指令进入 OP-TEE OS 的入口函数中去 执行 OP-TEE OS 的启动. 当 OP-TEE OS 启动完成之后, entry.S 文件会调用 `main_init_ns` 函数来启动 Linux 内核. 待 Linux 内核启动完成之后, 整个系统也就启动完成.

entry.S 文件通过调用 `main_init_ns` 函数来完成对 Linux 内核的启动, 该函数会调用 `call_kernel` 函数来完成 Linux 内核的启动.

# 2. OPTEE 驱动启动

在 OP-TEE 工程中, OP-TEE 在 REE 侧的驱动会被编译到 Linux 内核镜像中, Linux 系统在启动的过程中会自动挂载 OP-TEE 的驱动, 驱动挂载过程中会创建 `/dev/tee0` 和 `/dev/teepriv0` 设备, 其中 `/dev/tee0 ` 设备将会被 REE 侧的用户空间的库 (libteec) 使用,`/dev/teepriv0` 设备将会被系统中的常驻进程 `tee_supplicant` 使用, 并且在 OP-TEE 驱动的挂载过程中会建立正常世界状态与安全世界状态之间的共享内存, 用于 OP-TEE 驱动与 OP-TEE 之间的数据共享, 同时还会创建两个链表, 分别用于保存来自 OP-TEE 的 RPC 请求和发送 RPC 请求的处理结果给 OP-TEE.

# 3. tee_supplicant 启动

`tee_supplicant` 是 Linux 系统中的常驻进程, 该进程用于接收和处理来自 OP-TEE 的 RPC 请求, 并将处理结果返回给 OP-TEE.** 来自 OP-TEE 的 RPC 请求主要包括 socket 操作, REE 侧文件系统操作, 加载 TA 镜像文件, 数据库操作, 共享内存分配和注册操作等 **. 该进程在 Linux 系统启动过程中被自动创建, 在编译时, 该进程的启动信息会被写入到 / etc/init.d 文件中, 而该进程的可执行文件则被保存在文件系统的 bin 目录下. 该进程中会使用一个 loop 循环接收来自 OP-TEE 的远程过程调用 (Remote Procedure Call,RPC) 请求, 且每次获取到来自 OP-TEE 的 RPC 请求后都会自动创建一个线程, 用于接收 OP-TEE 驱动队列中来自 OP-TEE 的 RPC 请求, 之所以这么做是因为时刻需要保证在 REE 侧有一个线程来接收 OP-TEE 的请求, 实现 RPC 请求的并发处理.

