# 08_OPTEE-OS_系统集成之(六)TEE 的驱动
O**P-TEE 驱动是 REE 侧与 TEE 侧之间进行交互的重要通道 **, 在 REE 侧的 CA 接口以及 RPC 请求的接收和结果的返回最终都会被发送到驱动中, 由驱动对数据做进一步的处理. OP-TEE 驱动通过解析传入的参数, 重新组合数据,** 将需要被传入到 TEE 侧的数据载入到共享内存中 **,** 触发安全监控模式调用 (smc) 进入到 Monitor 模式或 EL3 中将数据发送给 TEE**.

<div align='center'><img src="https://raw.githubusercontent.com/carloscn/images/main/typoratypora20221005185005.png" width="80%" /></div>

# 1. 编译

OP-TEE 的驱动通过 **subsys_initcall 和 module_init** 宏来告知系统在初始化阶段的什么时候去加载 OPTEE 驱动. subsys_initcall 定义在 linux/include/init.h 文件中. 这是 Linux 内核提供的一系列 init 的操作集合中的一个. 该方法利用 Linux 内核段属性机制.

>subsys_initcall 是一个宏, 定义在 `linux/init.h` 中. 经过对这个宏进行展开, 发现这个宏的功能是: 将其声明的函数放到一个特定的段:`.initcall4.init`
>
>```C
>subsys_initcall
>    __define_initcall("4",fn,4)
>```
>
>以下文件在 / include/linux/init.h:
>
><div align='left'><img src="https://raw.githubusercontent.com/carloscn/images/main/typora%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87008.png" width="80%" /></div>
>
>分析 module_init 宏, 可以看出它将函数放到 `.initcall6.init` 段
>
>```C
>module_init
>    __initcall
>        device_initcall
>            __define_initcall("6",fn,6)
>```
>
>打开编译过的内核源码树中的的 / arch/arm/kernel/vmlinux.lds 文件(没编译没有这个文件):
>
>```bash
>SECTIONS
>{
> . = 0xC0000000 + 0x00008000;
> .init : { /* Init code and data                */
>  _stext = .;
>  _sinittext = .;
>   *(.head.text)
>   *(.init.text) *(.cpuinit.text) *(.meminit.text)
>  ......
>  . = ALIGN(16); __setup_start = .; *(.init.setup) __setup_end = .;
>  __initcall_start = .; *(.initcallearly.init) __early_initcall_end = .;
> *(.initcall0.init) *(.initcall0s.init)  *(.initcall1.init) *(.initcall1s.init)
> *(.initcall2.init) ... __initcall_end = .;
>  ......
>```
>
>内核在启动过程中需要顺序的做很多事, 内核如何实现按照先后顺序去做很多初始化操作. 内核的解决方案就是给内核启动时要调用的所有函数归类, 执行内核某一个函数然后每个类就会按照一定的次序被调用执行. 这些分类名就叫. initcallx.init.x 的值从 1 到 8.** 内核开发者在编写内核代码时只要将函数设置合适的级别, 这些函数就会被链接的时候放入特定的段, 内核启动时再按照段顺序去依次执行各个段即可 **(通过某一个函数, 链接脚本只是规定了某一程序段在内存中的存放位置).
>
>** 内核源代码 **:
>
>以下文件在 / init/main.c
>
>```C
>extern initcall_t __initcall_start[], __initcall_end[], __early_initcall_end[];
>
>static void __init do_initcalls(void)
>{
>    initcall_t *fn;
>
>    for (fn = __early_initcall_end; fn < __initcall_end; fn++)
>        do_one_initcall(*fn);
>
>    /* Make sure there is no pending stuff from the initcall sequence */
>    flush_scheduled_work();
>}
>```
>
>执行 do_initcalls 就会按照设定好的顺序去执行, 通过函数的内容可以猜测出其原理就是链接脚本设置好的顺序, 然后 do_initcalls 执行就会去按照链接脚本设置好的顺序一个个遍历.
>

OP-TEE 驱动通过 subsys_initcall 和 module_init 宏来告知系统在初始化的什么时候去加载 OP-TEE 驱动, subsys_initcall 定义在 `linux/include/init.h` 文件中, 内容如下:

```C
#define __define_initcall(fn, id) \
	static initcall_t __initcall_##fn##id __used \
	__attribute__((__section__(".initcall" #id ".init"))) = fn;

#define core_initcall(fn)		__define_initcall(fn, 1)
#define core_initcall_sync(fn)		__define_initcall(fn, 1s)
#define postcore_initcall(fn)		__define_initcall(fn, 2)
#define postcore_initcall_sync(fn)	__define_initcall(fn, 2s)
#define arch_initcall(fn)		__define_initcall(fn, 3)
#define arch_initcall_sync(fn)		__define_initcall(fn, 3s)
#define subsys_initcall(fn)		__define_initcall(fn, 4)
#define subsys_initcall_sync(fn)	__define_initcall(fn, 4s)
#define fs_initcall(fn)			__define_initcall(fn, 5)
#define fs_initcall_sync(fn)		__define_initcall(fn, 5s)
#define rootfs_initcall(fn)		__define_initcall(fn, rootfs)
#define device_initcall(fn)		__define_initcall(fn, 6)
#define device_initcall_sync(fn)	__define_initcall(fn, 6s)
#define late_initcall(fn)		__define_initcall(fn, 7)
#define late_initcall_sync(fn)		__define_initcall(fn, 7s)
```

使用 subsys_initcall 宏定义的函数最终会被编译到 `.initcall4.init` 段中, linux 系统在启动的时候会执行 `initcallx.init` 段中的所有内容, 而使用 `subsys_initcall` 宏定义段的执行优先级为 4.

`module_init` 的定义和相关扩展在 `linux/include/linux/module.h` 文件和 `linux/include/linux/init.h` 中, 内容如下:

```C
#define device_initcall(fn)		__define_initcall(fn, 6)
#define __initcall(fn) device_initcall(fn)
#define module_init(x)  __initcall(x);
```

由此可见, 使用 module_init 宏构造的函数将会在编译的时候被编译到 `initcall6.init` 段中, 该段在 linux 系统启动的过程中的优先等级为 6.

结合上述两点看, 在系统加载 OP-TEE 驱动的时候, 首先 ** 会执行 OP-TEE 驱动中使用 subsys_init 定义的函数 **, 然后再 ** 执行使用 module_init 定义的函数 **. 在 OP-TEE 驱动源代码中使用 `subsys_init` 定义的函数为 `tee_init`, 使用 `module_init` 定义的函数为 `optee_driver_init`.

# 2. REE 侧 OP-TEE 驱动的加载
OP-TEE 驱动主要作用是 REE 与 TEE 端进行数据交互的桥梁作用.**`tee_supplicant` 和 `libteec` 调用接口之后几乎都会首先通过系统调用陷入到 kernel space, 然后 kernel 根据传递的参数找到 OP-TEE 驱动, 并命中驱动的 operation 结构体中的具体处理函数来完成实际的操作, 对于 OP-TEE 驱动, 一般都会触发 `SMC` 调用, 并带参数进入到 ARM 的 monitor 模式, 在 monitor 模式中对执行 normal world 和 secure world 的切换, 待状态切换完成之后, 会将驱动端带入的参数传递給 OP-TEE 中的 thread 进行进一步的处理 **.OP-TEE 驱动的源代码存放在 `linux/drivers/tee` 目录中, 其内容如下:

```bash
libteec(tee_supplicant)
    -> linux kernel_space
        -> find operation structure
            -> call `SMC`
	        -> pass params to thread of opteeos.
```

https://github.com/carloscn/raspi-linux/tree/master/drivers/tee

<div align='center'><img src="https://raw.githubusercontent.com/carloscn/images/main/typora20221005092357.png" width="80%" /></div>

OP-TEE 驱动的加载过程分为两部分:
* 第一部分是创建 class 和分配设备号;
* 第二部分是 probe 过程.

首先需要明白两个 Linux 内核中加载驱动的宏: subsys_initcall 和 module_init.OP-TEE 驱动的第一部分是调用 subsys_initcall 宏来实现 tee_init, 而第二部分则是调用 module_init 宏来实现 tee_driver_init.

OP-TEE 驱动主要是被期望能够功能实现和数据交换.
* ** 用于功能 **:OPTEE 驱动会创建两个设备, 分别为 / dev/tee0 和 / dev/teepriv0, 这两个设备分别被 **libteec 库和 tee_supplicant 使用 **, 用于实现各自的功能.
* ** 用于数据交换 **: 驱动与 TEE 侧之间的数据传递是通过 ** 共享内存 ** 的方式来完成的, 即 ** 在 OP-TEE 驱动挂载过程中会创建 OPTEE 与 TEE 之间的专用共享内存空间 **, 在 Linux 的用户空间需要发送到 TEE 的数据最终都会被保存在该共享内存中, 然后再切换 ARM 核的状态后, OPTEE 从该共享内存中去获取数据.

<div align='center'><img src="https://raw.githubusercontent.com/carloscn/images/main/typora20221005100336.png" width="100%" /></div>

## 2.1 tee_init
该函数定义在 `linux/drivers/tee/tee_core.c` 文件中, 主要完成 class 的创建和设备号的分配, 其内容如下:
```C
static int __init tee_init(void){
	int rc;
	// 分配 OP-TEE 驱动的 class
	tee_class=class_create(THIS_MODULE,"tee");
	if(IS_ERR(tee_class)){
		pr_err("couldn't create class \n");
		return PTR_ERR(tee_class);
	}
	// 分配 OP-TEE 的设备号
	rc=alloc_chrdev_region(&tee_devt,0,TEE_NUM_DEVICES,"tee");
	if(rc){
		pr_err("failed to allocate char dev region\n");
		class_destory(tee_class);
		tee_class = NULL;
	}
	retrun rc;
}
```
` 设备号 ` 和 `class` 将会在驱动执行 `probe` 的时候被使用到.

## 2.2 optee_driver_init
linux 启动过程中会执行 moudule_init 宏定义的函数, 在 OP-TEE 驱动的挂载过程中将会执行 `optee_driver_ini`t 函数, 该函数定义在 `linux/drivers/tee/optee/core.c` 文件中, 其内容如下:

```C
static int __init optee_driver_init(void){
	struct device_node* fw_np;
	struct device_node* np;
	struct optee* optee;

	//node is supposed to be below /firmware
	// 从 device tree 查找到 firmware 的节点
	fw_np = of_find_node_by_name(NULL,"firmware");
	if(!fw_np)
		return -ENODEV;
	// 匹配 device tree 中 firmware 节点下为 linaro,optee-tz 内容的节点
	np = of_find_match_node(fw_np, optee_match);
	of_node_put(fw_np);
	if(!np)
		return -ENODEV;
	// 使用查找到的节点执行 OP-TEE 驱动的 probe 操作
	optee= optee_probe(np);
	of_node_put(np);
	if(IS_ERR(optee))
		return PTR_ERR(optee);
	// 保存初始化完成之后 OP-TEE 设备信息到 optee_svc 中, 以备在卸载时使用
	optee_svc=optee;
	return 0;
}
```

## 2.3 挂载 probe 操作
OP-TEE 驱动在 optee_driver_init 函数中完成 probe 操作. 该函数首先会通过设备树找到 OP-TEE 驱动的设备信息, 然后将获取到的信息传递给 optee_probe 函数执行 probe 操作. probe 操作主要完成版本的校验, 获取 OP-TEE 驱动与 TEE 侧共享内存的配置, 建立共享内存的地址映射, 添加安全监控模式调用 (smc) 接口, 分配 / dev/tee0 和 / dev/teepriv0 设备, 建立 RPC 请求队列等操作. optee_probe 函数内容如下:
```C
static struct optee *optee_probe(struct device_node *np){
	optee_invoke_fn *invoke_fn;
	struct tee_shm_pool* pool;
	struct optee* optee=NULL;
	void *memremaped_shm=NULL;
	struct tee_device* teedev;
	u32 sec_caps;
	int rc;

	// 获取 OP-TEE 驱动在 device tree 中及节点描述内容中的定义的执行切换到 monitor 模式的接口
	invoke_fn = get_invoke_func(np);
	if(IS_ERR(invoke_fn))
		return (void* )invoke_fn;
	// 调用到 secure world 中获取 API 版本信息是否匹配
	if(!optee_msg_api_uid_is_optee_api(invoke_fn)){
		pr_warn("api uid mismatch \n");
		return ERR_PTR(-EINVAL);
	}
	// 调用到 secure world 中获取版本信息是否匹配
	if(!optee_msg_api_revision_is_compatible(invoke_fn)){
		pr_warn("api revision mismatch \n");
		return ERR_PTR(-EINVAL);
	}
	// 调用到 secure world 中获取 secure world 是否 reserved shared memory
	if(!optee_msg_exchange_capbilities(invoke_fn,&sec_caps)){
		pr_warn("capabilities mismatch\n");
		return ERR_PTR(-EINVAL);
	}
	//we have no other option for shared memory,if secure world
	//doesn't have any reserved memory we can use we can't continue
	// 判断 secure world 中是否 reserve 了 share memory, 如果没有则报错
	if(!(sec_caps & OPTEE_SMC_CAP_HAVE_RESERVED_SHM)){
		return ERR_PTR(-EINVAL);
	}
// 配置 secure world 与驱动之间的 shared memory, 并进行地质映射到建立共享内存池
	pool= optee_config_shm_memremap(invoke_fn,&memremaped_shm);
	if(IS_ERR(pool)){
		return (void* )pool;
	}
// 在 kernel space 内存空间中分配一块内存用于存放 OP-TEE 驱动的结构体变量
	optee = kzalloc(sizeof(*optee),GFP_KERNEL);
	if(!optee){
		rc=-ENOMEM;
		goto err;
	}
// 将驱动用于实现进入 monitor 模式的接口赋值到 optee 结构体中的 invoke_fn 成员中
	optee->invoke_fn= invoke_fn;
// 分配设备信息, 填充 libteec 使用的驱动文件信息和 operation 结构体
// 并创建 / dev/tee0 文件, libteec 将会使用该文件来实现 op-tee 驱动
	teedev=tee_device_alloc(&optee_desc,NULL,pool,optee);
	if(IS_ERR(teedev)){
		rc=PTR_ERR(teedev);
		goto err;
	}

	//libteec 使用的驱动文件信息填充到 optee 中的 teedev 的成员中
	// 分配设备信息, 填充被 tee_supplicant 使用的驱动文件信息和 operation 结构体并创建
	// 到 dev/teepri0 文件, tee_supplicant 将会使用该文件使用 op-tee 驱动
	teedev=tee_device_alloc(&optee_supp_desc,NULL,pool,optee);
	if(IS_ERR(teedev)){
		rc=PTR_ERR(teedev);
		goto err;
	}
// 将 tee_supplicant 使用的驱动文件信息填充到 optee 中的 supp_teedev 成员中
	optee->supp_teedev=teedev;
// 将被 libteec 使用的设备信息注册到系统设备中
	rc=tee_device_register(optee->teedev);
	if(rc)
		goto err;
// 将被 tee_supplicant 使用的设备信息注册到系统设备中
	tc=tee_device_register(optee->supp_teedev);
	if(rc)
		goto err;
	mutex_init(&optee->call_queue.mutex);
	INIT_LIST_HEAD(&optee->call_queue.waiters);
// 初始化 RPC 操作队列
	optee_wait_queue_init(&optee->wait_queue);
// 初始化被 tee_supplicant 用到的用于存放来自 TA 的请求队列
	optee_supp_init(&optee->supp);
// 填充 optee 中的共享内存信息和共享内存池信息成员
	optee->memremaped_shm=memremaped_shm;
	optee->pool=pool;
// 使能共享内存的 cache
	optee_enable_shm_cache(optee);
	ptr_info("initialized driver\n");
	return optee;
err:
	if(optee){
		//tee_device_unregister() is safe to call even if the
		//devices hasn't been registered with tee_device_register() yet
		tee_device_unregister(optee->supp_teedev);
		tee_device_unregister(optee->teedev);
		kfree(optee);
	}
	if(pool)
		tee_shm_pool_free(pool);
	if(memremaped_shm)
		memunmap(memremaped_shm);
	return ERR_PTR(rc);
}
```

## 2.4 获取切换到 Monitor 模式或 EL3 的接口

正常世界状态与安全世界状态之间的切换是通过在 Monitor 模式或 EL3 下设定 SCR 寄存器中的安全状态位 (NS bit) 来实现的, OP-TEE 驱动被上层调用时, 最终会通过触发安全监控模式调用 (smc) 切换到 Monitor 模式或 EL3, 并通过共享内存的方式将数据发送给安全世界状态来进行处理. 而用户触发安全监控模式调用的接口函数将在 OP-TEE 驱动初始化时被填充到 OP-TEE 驱动的 device info 中, 在 OP-TEE 驱动中通过调用 get_invoke_func 函数来获取该接口的指针. 该函数的内容如下:

```C
static optee_invoke_fn *get_invoke_func(struct device_node*np){
	const char*method;
	pr_info("probing for conduit method from DT.\n");
// 获取 op-tee 驱动在 device tree 中的节中 method 属性的值
	if(of_property_read_string(np,"method",&methodd)){
		pr_warn("missing \"method\"property\n");
		return ERR_PTR("-ENXIO");
	}
// 判定 op-tee 驱动是使用 smc 的方式还是使用 hvc 的方式来实现进入 monitor 模式的操作,
// 根据 method 的值与 hvc 还是 smc 匹配来决定那种切换方法, 并将用于切换到 monitor 的接口
	if(!strcmp("hvc",method))
		return optee_smccc_hvc;
	else if(!strcmp("smc",method))
		return optee_smccc_smc;

	pr_warn("invalid \"method\"property:%s\n",method);
	return ERR_PTR(-EINVAL);
}
```

执行安全监控模式调用指令会使 ARM 核进入 EL3 或 Monitor 模式. 如果使用 hvc, 会是 ARM 核进入到 EL2 或者 hyp 模式, 该模式主要用在使能虚拟机的系统上. 这里以安全监控模式调用为例, 实现系统状态切换的函数就是 optee_smccc_smc, 该函数内容如下:

```C
static void optee_smcc_smc(unsigned long a0,unsigned long a1,
	unsigned long a2,unsinged long a3,
	unsinged long a4,unsinged long a4,
	unsinged long a4,unsinged long a5,
	unsinged long a6,unsinged long a7,
	struct arm_smccc_res* res){
	arm_smccc_smc(a0,a1,a2,a3,a4,a5,a6,a7,res);
}
```

即是函数 get_invoke_func 执行完成之后会返回 arm_smccc_smc 函数的地址. arm_smccc_smc 函数就是驱动用来将 cortex 切换到 monitor 模式的函数, 该函数是以汇编的方式编写, 定义在 `linux/arch/arm/kernel/smccc-call.S` 文件中. 如果是 64 位系统, 则该函数定义在 `linux/arch/arm64/kernel/smccc-call.S` 目录中, 本文以 32 位系统为例, 该函数内容如下:

```assembly
//wrap cmacros in asm macros to delay expansion until after the
//SMCCC asm macro is expanded
/*SMC_SMC 宏, 触发 smc*/
	.macro SMCCC_SMC
	__SMC(0)
	.endm

/*SMCCC_HVC 宏, 触发 hvc*/
	.macro SMCCC_HVC
	__HVC(0)
	.endm

/* 定义 SMCCC 宏, 其参数为 instr*/
	.mac SMCCC instr
UNWIND( .fnstart) // 将 normal world 中的寄存器入栈, 保存现场
	mov r12,sp
	push {r4-r7}
UNWIND( .save {r4-r7})
	ldm r12,{r4-r7}
	\instr   // 执行 instr 参数的内容, 即执行 smc 切换
	pop {r4-r7}   // 出栈操作, 恢复现场
	ldr r12,[sp,#(4 * 4)]
	stm r12,{r0-r3}
	bx lr
UNWIND( .fnend)
	.endm

ENTRY(arm_smccc_smc)
	SMCCC SMCCC_SMC
ENDPROC(arm_smccc_smc)
```

## 2.5 驱动版本和 API 版本校验
OP-TEE 驱动挂载过程中会校验驱动的版本以及提供的 API 版本是否一致, 该检查是通过触发快速安全监控模式调用 (fast smc) 从 OP-TEE 中获取到版本信息来实现的. 快速安全监控模式调用与标准安全监控模式调用 (std smc) 的不同之处就在于第一个参数的 BIT31 的值不一样.

驱动加载过程中获取到 REE 侧与 TEE 侧之间进行交互的接口函数 (调用 `get_invoke_func` 函数返回的函数地址) 之后, OP-TEE 驱动会对 API 的 UID 和版本信息进行校验. 上述操作是通过调用 `optee_msg_api_uid_is_optee_api` 函数和 `optee_msg_api_revision_is_compatible` 函数来实现的. 这两个函数的内容如下:
```C
static bool optee_msg_api_uid_is_optee_api(optee_invoke_fn *invoke_fn) {
	struct arm_smccc_res res;
	/* 调用执行 smc 操作的接口函数, 带入的 command ID 为 OPTEE_SMC_CALLS_UID */
	invoke_fn(OPTEE_SMC_CALLS_UID, 0, 0, 0, 0, 0, 0, 0, &res);
	/* 比较返回的 UID 的值与在驱动中定义的 UID 的值是否匹配 */
	if (res.a0 == OPTEE_MSG_UID_0 && res.a1 == OPTEE_MSG_UID_1 && 			res.a2 == OPTEE_MSG_UID_2 && res.a3 == OPTEE_MSG_UID_3)
		return true;
	return false;
}
```

```C
static bool optee_msg_api_revision_is_compatible(optee_invoke_fn *invoke) {
	union {
            struct arm_smccc_res smccc;
            struct optee_smc_calls_revision_result result;
        } res;
/* 调用执行 smc 操作的接口函数, 带入的 command ID 为 OPTEE_SMC_CALLS_REVISION*/
	invoke_fn(OPTEE_SMC_CALLS_REVISION, 0, 0, 0, 0, 0, 0, 0, &res.smccc);
/* 比较返回的版本信息的值与驱动中定义的版本值是否匹配 */
        if (res.result.major == OPTEE_MSG_REVISION_MAJOR &&
            (int)res.result.minor >= OPTEE_MSG_REVISION_MINOR)
	    return true;
	return false;
}
```

>   ### FAST SMC 和 STD SMC[^3]
>
>   在 Two types of calls are defined:
>
>   *   Fast Calls used to execute atomic Secure operations.  (原子的安全操作)
>   *    Standard Calls used to start pre-emptible Secure operations.  (可抢占的安全操作)
>
>   在 OP-TEE 驱动的挂载过程中会使用 fast smc 的方式从 OP-TEE 中获取到相关数据, 例如从 secure world 中获取 reserve 的共享内存信息时就是通过调用如下函数来实现的:
>   ```C
>   invoke_fn(OPTEE_SMC_GET_SHM_CONFIG, 0, 0, 0, 0, 0, 0, 0, &res.smccc);
>   ```
>   在支持 smc 操作的 32 位系统中该函数等价于:
>   ```C
>   arm_smccc_smc(OPTEE_SMC_GET_SHM_CONFIG, 0, 0, 0, 0, 0, 0, 0, &res.smccc);
>   ```
>   而 OPTEE_SMC_ENABLE_SHM_CACHE 的定义如下:
>   ```C
>   #define OPTEE_SMC_FUNCID_GET_SHM_CONFIG 7
>   #define OPTEE_SMC_GET_SHM_CONFIG OPTEE_SMC_FAST_CALL_VAL(OPTEE_SMC_FUNCID_GET_SHM_CONFIG)
>   ```
>   完全展开之 `OPTEE_SMC_GET_SHM_CONFIG` 宏的值的各个 bit 中的数值如下如所示:
>   ![](https://raw.githubusercontent.com/carloscn/images/main/typoratypora20221005104922-20221005105257334.png)
>
>   在执行 smc 操作时, cortex 会解读第一个参数中的相关位来决定进入到 monitor 模式后的相关操作, 在 ARM 官方定义了第一个参数 `(a0)` 的定义如下:
>
>   <div align='center'><img src="https://raw.githubusercontent.com/carloscn/images/main/typoraimage-20221005110114768.png" width="80%" /></div>
>
>   当 bit31 为 1 时, 则表示进入 monitor 模式后会执行 `fast smc` 的中断处理函数, 而在不带 ATF 的 ARCH32 中, monitor 的中断向量表如下:
>   ```
>   FUNC thread_vector_table , :
>   UNWIND( .fnstart)
>   UNWIND( .cantunwind)
>   	b vector_std_smc_entry
>   	b vector_fast_smc_entry
>   	b vector_cpu_on_entry
>   	b vector_cpu_off_entry
>   	b vector_cpu_resume_entry
>   	b vector_cpu_suspend_entry
>   	b vector_fiq_entry
>   	b vector_system_off_entry
>   	b vector_system_reset_entry
>   UNWIND( .fnend)
>   END_FUNC thread_vector_table
>   ```
>   由此可以见:
>   -   驱动 ** 挂载的时候 ** 请求共享内存配置数据的请求将会被 `vector_fast_smc_entry` 处理.
>   -   当请求来自于 CA 的请求时, 驱动会将第一个参数的 `bi31` 设置成 `0`, 也就是 CA 的请求会被 `vector_std_smc_entry` 进行处理

## 2.6 判断 OP-TEE 是否预留了共享内存
OP-TEE 驱动与 TEE 之间需要进行数据的交互, 而进行数据交互则需要一定的共享内存来保存 OP-TEE 和驱动之间共有的数据. 所以在驱动初始化时需要检查该共享内存空间是否被预留出来.** 通过获取安全世界状态 (SWS) 中的相关变量的值并判定该相关标识变量是否相等来判定安全世界状态是否预留有共享内存空间 **. 在 **OP-TEE OS 启动过程中, 执行 MMU 初始化时会初始化该变量 **. 在 OPTEE 驱动端通过调用 `optee_msg_exchange_capabilities` 函数来获取该变量的值.
```C
static pool optee_msg_exchange_capabilities(optee_invoke_fn *invoke_fn,u32* sec_caps){
	union{
		struct arm_smccc_res smccc;
		struct optee_smc_exchange_capabilities_result result;
	} res;
	u32 a1=0;
//TODO this isn't enough to tell if it's UP system
//(from kernel point of view) or not,is_smp() returns the information needed,
//but can't be called directly from here
	if(!IS_ENABLED(CONFIG_SMP) || nr_cpu_ids ==1)
		a1 |=OPTEE_SMC_NSEC_CAP_UNIPROCESSOR;
// 调用 smc 操作接口, 获取 secure world 中的变量
	invoke_fn(OPTEE_SMC_EXCHANGE_CAPABILITES,a1,0,0,0,0,0,0,&res.smccc);
	if(res.result.status != OPTEE_SMC_RETURN_OK){
		return false;
	}
	// 将返回值中的变量赋值为 sec_caps
	*sec_caps = res.result.capabilities;
	return true;
}
```

当驱动获取到 sec_caps 的值之后会查看该值是否为宏 `OPTEE_SMC_SEC_CAP_HAVE_RESERVED_SHM` 定义的值 `BIT(0)`, 如果该值不为 BIT(0), 则会报错, 因为在 secure world 端都没有预留 share memory 空间, 那驱动与 secure world 之间也就没法传输数据, 所以有没有驱动也就没有必要了.

## 2.7 配置驱动与 OP-TEE 之间的共享内存
驱动与安全世界状态之间的数据交互是通过共享内存来完成的,** 在 OP-TEE 启动过程中会将作为共享内存的物理内存块预留出来 **, 具体可查看 OPTEE 启动代码中的 `core_init_mmu_map` 函数. O**PTEE 驱动初始化阶段会将预留出来作为共享内存的物理内存配置成驱动的内存池, 并通知 OP-TEE OS 执行相同的操作 **. 配置完成后, 安全世界状态就能从共享内存中获取到来自 REE 侧的数据.

OP-TEE 驱动进行 probe 操作时, 会调用到 `optee_config_shm_memremap` 函数来完成 OP-TEE 驱动和 OP-TEE 之间共享内存的配置. 该函数定义在 `linux/drivers/tee/optee/core.c` 文件中. 其内容如下:

```c
static struct tee_shm_pool * optee_config_shm_memremap(optee_invoke_fn* invoke_fn,void **memreamped_shm){
	union{
		struct arm_smccc_res smccc;
		struct optee_smc_get_shm_config_result result;
	} res;
	struct tee_shm_pool* pool;
	unsinged long vaddr;
	phys_addr_t paddr;
	size_t size;
	phys_addr_t begin;
	phys_addr_t end;
	void *va;
	struct tee_shm_pool_mem_info priv_info;
	struct tee_shm_pool_mem_info dmabuf_info;

// 调用 smc 类操作, 通知 OP-TEE OS 返回被 reserve 出来的共享内存的物理地址和大小
	invoke_fn(OPTEE_SMC_GET_SHM_CONFIG,0,0,0,0,0,,&res.smccc);
	if(res.result.status != OPTEE_SMC_RETURN_OK){
		pr_info("shm service not avaliable \n");
		return ERR_PTR(-ENOENT);
	}
// 判断是否提供 secure world 中的 cache
	if(res.result.settings != OPTEE_SMC_SHM_CACHED){
		pr_err("only normal cached shared memory supported\n");
		return ERR_PTR(-EINVAL);
	}
// 将对齐操作之后的物理内存块的起始地址赋值给 paddr, 该块内存的大小赋值给 size
	begin =roundup(res.result.start,PAGE_SIZE);
	end=rounddown(res.result.start+res.result.size,PAGE_SIZE);
	paddr=begin;
	size=end - begin;
// 判断作为共享内存的物理地址的大小是否大于两个 page 大小
// 如果小于则报错, 因为驱动配置用于 dma 操作和普通共享内训的大小分别为一个 page 大小
	if(size<2*OPTION_SHM_NUM_PRIV_PAGES*PAGE_SIZE){
		pr_err("too small shared memory area \n");
		return ERR_PTR(-EINVAL);
	}
// 配置驱动私有内存空间的虚拟地址的启动地址, 物理地址的起始地址以及大小, 配置
//dma 缓存的虚拟起始地址和物理地址以及大小.
//dmabuf 与 pribuf 两个相邻, 分贝为一个 page 的大小
	priv_info.vaddr=vaddr;
	priv_info.paddr=paddr;
	priv_info.size = OPTEE_SHM_NUM_PRIV_PAGES * PAGE_SIZE;
	dmabuf_info.vaddr=vaddr+OPTEE_SHM_NUM_PRIV_PAGES *PAGE_SIZE;
	dmabuf_info.paddr=paddr+OPTEE_SHM_NUM_PRIV_PAGES*PAGE_SIZE;
	dmabuf_info.size=size-OPTEE_SHM_NUM_PRIV_PAGES*PAGE_SIZE;
// 将驱动的私有 buffer 和 dma buffer 天剑到内存池中, 以便驱动在使用本身的 alloc
// 函数的时候能够从私有共享和 dma buffer 中分配内存来使用
	pool= tee_shm_pool_alloc_res_mem(&priv_info,&dmabuf_info);
	if(IS_ERR(pool)){
		memunmap(va);
		goto out;
	}
// 将驱动与 OP-TEE 的共享内存赋值给 memremaped_shm 变量执行的地址
	*memremaped_shm=va;
out:
	return pool;// 返回共享内存池的结构体
}
```

在 secure world 中预留出来的内存块作为驱动与 sercure world 之间的共享内存使用. 在 ** 驱动端将会建立一个内存池 **,** 以便驱动在需要的使用通过调用驱动的 alloc 函数来完成共享内存的分配 **. 而共享内存池的建立是通过调用 `tee_shm_pool_alloc_res_mem` 来实现的. 其函数内容如下:

```c
struct tee_shm_pool* tee_shm_pool_alloc_res_mem(struct tee_shm_pool_mem_info *priv_info,
	struct tee_shm_pool_mem_info *dmabuf_info){
	struct tee_shm_pool *pool=NULL;
	int ret;
// 从内核空间的 memory 中分配一块用于存放驱动内存池结构体变量的内存
	pool = kzalloc(sizeof(*pool),GFP_KERNEL);
	if(!pool){
		ret = -ENOMEM;
		goto err;
	}
	//TODO create the pool for driver private shared memory
// 调用 pool 相关函数完成内存池的创建, 设定 alloc 的分配算法, 并将私有化共享内存的
// 其实虚拟地址, 其实物理地址以及大小信息保存到私有共享内存池中
	ret = pool_res_mem_mgr_init(&pool->private_mgr,pri_info,3/*8 byte aligned*/);
	if(ret)
		goto err;

	//TODO create the pool for dma_buf shared memory
// 调用 pool 相关函数完成内存池的创建, 设定 alloc 时分配算法, 并将 dma 共享内存的起始的
// 虚拟地址, 起始物理地址以及大小信息保存到 dma 的共享内存池中
	ret = pool_res_mem_mgr_init(&pool->dma_buf_mgr,dmabuf_info,PAGE_SHIFT);
	if(ret)
		goto err;
// 设定销毁共享内存池的接口函数
	pool->destory = pool_res_mem_destory;
	return pool;// 返回内存池结构体
err:
	if(ret == -ENOMEM)
		pr_err("%s:can't allocate memory for res_mem shared memory pool\n",__func__);
	if(pool && pool->private_mgr.private_data)
		gen_pool_destory(pool->private_mgr.private_data);
	kfree(pool);
	return ERR_PTR(ret);
}
```

## 2.8 分配 tee0 和 teepriv0 变量
在 OP-TEE 驱动进行 probe 操作时会分配和设置两个 tee_device 结构体变量, 分别用来表示被 `libteec` 库和 `tee_supplicant` 使用的设备. 分别通过执行 `tee_device_alloc(&optee_desc,NULL,pool,optee)` 和 `tee_device_alloc(&optee_supp_desc,NULL,pool,optee)` 来实现, 主要是设置驱动被 libteec 库和 tee_supplicant 使用时的设备具体操作和设备对应的名称等信息.
* 当 libteec 库调用文件操作函数执行打开, 关闭等操作 `/dev/tee0` 设备文件时, 系统最终将调用到 `optee_desc` 中具体的函数来实现对应操作.
* 当 tee_supplicant 调用文件操作函数执行打开, 关闭等操作 `/dev/teepriv0` 设备文件时, 系统最终将调用到 `optee_supp_desc` 中具体的函数来实现对应操作.

上述配置操作都是通过调用 `tee_device_all` 函数来实现的.
```C
static tee_device* tee_device_alloc(const struct tee_desc *teedesc,struct device*dev,struct tee_shm_pool* pool,void *driver_data){
	struct tee_device* teedev;
	void *ret;
	int rc;
	int offs=0;

// 参数检查
	if(!teedesc || !teedesc->name ||!teedesc->ops||
		!teedesc->ops->get_version || !teedesc->ops->open ||
		|| !teedesc->ops->release ||!pool){
		return ERR_PTR(-EINVAL);
	}
// 从 kernel space 中分配用于存放 tee_device 变量的内存
	teedev=kzalloc(sizeof(*teedev),GFP_KERNEL);
	if(!teedev){
		ret=ERR_PTR(-ENOMEM);
		goto err;
	}
// 判定当前分配的设备结构体是提供给 libteec 还是 tee_supplicant, 如果该设备时分配
// 给 tee_supplicant, 则将 offs 设置成 16,offs 将会在用于设置设备的 id 时被使用
	if(teedesc->flags & TEE_DESC_PRIVILEGED)
		offs = TEE_NUM_DEVICES /2;
// 查找 dev_mask 中的从 Offs 开始的第一个为 0 的 bit 位, 然后将该值作为设备的 id 值
	spin_lock(&driver_lock);
	teedev->id=find_next_zero_bit(dev_mask,TEE_NUM_DEVICES,offs);
	if(teedev->id<TEE_NUM_DEVICES)
		set_bit(teedev_id,dev_mask);
	spin_unlock();
// 判断设定的设备 id 是否超出最大数
	if(teedev->id >= TEE_NUM_DEVICES){
		ret =ERR_PTR(-ENOMEM);
		goto err;
	}
// 组合出设备名, 对于 libteec 来说, 设备名为 tee0. 对于 tee_supplicant 来说, 设备名为 teepriv0
	snprintf(teedev->name, sizeof(teedev->name), "tee%s%d",
		 teedesc->flags & TEE_DESC_PRIVILEGED ? "priv" : "",
		 teedev->id - offs);
// 设定设备的 class,tee_class 在 tee_init 函数中被分配. 设定执行设备 release 的操作函数
   和 dev.parent
	teedev->dev.class = tee_class;
	teedev->dev.release = tee_release_device;
	teedev->dev.parent=dev;
// 将设备的主设备号和设备 ID 组合后转化成 dev_t 类型
	teedev->dev.devt=MKDEV(MAJOR(tee_devt),teedev->id);
// 设置设备名, 驱动被 libteec 使用时设备名为 tee0, 驱动被 tee_supplicant 使用时设备名为 teepriv0
	rc = dev_set_name(&teedev->dev, "%s", teedev->name);
	if (rc) {
		ret = ERR_PTR(rc);
		goto err_devt;
	}
// 设置驱动作为字符设备的操作函数接口, 即指定该驱动在执行 open,close, ioctl 的函数接口
	cdev_init(&teedev->cdev, &tee_fops);
	teedev->cdev.owner = teedesc->owner;	// 初始化字符设备的 owner
	teedev->cdev.kobj.parent = &teedev->dev.kobj;	// 初始化 kobj.parent 成员
// 设置设备的私有数据
	dev_set_drvdata(&teedev->dev, driver_data);
// 初始化设备
	device_initialize(&teedev->dev);
//1 as tee_device_unregister() does one final tee_device_put()
	teedev->num_users =1;// 标记改设备可以被使用
	init_completion(&teedev->c_no_users);
	mutex_init(&teedev->mutex);
	idr_init(&teedev->idr);

// 设定设备的 desc 成员, 该成员包含设备最终执行具体操作的函数接口
	teedev->desc=teedesc;
// 设置设备的内存池, 主要是驱动与 secure world 之间共享内存的私有共享内存和 dma 操作共享内存
	teedev->pool = pool;
	return teedev;
err_devt:
	unregister_chrdev_region(teedev->dev.devt,1);
err:
	pr_err("could not register %s driver\n",teedesc->flags & TEE_DESC_PRIVILEGED?"privlieged":"client");
	if(teedev && teedev->id< TEE_NUM_DEVICES){
		spin_lock(&driver_lock);
		clear_bit(teedev->id,dev_mask);
		spin_unlock(&driver_lock);
	}
	kfree(teedev);
	return ret;
}
```

完成版本检查, 共享内存池配置, 不同设备的配置之后就需要将这些配置好的设备注册到系统中去. 对于被 liteec 和 tee_supplicant 使用的设备分别通过调用 `tee_device_register(optee->teedev)` 和 `tee_device_register(optee->supp_teedev)` 来实现. 其中 `optee->teedev` 和 `optee->supp_teedev` 就是在上一章中被配置好的分别被 libteec 和 tee_supplicant 使用的设备结构体. 调用 `tee_device_register` 函数来实现将设备注册到系统的目的, 该函数内容如下:

```C
int tee_device_register(struct tee_device*teedev){
	int rc;
// 判定设备是否已经被注册过
	if(teedev->flags & TEE_DEVICE_FLAG_REGISERED){
		dev_err(&teedev->dev,"attempt to register twice\n");
		return -EINVAL;
	}
// 注册字符设备
	rc=cdev_add(&teedev->cdev,teedev->dev.devt,1);
	if(rc){
		dev_err(&teedev->dev,
			"unable to cdev_add() %s, major %d, minor %d, err=%d\n",
			teedev->name, MAJOR(teedev->dev.devt),
			MINOR(teedev->dev.devt), rc
		return rc;
	}
// 将设备添加到 linux 的设备模块中, 在该步中将会在 / dev 目录下创设备驱动文件节点, 即对于
// 被 libteec 使用的设备, 在该步将创建 / dev/tee0 设备驱动文件. 对于被 tee_supplicant 使用的设备, 在该步将创建 / dev/teepriv0 设备文件
	rc=device_add(&teedev->dev);
	if(rc){
		dev_err(&teedev->dev,
			"unable to device_add() %s, major %d, minor %d, err=%d\n",
			teedev->name, MAJOR(teedev->dev.devt),
			MINOR(teedev->dev.devt), rc
			goto err_device_add;
	}
// 在 / sys 目录下创建设备的属性文件
	rc=sysfs_create_group(&teedev->dev.kobj,&tee_dev_group);
	if(rc){
		dev_err(&teedev->dev,
			"failed to create sysfs attributes, err=%d\n", rc);
		goto err_sysfs_create_group;
	}
// 设定该设备以及被注册过
	teedev->flags |=TEE_DEVICE_FLAG_REGISTERED;
	return 0;
err_sysfs_create_group:
	device_del(&teedev->dev);
err_device_add:
	cdev_del(&teedev->cdev);
	return rc;
}
```
## 2.9 两个队列初始化
OP-TEE 驱动提供两个设备, 分别是被 libteec 库使用的 `/dev/tee0` 和被 tee_supplicant 使用的 `/dev/teepriv0`. 为确保正常世界状态与安全世界状态之间数据交互便利且能在正常世界状态进行异步处理, OP-TEE 驱动在挂载时会建立两个类似于消息队列的队列, 用于保存正常世界状态的请求数据和安全世界状态的请求.`optee_wait_queue_init` 用于初始化 `/dev/tee0` 设备使用的队列,`optee_supp_init` 用于初始化 `/dev/teepriv0` 设备使用的队列. 其代码分别如下:
```C
void optee_wait_queue_init(struct optee_wait_queue* priv){
	mutex_init(&priv->mu);
	INIT_LIST_HEAD(&priv-db);
}

void optee_supp_init(struct optee_supp* supp){
	memset(supp,0,sizeof(*supp));
	mutex_init(&supp->mutex);
	init_comletion(&supp->reqs_c);
	idr_init(&supp->idr);
	INIT_LIST_HEAD(&supp-reqs);
	supp->req_id=-1;
}
```

## 2.10 TEE 中的共享 cache
当一切执行完之后, 最后就剩下通知 OP-TEE 使能共享内存的缓存了, 在 OP-TEE 驱动的挂载过程中通过调用 `optee_enable_shm_cache` 函数来实现使能共享内存 Cache 的操作. 该函数内容如下:

```C
void optee_enable_shm_cache(struct optee*optee){
	struct optee_call_waiter w;
	//we need to retry until secure world isn't busy
// 确定 secure world 是否 ready
	optee_cp_wait_init(&optee->call_queue,&w);
// 进入 loop 循环, 通知 secure world 执行相应操作, 知道返回 OK 后跳出
	while(true){
		struct arm_smccc_res res;
// 通知 smc 操作, 通知 secure world 执行使能共享内存 cache 的操作
		optee->invoke_fn(OPTEE_SMC_ENABLE_SHM_CACHE,0,0,0,0,0,0,&res);
		if(res.a0 == OPTEE_SMC_RETURN_OK)
			break;
		optee_cq_wait_for_completion(&optee->call_queue,&w);
	}
	optee_cq_wait_final(&optee->call_queue,&w);
}
```

## 2.11 总结
从 OP-TEE 驱动的挂载过程来看, OP-TEE 驱动会分别针对 libteec 库和 tee_supplicant 建立不同的设备 / dev/tee0 和 / dev/teepriv0. 同时为两个设备中的 des 配置各自独有的 operation 结构体变量, 并建立类似消息队列来存放正常世界状态与安全世界状态之间的请求, 这样 libteec 库和 tee_supplicant 使用 OP-TEE 驱动时就能做到相对的独立. 安全世界状态与 OPTEE 驱动之间使用共享内存进行数据交互.** 用于作为共享内存的物理内存块在 OP-TEE 启动过程中进行 MMU 初始化时需要被预留出来 **, 在 OP-TEE 驱动的挂载过程中需要将该内存块映射到系统内存中.

# 3. REE 侧用户空间对驱动的调用
在 Linux 用户空间对文件系统中的文件执行打开, 关闭, 读写以及 ioctl 操作时, 最终都会穿透到 Linux 内核空间执行具体的操作. 而从用户空间陷入到内核空间是通过系统调用 (systemcall) 来实现的(关于 syscall 的实现可自行查阅资料了解), 进入 Linux 内核空间后, 系统会调用相应的驱动来获取设备对应的 file_operations 变量, 该结构体变量中存放了对文件进行各种操作的具体函数指针. 所以从用户空间对文件进行操作时, 其整个过程大致如图所示.

调用 libteec 库中按照 GP 标准定义的 API 或 tee_supplicant 执行具体操作时都会经历图所示的流程:

<div align='center'><img src="https://raw.githubusercontent.com/carloscn/images/main/typora20221005114651.png" width="100%" alt="REE 侧用户空间调用 OP-TEE 驱动的大致流程"/></div>


# 4. OP-TEE 驱动中的重要结构体变量
要了解 OP-TEE 驱动中具体做了哪些操作, 首先需要了解在 OP-TEE 驱动中存在的四个重要的结构体,`libteec` 和 `tee_supplicant` 以及 `dma 操作使用驱动时 ` 会使用到这四个结构体, 这四个结构体变量会在驱动挂载的时候被注册到系统设备模块或者是该设备的自由结构体中以便被 userspace 使用, 而 dma 操作的时候会对共享内存进行注册.

* **tee_fops** : OP-TEE 驱动的 file_operations
* **tee_driver_ops** : OP-TEE 驱动中 `/dev/tee0` 设备的 tee_driver_ops 结构体
* **optee_supp_ops**: OP-TEE 驱动中 `/dev/teepriv0` 设备的 tee_driver_ops 结构体
* **tee_shm_dma_buf_ops** : OP-TEE 驱动中共享驱动缓存操作的 dma_buf_ops 结构体


## 4.1 OP-TEE 驱动的 file_operation 结构体变量 tee_fops

OP-TEE 驱动的 file_operation 结构体变量定义在 `linux/drivers/tee/tee_core.c` 文件中, 该变量中包含了 OP-TEE 驱动文件操作函数指针, 其内容如下:

```c
struct const struct file_operations tee_fops={
	.owner=THIS_MODULE,
	.open=tee_open,
	.release=tee_release,
	.unlocked_ioctl=tee_ioctl,// 驱动文件的 ioctl 操作具体实现的函数指针
	.compat_ioctl=tee_ioctl,// 驱动文件 ioctl 操作具体实现指针
};
```

当在 userspace 层面调用 open, release, ioctl 函数操作驱动文件时就会调用到该结构体中的对应函数去执行具体操作.

## 4.2 OP-TEE 驱动中 `/dev/tee0` 设备的 tee_driver_ops 结构体变量 optee_ops

当用户调用 libteec 中的接口的时, 操作的就是 OP-TEE 驱动的 `/dev/tee0` 设备, 而 optee_ops 变量中存放的就是针对 `/dev/tee0` 设备的具体操作函数的指针, 用户调用 libteec 接口时, 首先会调用到 tee_fops 中的成员函数, tee_fops 中的成员函数再会去调用到 optee_ops 中对应的成员函数来完成对 `/dev/tee0` 设备的实际操作. optee_ops 变量定义在 `linux/drivers/tee/optee/core.c` 文件中. 其内容如下:

```c
static struct tee_driver_ops optee_ops = {
	.get_version = optee_get_version,	// 获取 OP-TEE 版本信息的接口函数
	.open = optee_open,  // 打开 / dev/tee0 设备的具体实现, 初始化列表和互斥体, 返 context
	.release = optee_release, // 释放掉打开的 / dev/tee0 设备资源, 并通知 secure world 关闭 session
	.open_session = optee_open_session, // 打开 session, 以便 CA 于 TA 进行交互
	.close_session = optee_close_session, // 关闭已经打开的 session, 断开 CA 与 TA 之间的交互
	.invoke_func = optee_invoke_func, 	// 通过 smc 操作发送 CA 请求到对应 TA
	.cancel_req = optee_cancel_req, // 取消 CA 端已经发送的 smc 请求
};
```

## 4.3 OP-TEE 驱动中 `/dev/teepriv0` 设备的 tee_driver_ops 结构体变量 optee_supp_ops

当 tee_supplicant 需要执行相关操作时, 操作的就是 OP-TEE 驱动的 `/dev/teepriv0` 设备, 而 optee_supp_ops 变量中存放的就是针对 `/dev/teepriv0` 设备的具体操作函数的指针, tee_supplicant 执行相关操作时, 首先会调用到 tee_fops 中的成员函数, tee_fops 中的成员函数再会去调用到 optee_supp_ops 中对应的成员函数来完成对 `/dev/teepriv0` 设备的实际操作. optee_supp_ops 变量定义在 `linux/drivers/tee/optee/core.c` 文件中. 其内容如下:

```c
static struct tee_driver_ops optee_supp_ops = {
	.get_version = optee_get_version,	// 获取 OP-TEE 的版本信息
	.open = optee_open,	// 打开 / dev/teepriv0 设备的具体实现
	.release = optee_release, // 释放掉打开的 / dev/teepriv0 设备, 并通知 secure world 关闭 session
	.supp_recv = optee_supp_recv, // 接收从 OP-TEE 发送给 tee_supplicant 的请求
	.supp_send = optee_supp_send,  // 执行完 OP-TEE 请求的操作后将结果和数据发送给 OP-TEE
};
```

## 4.4 OP-TEE 驱动中共享驱动缓存操作的 dma_buf_ops 结构体变量 tee_shm_dma_buf_ops

OP-TEE 驱动也支持其他设备访问 OP-TEE 驱动的共享缓存, 该部分的功能当前并不算太完全, 有一些功能尚未实现. 该变量定义在 `linux/drivers/tee/tee_shm.c` 文件中, 当需要被分配 dma 缓存时就会调用到该变量中对应的函数. 其内容如下:

```c
static struct dma_buf_ops tee_shm_dma_buf_ops = {
	.map_dma_buf = tee_shm_op_map_dma_buf,	// 暂未实现
	.unmap_dma_buf = tee_shm_op_unmap_dma_buf,	// 暂未实现
	.release = tee_shm_op_release,	// 释放掉指定的共享内存
	.kmap_atomic = tee_shm_op_kmap_atomic, // 暂未实现
	.kmap = tee_shm_op_kmap,	// 暂未实现
	.mmap = tee_shm_op_mmap, 	//dma 共享内存进行地址映射
};
```

**NOTE**: libteec 中接口的执行和 tee_supplicant 功能的执行都会用上述四个结构体变量中的两个或者多个.

# 5. OP-TEE 驱动与 OP-TEE 共享内存的注册和分配
当 libteec 和 tee_supplicant 需要分配和注册于 secure world 之间的共享内存时, 可以通过调用驱动的 ioctl 方法来实现, 然后驱动调用 tee_ioctl_shm_alloc 来实现具体的分配, 注册共享内存的操作. 该函数的内容如下:

```c
static int tee_ioctl_shm_alloc(struct tee_context *ctx,
			       struct tee_ioctl_shm_alloc_data __user *udata)
{
	long ret;
	struct tee_ioctl_shm_alloc_data data;
	struct tee_shm *shm;

/* 将 userspace 传递的参数数据拷贝到 kernel 的 buffer 中 */
	if (copy_from_user(&data, udata, sizeof(data)))
		return -EFAULT;

	/* Currently no input flags are supported */
	if (data.flags)
		return -EINVAL;

/* 将共享内存的 ID 值设置成 - 1, 以便分配好共享内存之后重新赋值 */
	data.id = -1;

/* 调用 tee_shm_all 函数, 从驱动与 secure world 之间的共享内存池中分配对应大小的内存,
并设定对应的 ID 值 */
	shm = tee_shm_alloc(ctx, data.size, TEE_SHM_MAPPED | TEE_SHM_DMA_BUF);
	if (IS_ERR(shm))
		return PTR_ERR(shm);

/* 设定需要返回给 userspace 的数据 */
	data.id = shm->id;
	data.flags = shm->flags;
	data.size = shm->size;

/* 将需要返回的数据从 kernespace 拷贝到 userspace 层面 */
	if (copy_to_user(udata, &data, sizeof(data)))
		ret = -EFAULT;
	else
		ret = tee_shm_get_fd(shm);

	/*
	 * When user space closes the file descriptor the shared memory
	 * should be freed or if tee_shm_get_fd() failed then it will
	 * be freed immediately.
	 */
	tee_shm_put(shm); // 如果分配的是 DMA 的 buffer 则要减少 count 值
	return ret;
}
```

而在 tee_shm_all 中驱动做了什么操作呢? 该函数的内容如下:

```c
struct tee_shm *tee_shm_alloc(struct tee_context *ctx, size_t size, u32 flags)
{
	struct tee_device *teedev = ctx->teedev;
	struct tee_shm_pool_mgr *poolm = NULL;
	struct tee_shm *shm;
	void *ret;
	int rc;

/* 判定参数是否合法 */
	if (!(flags & TEE_SHM_MAPPED)) {
		dev_err(teedev->dev.parent,
			"only mapped allocations supported\n");
		return ERR_PTR(-EINVAL);
	}

/* 判定 flags, 表明该操作只能从驱动的私有共享呢村或者 DMA buffer 中进行分配 */
	if ((flags & ~(TEE_SHM_MAPPED | TEE_SHM_DMA_BUF))) {
		dev_err(teedev->dev.parent, "invalid shm flags 0x%x", flags);
		return ERR_PTR(-EINVAL);
	}

/* 获取设备结构体 */
	if (!tee_device_get(teedev))
		return ERR_PTR(-EINVAL);

/* 判定设备的 poll 成员是否存在, 该成员在驱动挂载的时候会被初始化成驱动与 secure world
之间的共享内存池结构体 */
	if (!teedev->pool) {
		/* teedev has been detached from driver */
		ret = ERR_PTR(-EINVAL);
		goto err_dev_put;
	}

/* 分配存放 shm 的 kernel 空间的内存 */
	shm = kzalloc(sizeof(*shm), GFP_KERNEL);
	if (!shm) {
		ret = ERR_PTR(-ENOMEM);
		goto err_dev_put;
	}

/* 设定该块共享内存的 flag 以及对应的使用者 */
	shm->flags = flags;
	shm->teedev = teedev;
	shm->ctx = ctx;

/* 通过传入的 flags 来判定是从驱动的私有共享内存池还是工 dma buffer 中来进行分配 */
	if (flags & TEE_SHM_DMA_BUF)
		poolm = &teedev->pool->dma_buf_mgr;
	else
		poolm = &teedev->pool->private_mgr;

/* 调用 pool 中对应的 alloc 操作分配 size 大小的共享内存 */
	rc = poolm->ops->alloc(poolm, shm, size);
	if (rc) {
		ret = ERR_PTR(rc);
		goto err_kfree;
	}

/* 获取分配好的共享内存的 id */
	mutex_lock(&teedev->mutex);
	shm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);
	mutex_unlock(&teedev->mutex);
	if (shm->id < 0) {
		ret = ERR_PTR(shm->id);
		goto err_pool_free;
	}

/* 如果指定的是 DMA buffer, 则指定相关的 operation */
	if (flags & TEE_SHM_DMA_BUF) {
		DEFINE_DMA_BUF_EXPORT_INFO(exp_info);

		exp_info.ops = &tee_shm_dma_buf_ops;
		exp_info.size = shm->size;
		exp_info.flags = O_RDWR;
		exp_info.priv = shm;

		shm->dmabuf = dma_buf_export(&exp_info);
		if (IS_ERR(shm->dmabuf)) {
			ret = ERR_CAST(shm->dmabuf);
			goto err_rem;
		}
	}

/* 将分配的内存链接到共享内存队列的末尾 */
	mutex_lock(&teedev->mutex);
	list_add_tail(&shm->link, &ctx->list_shm);
	mutex_unlock(&teedev->mutex);

	return shm;
err_rem:
	mutex_lock(&teedev->mutex);
	idr_remove(&teedev->idr, shm->id);
	mutex_unlock(&teedev->mutex);
err_pool_free:
	poolm->ops->free(poolm, shm);
err_kfree:
	kfree(shm);
err_dev_put:
	tee_device_put(teedev);
	return ret;
}
```

从整个过程来看
-   如果在 libteec 执行 ` 共享内存 ` 的分配或者是 ` 注册操作 ` 时, 驱动都会从驱动与 secure world 的共享内存池中分配一块内存, 然后将该分配好的内存的 id 值返回给 libteec, 然后在 libteec 中
-   如果是调用 `TEEC_AllocateSharedMemory` 函数, 则会将该共享内存的 id 值进行 mmap, 然后将 map 后的值赋值给 shm 中的 buffer 成员. 如果调用的是 `TEEC_RegisterSharedMemory`, 则会将共享内存 id 执行 mmap 之后的值赋值给 shm 中的 shadow_buffer 成员.

由此可见, 当 libteec 中是执行注册共享内存操作时, 并不是讲 userspace 的内存直接共享给 secure world, 而是将 userspace 的内存与驱动中分配的一块共享内存做 shadow 操作, 是两者实现一个类似映射的关系.

# 6. libteec 中的接口在驱动中的 handle
libteec 提供给上层使用的接口总共有十个, 这十个接口的功能和定义介绍请参阅[07_OPTEE-OS_系统集成之(五)REE 侧上层软件](https://github.com/carloscn/blog/issues/97), 这十个接口通过系统调用最终会调用到驱动中, 在接口 libteec 中调用 Open 函数的时候, 在驱动中就对调用到 file_operations 结构体变量 tee_fops 中的 Open 成员. 同理在 libteec 接口中调用 ioctl 函数, 则在驱动中最终会调用 tee_fops 中的 ioctl 函数. 本文将介绍驱动中 file_operation 结构体变量 `tee_fops` 中各成员函数的具体实现.

驱动挂载完成后, CA 程序通过调用 libteec 库中的接口调用 OP-TEE 驱动来穿透到 OP-TEE 中, 然后调用对应的 TA 程序. OP-TEE 驱动在挂载完成后会在 / dev 目录下分别创建两个设备节点, 分别为 `/dev/tee0` 和 `/dev/teepriv`, 对 `/dev/tee0` 设备进行相关操作就能够穿透到 OP-TEE 中实现特定请求的发送.

## 6.1 tee_open
tee_core:
https://github.com/carloscn/raspi-linux/blob/master/drivers/tee/tee_core.c#L109

调用栈如下:
```bash
libteec(tee_supplicant)
    -> linux kernel_space
        -> find tee_open() (tee_core.c)
            -> call teedev_open() (tee_core.c)
	        -> call optee_open() (optee/core.c)
```

在 libteec 中调用 open 函数来打开 `/dev/tee0` 设备的时候, 最终会调用到 tee_fops 中的 open 成员指定的函数指针 tee_open, 该函数的内容如下:

```c
static int tee_open(struct inode*inode,struct file*filp){
	struct tee_context *ctx;
// 调用 container_of 函数, 获取设备的 tee_device 变量的内容 . 该变量对于 / dev/tee0
// 和 / dev/teepriv0 设备时不一样的, 这点可以在驱动过载的过程中查阅
	ctx=teedev_open(container_of(inode->icdev,struct tee_device,cdev));
	if(IS_ERR(ctx)){
		return PTR_ERR(ctx);
	}
	filp->private_data=ctx;
	return 0;
}

static struct tee_context *teedev_open(struct tee_device* teedev){
	int rc;
	struct tee_context* ctx;
// 标记该设备的使用者加一
	if(!tee_device_get(teedev))
		return ERR_PTR(-EINVAL);
// 分配 tee_context 结构体空间
	ctx=kzalloc(sizeof(*ctx),GFP_KERNEL);
	if(!ctx){
		rc=-ENOMEM;
		goto err;
	}
// 将 tee_context 结构体汇中 teedev 变量赋值
	ctx->teedev=teedev;
	INIT_LIST_HEAD(&ctx->list_shm);
// 调用设备的 des 中的 open 执行设备几倍的 open 操作
	rc=teedev->desc->ops->open(ctx);
	if(rc)
		goto err;
	return ctx;
err:
	kfree(ctx);
	tee_device_put(teedev);
	return ERR_PTR(rc);
}
```

对于设备级别(`/dev/tee0` 和 `/dev/teepriv0`), 最终会调用到 optee_open 函数, 该函数内容如下: https://github.com/carloscn/raspi-linux/blob/master/drivers/tee/optee/core.c#L220

```c
static int optee_open(struct tee_context*ctx){
	struct optee_context_data* ctxdata;
	struct tee_device*teedev=ctx->teedev;
	struct optee* optee=tee_get_drvdata(teedev);
// 分配 optee_context_data 结构体变量空间
	ctxdata=kzalloc(sizeof(*ctxdata),GFP_KERNEL);
	if(!ctxdata)
		return -ENOMEM;
// 通过 teedev 的值是否为 optee->supp_teedev, 以此来判定当前的 Open 操作是打
// 开 / dev/tee0 设备还是 / dev/teepriv0 设备, 如果相等, 则表示当前是打开 / dev/teepriv0 设备
	if(teedev == optee->supp_teedev){
		bool busy=true;// 标记 / dev/teepriv0 正在使用
		mutex_lock(&optee->supp.mutex);
		if(!optee->supp.ctx){
			busy=false;
			optee->supp.ctx=ctx;
		}
		mutex_unlock(&optee->supp.mutex);
		if(busy){
			kfree(ctxdata);
			return -EBUSY;
		}
	}
// 初始化互斥提队列
	mutex_init(&ctxdata->mutex);
	INIT_LIST_HEAD(&ctxdata->sess_list);
// 赋值
	ctx->data=ctxdata;
	return 0;
}
```

相应的 release 的操作也是一样.

## 6.2 获取版本

在 libteec 中获取 OP-TEE 版本信息, 创建和关闭 session, 调用 TA, 分配和注册共享内存和 fd 以及释放共享内存,** 接收来自 OP-TEE 的请求以及回复数据给 OP-TEE 都是通过 ioctl 来完成的 **. 在 libteec 和 tee_supplicant 中通过带上对应的参数调用 ioctl 函数来实现对应的操作需求, 最终会调用到 OP-TEE 驱动中 file_operation 结构体变量 tee_fops 变量中的 tee_ioctl 函数, 该函数的内容如下.
```C
static long tee_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
	struct tee_context *ctx = filp->private_data;	// 获取设备的私有数据
	void __user *uarg = (void __user *)arg; 	// 定义指向 userspace 层面输入参数的指针变量

	switch (cmd) {
/* 和获取 OP-TEE OS 的版本 */
	case TEE_IOC_VERSION:
		return tee_ioctl_version(ctx, uarg);

/* 分配, 注册, 释放共享内存操作 */
	case TEE_IOC_SHM_ALLOC:
		return tee_ioctl_shm_alloc(ctx, uarg);

/* 注册共享文件句柄 */
	case TEE_IOC_SHM_REGISTER_FD:
		return tee_ioctl_shm_register_fd(ctx, uarg);

/* 打开 CA 与 TA 通信的 session */
	case TEE_IOC_OPEN_SESSION:
		return tee_ioctl_open_session(ctx, uarg);

/* 调用 TA, 让根据参数中的 command ID 让 TA 执行特定的 command */
	case TEE_IOC_INVOKE:
		return tee_ioctl_invoke(ctx, uarg);

/* 通知 TA 取消掉对某一个来自 normal world 请求的响应 */
	case TEE_IOC_CANCEL:
		return tee_ioctl_cancel(ctx, uarg);

/* 关闭 CA 与 TA 之间通信的 session */
	case TEE_IOC_CLOSE_SESSION:
		return tee_ioctl_close_session(ctx, uarg);

/* 接收来自 secure world 的请求 */
	case TEE_IOC_SUPPL_RECV:
		return tee_ioctl_supp_recv(ctx, uarg);

/* 根据来自 secure world 的请求执行处理后回复数据给 secure world */
	case TEE_IOC_SUPPL_SEND:
		return tee_ioctl_supp_send(ctx, uarg);
	default:
		return -EINVAL;
	}
}
```

当 libteec 和 tee_supplicant 调用 ioctl 来获取 OP-TEE OS 的版本信息时, 驱动会执行 tee_ioctl_version 函数, 该函数内容如下:

```C
static int tee_ioctl_version(struct tee_context *ctx,
			     struct tee_ioctl_version_data __user *uvers)
{
	struct tee_ioctl_version_data vers;

/* 调用设备的 get_version 操作 */
	ctx->teedev->desc->ops->get_version(ctx->teedev, &vers);

/* 判定该操作是来至于 tee_supplicant 还是 libteec */
	if (ctx->teedev->desc->flags & TEE_DESC_PRIVILEGED)
		vers.gen_caps |= TEE_GEN_CAP_PRIVILEGED;

/* 将获取到的版本信息数据拷贝到 userspace 层面提供的 buffer 中 */
	if (copy_to_user(uvers, &vers, sizeof(vers)))
		return -EFAULT;

	return 0;
}
```

而设备的 `get_version` 的内容如下:

```C
static void optee_get_version(struct tee_device *teedev,
			      struct tee_ioctl_version_data *vers)
{
	struct tee_ioctl_version_data v = {
		.impl_id = TEE_IMPL_ID_OPTEE,
		.impl_caps = TEE_OPTEE_CAP_TZ,
		.gen_caps = TEE_GEN_CAP_GP,
	};
	*vers = v;
}
```

## 6.3 libteec 中 open session 的操作
当用户调用 libteec 中的 TEEC_OpenSession 接口时会执行驱动中 ioctl 函数的 TEE_IOC_OPEN_SESSION 分支去执行 tee_ioctl_open_session 函数, 该函数只会在打开了 `/dev/tee0` 设备之后才能被使用, 其的内容如下:

```C
static int tee_ioctl_open_session(struct tee_context *ctx,
				  struct tee_ioctl_buf_data __user *ubuf)
{
	int rc;
	size_t n;
	struct tee_ioctl_buf_data buf;
	struct tee_ioctl_open_session_arg __user *uarg;
	struct tee_ioctl_open_session_arg arg;
	struct tee_ioctl_param __user *uparams = NULL;
	struct tee_param *params = NULL;
	bool have_session = false;

/* 判定设备是否存在 open_session 函数 */
	if (!ctx->teedev->desc->ops->open_session)
		return -EINVAL;

/* 将 userspace 传入的参数拷贝到 kernelspace */
	if (copy_from_user(&buf, ubuf, sizeof(buf)))
		return -EFAULT;

/* 判定传入的参数的大小是否合法 */
	if (buf.buf_len > TEE_MAX_ARG_SIZE ||
	    buf.buf_len < sizeof(struct tee_ioctl_open_session_arg))
		return -EINVAL;

/* 为兼容性 64 位系统做出转换, 并将数据拷贝到 arg 变量中 */
	uarg = u64_to_user_ptr(buf.buf_ptr);
	if (copy_from_user(&arg, uarg, sizeof(arg)))
		return -EFAULT;

	if (sizeof(arg) + TEE_IOCTL_PARAM_SIZE(arg.num_params) != buf.buf_len)
		return -EINVAL;

/* 判定 userspace 层面传递的参数的个数并在 kernelspace 中分配对应的空间, 将该空间的起
始地址保存在 params, 然后 userspace 中的参数数据存放到 params 中 */
	if (arg.num_params) {
		params = kcalloc(arg.num_params, sizeof(struct tee_param),
				 GFP_KERNEL);
		if (!params)
			return -ENOMEM;
		uparams = uarg->params;

 /* 将来自 userspace 的参数数据保存到刚刚分配的 params 指向的内存中 */
		rc = params_from_user(ctx, params, arg.num_params, uparams);
		if (rc)
			goto out;
	}

/* 调用设备的 open_session 方法, 并将参数传递給该函数  */
	rc = ctx->teedev->desc->ops->open_session(ctx, &arg, params);
	if (rc)
		goto out;
	have_session = true;

	if (put_user(arg.session, &uarg->session) ||
	    put_user(arg.ret, &uarg->ret) ||
	    put_user(arg.ret_origin, &uarg->ret_origin)) {
		rc = -EFAULT;
		goto out;
	}
/* 将从 secure world 中返回的数据保存到 userspace 对应的 buffer 中 */
	rc = params_to_user(uparams, arg.num_params, params);
out:
	/*
	 * If we've succeeded to open the session but failed to communicate
	 * it back to user space, close the session again to avoid leakage.
	 */
/* 如果调用不成功则执行 close session 操作 */
	if (rc && have_session && ctx->teedev->desc->ops->close_session)

	if (params) {
		/* Decrease ref count for all valid shared memory pointers */
		for (n = 0; n < arg.num_params; n++)
			if (tee_param_is_memref(params + n) &&
			    params[n].u.memref.shm)
				tee_shm_put(params[n].u.memref.shm);
		kfree(params);
	}

	return rc;
}
```

调用设备的 open_session 操作来完成向 OP-TEE 发送打开与特定 TA 的 session 操作, open_session 函数的执行流程如下图所示:

<div align='center'><img src="https://raw.githubusercontent.com/carloscn/images/main/typora20221005122759.png" width="80%" /></div>

整个调用中会调用 `optee_do_call_with_arg` 函数来完成驱动与 secure world 之间的交互, 该函数的内容如下:

```C
u32 optee_do_call_with_arg(struct tee_context *ctx, phys_addr_t parg)
{
	struct optee *optee = tee_get_drvdata(ctx->teedev);
	struct optee_call_waiter w;
	struct optee_rpc_param param = { };
	u32 ret;

/* 设定触发 smc 操作的第一个参数 a0 的值为 OPTEE_SMC_CALL_WITH_ARG
    通过 OPTEE_SMC_CALL_WITH_ARG 值可以知道, 该函数将会执行 std 的 smc 操作 */
	param.a0 = OPTEE_SMC_CALL_WITH_ARG;
	reg_pair_from_64(¶m.a1, ¶m.a2, parg);
	/* Initialize waiter */
/* 初始化调用的等待队列 */
	optee_cq_wait_init(&optee->call_queue, &w);

/* 进入到 loop 循环, 触发 smc 操作并等待 secure world 的返回 */
	while (true) {
		struct arm_smccc_res res;

/* 触发 smc 操作 */
		optee->invoke_fn(param.a0, param.a1, param.a2, param.a3,
				 param.a4, param.a5, param.a6, param.a7,
				 &res);

/* 判定 secure world 是否超时, 如果超时, 完成一次啊调用, 进入下一次循环
    知道 secure world 端完成 open session 请求 */
		if (res.a0 == OPTEE_SMC_RETURN_ETHREAD_LIMIT) {
			/*
			 * Out of threads in secure world, wait for a thread
			 * become available.
			 */
			optee_cq_wait_for_completion(&optee->call_queue, &w);
		} else if (OPTEE_SMC_RETURN_IS_RPC(res.a0)) {
/* 处理 rpc 操作 */
			param.a0 = res.a0;
			param.a1 = res.a1;
			param.a2 = res.a2;
			param.a3 = res.a3;
			optee_handle_rpc(ctx, ¶m);
		} else {
/* 创建 session 完成之后跳出 loop, 并返回 a0 的值 */
			ret = res.a0;
			break;
		}
	}

	/*
	 * We're done with our thread in secure world, if there's any
	 * thread waiters wake up one.
	 */
/* 执行等待队列最后完成操作 */
	optee_cq_wait_final(&optee->call_queue, &w);

	return ret;
}
```

## 6.4 invoke 操作
当完成了 session 打开之后, 用户就可以调用 TEEC_InvokeCommand 接口来调用对应的 TA 中执行特定的操作了, TEEC_InvokeCommand 函数最终会调用驱动的 tee_ioctl_invoke 函数来完成具体的操作. 该函数内如如下:
```c
static int tee_ioctl_invoke(struct tee_context *ctx,
			    struct tee_ioctl_buf_data __user *ubuf)
{
	int rc;
	size_t n;
	struct tee_ioctl_buf_data buf;
	struct tee_ioctl_invoke_arg __user *uarg;
	struct tee_ioctl_invoke_arg arg;
	struct tee_ioctl_param __user *uparams = NULL;
	struct tee_param *params = NULL;

/* 参数检查 */
	if (!ctx->teedev->desc->ops->invoke_func)
		return -EINVAL;

/* 数据拷贝到 kernel space */
	if (copy_from_user(&buf, ubuf, sizeof(buf)))
		return -EFAULT;

	if (buf.buf_len > TEE_MAX_ARG_SIZE ||
	    buf.buf_len < sizeof(struct tee_ioctl_invoke_arg))
		return -EINVAL;

	uarg = u64_to_user_ptr(buf.buf_ptr);
	if (copy_from_user(&arg, uarg, sizeof(arg)))
		return -EFAULT;

	if (sizeof(arg) + TEE_IOCTL_PARAM_SIZE(arg.num_params) != buf.buf_len)
		return -EINVAL;

/* 组合需要传递到 secure world 中的参数 buffer */
	if (arg.num_params) {
		params = kcalloc(arg.num_params, sizeof(struct tee_param),
				 GFP_KERNEL);
		if (!params)
			return -ENOMEM;
		uparams = uarg->params;
		rc = params_from_user(ctx, params, arg.num_params, uparams);
		if (rc)
			goto out;
	}

/* 使用对应的 session 出发 smc 操作 */
	rc = ctx->teedev->desc->ops->invoke_func(ctx, &arg, params);
	if (rc)
		goto out;

/* 检查和解析返回的数据, 并将数据拷贝到 userspace 用户体用的 Buffser 中 */
	if (put_user(arg.ret, &uarg->ret) ||
	    put_user(arg.ret_origin, &uarg->ret_origin)) {
		rc = -EFAULT;
		goto out;
	}
	rc = params_to_user(uparams, arg.num_params, params);
out:
	if (params) {
		/* Decrease ref count for all valid shared memory pointers */
		for (n = 0; n < arg.num_params; n++)
			if (tee_param_is_memref(params + n) &&
			    params[n].u.memref.shm)
				tee_shm_put(params[n].u.memref.shm);
		kfree(params);
	}
	return rc;
}
```


# 7. tee_supplicant 接口在驱动的实现

在 [07_OPTEE-OS_系统集成之(五)REE 侧上层软件](https://github.com/carloscn/blog/issues/97) 一文中介绍了 tee_supplicant 主要作用, 用来实现 `secure world 端操作 REE 侧文件系统 `,`EMMC 的 rpmb 分区 `,` 网络 socket 操作 `,` 数据库操作 ` 的需求.

tee_supplicant 与 OP-TEE 之间的交互模式类似于生产者与消费者的关系. 完成上述需求的整个过程包含驱动接收来自 OP-TEE 的请求, tee_supplicant 从驱动中获取 OP-TEE 的请求并处理, 驱动返回请求操作结果给 OP-TEE 三部分.

![](https://raw.githubusercontent.com/carloscn/images/main/typora20221005124228.png)

当 libtee 调用驱动来与 OP-TEE 进行数据的交互的时候, 最终会调用 `optee_do_call_with_arg` 函数完成完成 smc 的操作, 而该函数中有一个 loop 循环, 每次触发 smc 操作之后会对从 secure world 中返回的参数 `res.a0` 进行判断, 判定当前从 secure world 返回的数据是 ** 要执行 RPC 操作 ** 还是直接 ** 返回到 CA**.

如果是来自 TEE 的 RPC 请求, 则会将请求存放到请求队列 req 中. 然后 block 住, 直到 tee_supplicant 处理完请求并将 `req->c` 标记为完成之后才会进入下一个 loop, 重新出发 smc 操作, 将处理结果返回给 TEE.

![](https://raw.githubusercontent.com/carloscn/images/main/typora20221005125408.png)


## 7.1 driver 获取来自 TEE 侧的请求

当 libteec 调用了需要做 smc 操作的请求之后, 最终会调用到驱动的 `optee_do_call_with_arg` 函数, 该函数会进入到死循环, 第一条语句就会调用 smc 操作, 进 userspace 的请求发送到 secure world, 待从 secure world 中返回之后. 会对返回值进行判定. 如果返回的 `res.a0` 参数是需要驱动做 RPC 操作, 则该函数会调用到 `optee_handle_rpc` 操作. 经过各种参数分析和函数调用之后, 程序最后会调用 `optee_supp_thrd_req` 函数来将来自 TEE 的请求存放到 tee_supplicant 的请求队列中. 该函数的内容如下:

```C
u32 optee_supp_thrd_req(struct tee_context *ctx, u32 func, size_t num_params,
			struct tee_param *param)

{
	struct optee *optee = tee_get_drvdata(ctx->teedev);
	struct optee_supp *supp = &optee->supp;
	struct optee_supp_req *req = kzalloc(sizeof(*req), GFP_KERNEL);
	bool interruptable;
	u32 ret;

	if (!req)
		return TEEC_ERROR_OUT_OF_MEMORY;

/* 初始化该请求消息的 c 成员并配置请求数据 */
	init_completion(&req->c);
	req->func = func;
	req->num_params = num_params;
	req->param = param;

	/* Insert the request in the request list */
/* 将接受到的请求添加到驱动的 TEE 请求消息队列中 */
	mutex_lock(&supp->mutex);
	list_add_tail(&req->link, &supp->reqs);
	mutex_unlock(&supp->mutex);

	/* Tell an eventual waiter there's a new request */
/* 将 supp->reqs_c 置位, 通知 tee_supplicant 的 receve 操作, 当前驱动中
   有一个来自 TEE 的请求 */
	complete(&supp->reqs_c);

	/*
	 * Wait for supplicant to process and return result, once we've
	 * returned from wait_for_completion(&req->c) successfully we have
	 * exclusive access again.
	 */
/* block 在这里, 通过判定 req->c 是否被置位来判定当前请求是否被处理完毕,
    而 req->c 的置位是有 tee_supplicant 的 send 调用来完成的, 如果被置位, 则进入到
   while 循环中进行返回值的设定并跳出 while*/
	while (wait_for_completion_interruptible(&req->c)) {
		mutex_lock(&supp->mutex);
		interruptable = !supp->ctx;
		if (interruptable) {
			/*
			 * There's no supplicant available and since the
			 * supp->mutex currently is held none can
			 * become available until the mutex released
			 * again.
			 *
			 * Interrupting an RPC to supplicant is only
			 * allowed as a way of slightly improving the user
			 * experience in case the supplicant hasn't been
			 * started yet. During normal operation the supplicant
			 * will serve all requests in a timely manner and
			 * interrupting then wouldn't make sense.
			 */
			interruptable = !req->busy;
			if (!req->busy)
				list_del(&req->link);
		}
		mutex_unlock(&supp->mutex);

		if (interruptable) {
			req->ret = TEEC_ERROR_COMMUNICATION;
			break;
		}
	}

	ret = req->ret;
	kfree(req);

	return ret;
}
```

当请求被处理完成之后, 函数返回处理后的数据到 optee_do_call_with_arg 函数中, 并进入 optee_do_call_with_arg 函数中 while 中的下一次循环, 将处理结果返回给 secure world.

## 7.2 tee_supplicant 从驱动中获取 TEE 侧的请求
在 tee_supplicant 会调用 read_request 函数来从驱动的请求队列中获取当前存在的来自 TEE 的请求. 该函数最终会调用到驱动中的 optee_supp_recv 函数. 该函数的内容如下:
```C
int optee_supp_recv(struct tee_context *ctx, u32 *func, u32 *num_params,
		    struct tee_param *param)
{
	struct tee_device *teedev = ctx->teedev;
	struct optee *optee = tee_get_drvdata(teedev);
	struct optee_supp *supp = &optee->supp;
	struct optee_supp_req *req = NULL;
	int id;
	size_t num_meta;
	int rc;

/* 对被用来存放 TEE 请求参数的数据的 buffer 进行检查 */
	rc = supp_check_recv_params(*num_params, param, &num_meta);
	if (rc)
		return rc;

/* 进入到 loop 循环中, 从 驱动的请求消息队列中获取来自 TEE 中的请求, 直到获取之后才会
跳出该 loop*/
	while (true) {
		mutex_lock(&supp->mutex);
/* 尝试从驱动的请求消息队列中获取来自 TEE 的一条请求 */
		req = supp_pop_entry(supp, *num_params - num_meta, &id);
		mutex_unlock(&supp->mutex);

/* 判定是否获取到请求如果获取到了则跳出该 loop */
		if (req) {
			if (IS_ERR(req))
				return PTR_ERR(req);
			break;
		}

		/*
		 * If we didn't get a request we'll block in
		 * wait_for_completion() to avoid needless spinning.
		 *
		 * This is where supplicant will be hanging most of
		 * the time, let's make this interruptable so we
		 * can easily restart supplicant if needed.
		 */
/* block 在这里, 直到在 optee_supp_thrd_req 函数中发送了
complete(&supp->reqs_c)操作后才继续往下执行 */
		if (wait_for_completion_interruptible(&supp->reqs_c))
			return -ERESTARTSYS;
	}

/* 设定参数进行异步处理请求的条件 */
	if (num_meta) {
		/*
		 * tee-supplicant support meta parameters -> requsts can be
		 * processed asynchronously.
		 */
		param->attr = TEE_IOCTL_PARAM_ATTR_TYPE_VALUE_INOUT |
			      TEE_IOCTL_PARAM_ATTR_META;
		param->u.value.a = id;
		param->u.value.b = 0;
		param->u.value.c = 0;
	} else {
		mutex_lock(&supp->mutex);
		supp->req_id = id;
		mutex_unlock(&supp->mutex);
	}

/* 解析参数, 设定 tee_supplicant 将要执行的具体(加载 TA, 操作文件系统, 操作 EMMC 的
rpmb 分区等)操作和相关参数 */
	*func = req->func;
	*num_params = req->num_params + num_meta;
	memcpy(param + num_meta, req->param,
	       sizeof(struct tee_param) * req->num_params);

	return 0;
}
```

从请求消息队列中获取到来自 TEE 的请求之后, 返回到 tee_supplicant 中继续执行. 根据返回的 func 值和参数执行 TEE 要求在 REE 端需要的操作

## 7.3 驱动返回请求操作的结果给 TEE 侧

当 tee_supplicant 执行完 TEE 请求的操作之后, 会调用 write_response 函数来实现将数据返回给 TEE. 而 write_response 函数最终会调用到驱动的 optee_supp_send 函数. 该函数主要是调用 `complete(&req->c);` 操作来完成对该请求的 c 成员的置位, 告诉 optee_supp_thrd_req 函数执行下一步操作, 返回到 optee_do_call_with_arg 函数中进入该函数中的下一轮 loop 中, 调用 smc 操作将结果返回给 TEE 侧. optee_supp_send 函数的内容如下:

```C
int optee_supp_send(struct tee_context *ctx, u32 ret, u32 num_params,
		    struct tee_param *param)
{
	struct tee_device *teedev = ctx->teedev;
	struct optee *optee = tee_get_drvdata(teedev);
	struct optee_supp *supp = &optee->supp;
	struct optee_supp_req *req;
	size_t n;
	size_t num_meta;

	mutex_lock(&supp->mutex);
/* 驱动中请求队列的 pop 操作 */
	req = supp_pop_req(supp, num_params, param, &num_meta);
	mutex_unlock(&supp->mutex);

	if (IS_ERR(req)) {
		/* Something is wrong, let supplicant restart. */
		return PTR_ERR(req);
	}

	/* Update out and in/out parameters */
/* 使用传入的参数, 更新请求的参数区域, 将需要返回给 TEE 侧的数据填入到对应的位置 */
	for (n = 0; n < req->num_params; n++) {
		struct tee_param *p = req->param + n;

		switch (p->attr & TEE_IOCTL_PARAM_ATTR_TYPE_MASK) {
		case TEE_IOCTL_PARAM_ATTR_TYPE_VALUE_OUTPUT:
		case TEE_IOCTL_PARAM_ATTR_TYPE_VALUE_INOUT:
			p->u.value.a = param[n + num_meta].u.value.a;
			p->u.value.b = param[n + num_meta].u.value.b;
			p->u.value.c = param[n + num_meta].u.value.c;
			break;
		case TEE_IOCTL_PARAM_ATTR_TYPE_MEMREF_OUTPUT:
		case TEE_IOCTL_PARAM_ATTR_TYPE_MEMREF_INOUT:
			p->u.memref.size = param[n + num_meta].u.memref.size;
			break;
		default:
			break;
		}
	}
	req->ret = ret;

	/* Let the requesting thread continue */
/* 通知 optee_supp_thrd_req 函数, 一个来自 TEE 侧的请求已经被处理完毕,
可以继续往下执行 */
	complete(&req->c);
	return 0;
}
```

## 7.4 总结

从 tee_supplicant 处理来自 TEE 侧的请求来看主要是有三个点:
-   第一是驱动在触发 smc 操作之后会进入到 loop 循环中, 根据 secure world 中的返回值来判定该返回时来自 TEE 的 RPC 请求还是最终处理结果, 如果是 RPC 请求, 也就是需要驱动或者 tee_supplicant 执行其他操作, 驱动将 RPC 请求会保存到驱动的请求消息队列中, 然后 block 住等待请求处理结果.
-   第二是在 tee_supplicant 作为一个常驻进程存在于 REE 中, 它会不停的尝试从驱动的请求消息队列中获取到来自 TEE 侧的请求. 如果请求消息队列中并没有请求则会 block 住, 直到拿到了请求才返回. 拿到请求之后会对请求进行解析, 然后根据 func 执行具体的操作.
-   第三是在 tee_supplicant 处理完来自 TEE 的请求后, 会调用 send 操作将处理结果存放到该消息队列的参数区域, 并使用 complete 函数通知驱动该请求已经被处理完毕. 驱动 block 住的地方可以继续往下执行, 调用 smc 操作将结果返回给 TEE 侧.

[^1][^2][^3]
# Ref
[^1]:[linux 内核中的 subsys_initcall 是干什么的?](https://www.bbsmax.com/A/n2d9KQooJD/)
[^2]:[01-OP-TEE 驱动篇 ---- 驱动编译, 加载和初始化(一).md](https://github.com/BabyMelvin/Aidroid/blob/bd5d1439c2fadbdf4a59721c7ebb7f8d1838580f/fingerprint/op-tee/02-driver/01-OP-TEE%E9%A9%B1%E5%8A%A8%E7%AF%87----%E9%A9%B1%E5%8A%A8%E7%BC%96%E8%AF%91%EF%BC%8C%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%88%E4%B8%80%EF%BC%89.md)
[^3]:[SMC CALLING CONVENTION  System Software on ARM Platforms.pdf](https://github.com/carloscn/doclib/blob/master/man/arm/standard/SMC%20CALLING%20CONVENTION%20%20System%20Software%20on%20ARM%20Platforms.pdf)
