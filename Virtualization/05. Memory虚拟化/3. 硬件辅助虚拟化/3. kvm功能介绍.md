
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 概述](#1-概述)
- [2. QEMU 的主要工作](#2-qemu-的主要工作)
- [3. QEMU 和 KVM 的工作分界](#3-qemu-和-kvm-的工作分界)
- [4. QEMU 中物理内存的注册](#4-qemu-中物理内存的注册)
- [5. 处理用户态虚拟地址](#5-处理用户态虚拟地址)
- [6. KVM 同步脏页位图到 Qemu](#6-kvm-同步脏页位图到-qemu)
- [7. EPT 页表映射](#7-ept-页表映射)

<!-- /code_chunk_output -->

# 1. 概述

在**虚拟机启动**时, 由 qemu 在**qemu 的进程地址空间申请内存**, 即**内存的申请**是在**用户空间**完成的, 申请的是**主机虚拟地址空间**, 而这个空间就作为**虚拟机物理内存**.

通过**kvm 提供的 API**, 把**地址信息注册到 KVM**中, 这样 KVM 中维护有**虚拟机相关的 slot**, **所有这些 slot**构成一个**完整的虚拟机物理地址空间**. slot 中记录其对应的 HVA、页面数、起始 GPA 等, 利用它可以将一个 GPA 转化为 HVA.

整个内存虚拟化可以分为两部分: qemu 部分和 kvm 部分.

- **QEMU**完成**内存的申请**
- **KVM**实现**内存的管理**

# 2. QEMU 的主要工作

查看 QEMU/memory

# 3. QEMU 和 KVM 的工作分界

QEMU 和 KVM 之间是通过 KVM 提供的`ioctl()`接口进行交互的.

# 4. QEMU 中物理内存的注册

通过`kvm_vm_ioctl(KVM_SET_USER_MEMORY_REGION)`实现, 这个 ioctl 主要目的就是设置`GPA->HVA`的映射关系

本质是创建并填充了一个临时 kvm_memslots 结构, 并把其赋值给 kvm->memslots(全局的).

# 5. 处理用户态虚拟地址

https://blog.csdn.net/jinzhuojun/article/details/8147463

# 6. KVM 同步脏页位图到 Qemu

https://frankjkl.github.io/2019/04/07/QemuKVM-Qemu%E5%90%8C%E6%AD%A5KVM%E8%84%8F%E9%A1%B5%E4%BD%8D%E5%9B%BE/

脏页位图: http://www.oenhan.com/linux-cache-writeback

插播 qemu 对内存条的模拟管理, 是通过`RAMBlock`和`ram_list`管理的, **RAMBlock**就是**每次申请的内存池**, `ram_list`则是 RAMBlock 的**链表**, 他们结构如下:

# 7. EPT 页表映射

内存的添加说完了, 看一下[EPT 页表](http://www.oenhan.com/kernel-program-exec)的映射, 在`kvm_arch_vcpu_setup`中有`kvm_mmu_setup`, 是 mmu 的初始化, EPT 的初始化是`init_kvm_tdp_mmu`, 所谓的初始化就是填充了`vcpu->arch.mmu`结构体, 里面有很多回调函数都会用到, 最终的是`tdp_page_fault`.

```cpp
context->page_fault = tdp_page_fault;
context->sync_page = nonpaging_sync_page;
context->invlpg = nonpaging_invlpg;
context->update_pte = nonpaging_update_pte;
context->shadow_root_level = kvm_x86_ops->get_tdp_level();
context->root_hpa = INVALID_PAGE;
context->direct_map = true;
context->set_cr3 = kvm_x86_ops->set_tdp_cr3;
context->get_cr3 = get_cr3;
context->get_pdptr = kvm_pdptr_read;
context->inject_page_fault = kvm_inject_page_fault;
```

当 guest 访问物理内存时发生 vm-exit, 进入 `vmx_handle_exit` 函数, 根据 `EXIT_REASON_EPT_VIOLATION` 走到 `handle_ept_violation` 函数, `exit_qualification = vmcs_readl(EXIT_QUALIFICATION)` 获取 vm-exit 的退出原因, 进入 kvm_mmu_page_fault 函数: vcpu->arch.mmu.page_fault(vcpu, cr2, error_code, false), 即是 tdp_page_fault, handle_mmio_page_fault 的流程不提.

```cpp
//填充 kvm mmu 专用的 slab
r = mmu_topup_memory_caches(vcpu);
//获取 gfn 使用的 level, 即 hugepage 的问题
force_pt_level = mapping_level_dirty_bitmap(vcpu, gfn);
if (likely(!force_pt_level)) {
    level = mapping_level(vcpu, gfn);
    gfn &= ~(KVM_PAGES_PER_HPAGE(level) - 1);
} else
    level = PT_PAGE_TABLE_LEVEL;

//顾名思义, 快速处理一个简单的 page fault
//即 present 同时有写权限的非 mmio page fault
//参考 page_fault_can_be_fast 函数
//一部分处理没有写权限的 page fault
//一部分处理 TLB lazy
//fast_pf_fix_direct_spte 也就是将 pte 获取的写权限
if (fast_page_fault(vcpu, gpa, level, error_code))
    return 0;
//下面函数主要就一件事情, gfn_to_pfn
if (try_async_pf(vcpu, prefault, gfn, gpa, &pfn, write, &map_writable))
      return 0;
//direct map 就是映射 ept 页表的过程
r = __direct_map(vcpu, gpa, write, map_writable,
      level, gfn, pfn, prefault);
```

在 try_async_pf 中就是 gfn 转换成 hva, 然后 hva 转换成 pfn 的过程, gfn 转换到 hva:

```cpp
static pfn_t
__gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,
             bool *async, bool write_fault, bool *writable)
{
    unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);

    if (addr == KVM_HVA_ERR_RO_BAD)
        return KVM_PFN_ERR_RO_FAULT;

    if (kvm_is_error_hva(addr))
        return KVM_PFN_NOSLOT;

    /* Do not map writable pfn in the readonly memslot. */
    if (writable && memslot_is_readonly(slot)) {
        *writable = false;
        writable = NULL;
    }

    return hva_to_pfn(addr, atomic, async, write_fault,
              writable);
}
```

gfn2hva 本质就是

```cpp
static inline unsigned long
__gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn)
{
    return slot->userspace_addr + (gfn - slot->base_gfn) * PAGE_SIZE;
}
```

而 hva_to_pfn 则就是 host 的线性区进行地址转换的问题了, 不提.

```cpp
static int __direct_map(struct kvm_vcpu *vcpu, gpa_t v, int write,
            int map_writable, int level, gfn_t gfn, pfn_t pfn,
            bool prefault)
{
    struct kvm_shadow_walk_iterator iterator;
    struct kvm_mmu_page *sp;
    int emulate = 0;
    gfn_t pseudo_gfn;

    if (!VALID_PAGE(vcpu->arch.mmu.root_hpa))
        return 0;
//遍历 ept 四级页表
    for_each_shadow_entry(vcpu, (u64)gfn << PAGE_SHIFT, iterator) {
//如果是最后一级, level 是 hugepage 下的 level
        if (iterator.level == level) {
//设置 pte, 页表下一级的 page 地址就是 pfn 写入到 pte
            mmu_set_spte(vcpu, iterator.sptep, ACC_ALL,
                     write, &emulate, level, gfn, pfn,
                     prefault, map_writable);
            direct_pte_prefetch(vcpu, iterator.sptep);
            ++vcpu->stat.pf_fixed;
            break;
        }

        drop_large_spte(vcpu, iterator.sptep);
//mmu page 不在位的情况, 也就是缺页
        if (!is_shadow_present_pte(*iterator.sptep)) {
            u64 base_addr = iterator.addr;
//获取指向的具体 mmu page entry 的 index
            base_addr &= PT64_LVL_ADDR_MASK(iterator.level);
            pseudo_gfn = base_addr >> PAGE_SHIFT;
//获取 mmu page
            sp = kvm_mmu_get_page(vcpu, pseudo_gfn, iterator.addr,
                          iterator.level - 1,
                          1, ACC_ALL, iterator.sptep);
//将当前的 mmu page 的地址写入到上一级别 mmu page 的 pte 中
            link_shadow_page(iterator.sptep, sp, true);
        }
    }
    return emulate;
}

static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
                         gfn_t gfn,
                         gva_t gaddr,
                         unsigned level,
                         int direct,
                         unsigned access,
                         u64 *parent_pte)
{
    union kvm_mmu_page_role role;
    unsigned quadrant;
    struct kvm_mmu_page *sp;
    bool need_sync = false;

    role = vcpu->arch.mmu.base_role;
    role.level = level;
    role.direct = direct;
    if (role.direct)
        role.cr4_pae = 0;
    role.access = access;
    if (!vcpu->arch.mmu.direct_map
        && vcpu->arch.mmu.root_level <= PT32_ROOT_LEVEL) {
        quadrant = gaddr >> (PAGE_SHIFT + (PT64_PT_BITS * level));
        quadrant &= (1 << ((PT32_PT_BITS - PT64_PT_BITS) * level)) - 1;
        role.quadrant = quadrant;
    }
//根据一个 hash 索引来的
    for_each_gfn_sp(vcpu->kvm, sp, gfn) {
//检查整个 mmu ept 是否被失效了
        if (is_obsolete_sp(vcpu->kvm, sp))
            continue;

        if (!need_sync && sp->unsync)
            need_sync = true;

        if (sp->role.word != role.word)
            continue;

        if (sp->unsync && kvm_sync_page_transient(vcpu, sp))
            break;

        mmu_page_add_parent_pte(vcpu, sp, parent_pte);
        if (sp->unsync_children) {
            kvm_make_request(KVM_REQ_MMU_SYNC, vcpu);
            kvm_mmu_mark_parents_unsync(sp);
        } else if (sp->unsync)
            kvm_mmu_mark_parents_unsync(sp);

        __clear_sp_write_flooding_count(sp);
        trace_kvm_mmu_get_page(sp, false);
        return sp;
    }
    ++vcpu->kvm->stat.mmu_cache_miss;
    sp = kvm_mmu_alloc_page(vcpu, parent_pte, direct);
    if (!sp)
        return sp;
    sp->gfn = gfn;
    sp->role = role;
//新的 mmu page 加入 hash 索引, 所以前面的 for 循环中才能知道 gfn 对应的 mmu 有没有
//被分配
    hlist_add_head(&sp->hash_link,
        &vcpu->kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)]);
    if (!direct) {
        if (rmap_write_protect(vcpu->kvm, gfn))
            kvm_flush_remote_tlbs(vcpu->kvm);
        if (level > PT_PAGE_TABLE_LEVEL && need_sync)
            kvm_sync_pages(vcpu, gfn);

        account_shadowed(vcpu->kvm, gfn);
    }
    sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
    init_shadow_page_table(sp);
    trace_kvm_mmu_get_page(sp, true);
    return sp;
}
```

这样看每次缺页都会分配新的 mmu page, 虚拟机每次启动是根据 guest 不停的进行`EXIT_REASON_EPT_VIOLATION`, 整个页表就建立起来了.


一、qemu 中物理内存的注册
cpu_register_physical_memory 调用 cpu_notify_set_memory
cpu_notify_set_memory 调用 kvm_client_set_memory
kvm_client_set_memory 调用 kvm_set_phys_mem
kvm_set_phys_mem 调用 kvm_set_user_memory_region
kvm_set_user_memory_region 调用的 kvm_vm_ioctl 进入内核
内核中会调用 kvm_vm_ioctl_set_memory_region 最终调用到__kvm_set_memory_region 函数
在__kvm_set_memory_region 函数中有如下代码:
738 ____slots->memslots[mem->slot] = new;
739 ____old_memslots = kvm->memslots;
740 ____rcu_assign_pointer(kvm->memslots, slots);
741 ____synchronize_srcu_expedited(&kvm->srcu);
因此函数__kvm_set_memory_region 本质是创建并填充了一个临时 kvm_memslots 结构, 并把其赋值给 kvm->memslots(全局的).

二、处理用户态虚拟的地址(主要考虑 tlb 不能命中的情况)
1、查物理 tlb 如果不能命中会调用 host 中 do_kvm_tlbmiss
2、do_kvm_tlbmiss 会先判断地址是 IO 地址还是访存的地址, 如果是访存地址, 会进一步查 guest tlb 表, 如果查 guest tlb 还没有命中, 就会把 guest tlb miss 异常注入到 guest 系统中, guest kernel 会根据页表来填充 guest tlb, 当 guest 调用 TLBWI 特权指令时, 会再次陷入 host 中, 调用 do_kvm_cpu 异常处理
3、在 do_kvm_cpu 中模拟 TLBWI 指令, 先填充 guest tlb 表项, 在调用 kvmmips_update_shadow_tlb 来更新物理 tlb(shadow tlb)
4、在 kvmmips_update_shadow_tlb 中, 通过 gfn_to_page 和 page_to_phys 两个函数将 gpa 转化成 hpa,再将 hpa 填充到物理 tlb 中
5、gfn_to_page 函数(我想讲的重点)这个函数会调用到 gfn_to_hva
6、gfn_to_hva 调用 gfn_to_memslot 和 gfn_to_hva_memslot
gfn_to_memslot 代码如下:
859 struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn)
860 {
861 ____int i;
862 ____struct kvm_memslots *slots = kvm_memslots(kvm);
863
864 ____for (i = 0; i < slots->nmemslots; ++i) {
865 ________struct kvm_memory_slot *memslot = &slots->memslots[i];
866
867 ________if (gfn >= memslot->base_gfn
868 ________ && gfn < memslot->base_gfn + memslot->npages)
869 ____________return memslot;
870 ____}
871 ____return NULL;
872 }
代码中首先调用 kvm_memslots 获得 slots,kvm_memslots 代码如下:
255 static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
256 {
257 ____return rcu_dereference_check(kvm->memslots,
258 ____________srcu_read_lock_held(&kvm->srcu)
259 ____________|| lockdep_is_held(&kvm->slots_lock));
260 }
本质是 return kvm->memsolts.
gfn_to_hva_memslot 代码如下:
935 static unsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn)
936 {
937 ____return slot->userspace_addr + (gfn - slot->base_gfn) * PAGE_SIZE;
938 }
由此看来 gpa 到 hva 的关键是 slot->userspace_addr, 其在 qemu 中 kvm_set_user_memory_region 中通过 qemu_safe_ram_ptr 函数赋值.
qemu_safe_ram_ptr 代码如下:
2945 void *qemu_safe_ram_ptr(ram_addr_t addr)
2946 {
2947 RAMBlock *block;
2948
2949 QLIST_FOREACH(block, &ram_list.blocks, next) {
2950 if (addr - block->offset < block->length) {
2951 return block->host + (addr - block->offset);
2952 }
2953 }
2954
2955 fprintf(stderr, "Bad ram offset %" PRIx64 "\n", (uint64_t)addr);
2956 abort();
2957
2958 return NULL;
2959 }
因此得找到 block->host,在 qemu 的 qemu_ram_alloc_from_ptr 函数中赋值, 在该函数中有这么一句话 new_block->host = qemu_vmalloc(size);从 host 系统中分配一个 hva 地址.

结论:
综合以上分析可以看出, 在 qemu 中调用 qemu_ram_alloc 主要是分配 RAMBlock 结构, 并将其插入 ram_list.blocks 链表, 它的本质上分配了一个 hva 地址, 把它放到 RAMBlock 结构 host 域; 调用 cpu_register_physical_memory 主要填充 struct kvm 结构的 slots 域, 它的本质是将一个 gha 地址与 hva 地址对应起来, 将 hva 放在 slot->userspace_addr 中, 将 gha 放在 slot->base_gfn 中. quma 通过上面两个函数就把一段 gha 的空间映射成一段 hva 空间.

三、console 显示过程(基于 cirrusfb)
先看一个函数栈:
2 [<4000000080451164>] cirrusfb_imageblit+0xa0/0x284
3 [<400000008043ce5c>] bit_putcs+0x3dc/0x48c
4 [<400000008046eb8c>] do_update_region+0x148/0x1a4
5 [<40000000804705f4>] update_region+0xb4/0xdc
6 [<40000000804393bc>] fbcon_switch+0x5b8/0x61c
7 [<4000000080470ef4>] redraw_screen+0x188/0x2a8
8 [<4000000080472c84>] take_over_console+0x368/0x3cc
9 [<4000000080436030>] fbcon_takeover+0x108/0x188
10 [<4000000080160204>] notifier_call_chain.isra.1+0x40/0x90
11 [<4000000080160540>] __blocking_notifier_call_chain+0x48/0x68
12 [<400000008042ee8c>] register_framebuffer+0x2b0/0x2dc
13 [<400000008010f4b4>] cirrusfb_pci_register+0x608/0x6c4
14 [<400000008042650c>] pci_device_probe+0x60/0xa0
15 [<4000000080489008>] driver_probe_device+0x108/0x1f0
16 [<400000008048915c>] __driver_attach+0x6c/0xa4
17 [<40000000804879f8>] bus_for_each_dev+0x54/0xa0
18 [<40000000804881ec>] bus_add_driver+0xf0/0x310
19 [<4000000080489838>] driver_register+0xe0/0x194
20 [<4000000080426214>] __pci_register_driver+0x5c/0x11c
21 [<4000000080886710>] cirrusfb_init+0x164/0x198
22 [<4000000080870c18>] do_one_initcall+0xbc/0x204
23 [<4000000080870ecc>] kernel_init+0x16c/0x244
24 [<40000000801189e8>] kernel_thread_helper+0x10/0x18
从函数栈可以看出, register_framebuffer 会触发一个 FB_EVENT_FB_REGISTERED 事件, 调用函数 fbcon_fb_registered, 该函数中调用 fbcon_takeover 来接管操作 console 的函数, 从此之后 console 的操作, 就会调用下面函数
3281 static const struct consw fb_con = {
3282 ____.owner__________= THIS_MODULE,
3283 ____.con_startup _______= fbcon_startup,
3284 ____.con_init ______= fbcon_init,
3285 ____.con_deinit ________= fbcon_deinit,
3286 ____.con_clear _____= fbcon_clear,
3287 ____.con_putc ______= fbcon_putc,
3288 ____.con_putcs _____= fbcon_putcs,
3289 ____.con_cursor ________= fbcon_cursor,
3290 ____.con_scroll ________= fbcon_scroll,
3291 ____.con_bmove _____= fbcon_bmove,
3292 ____.con_switch ________= fbcon_switch,
3293 ____.con_blank _____= fbcon_blank,
3294 ____.con_font_set ______= fbcon_set_font,
3295 ____.con_font_get ______= fbcon_get_font,
3296 ____.con_font_default___= fbcon_set_def_font,
3297 ____.con_font_copy _____= fbcon_copy_font,
3298 ____.con_set_palette ___= fbcon_set_palette,
3299 ____.con_scrolldelta ___= fbcon_scrolldelta,
3300 ____.con_set_origin ____= fbcon_set_origin,
3301 ____.con_invert_region _= fbcon_invert_region,
3302 ____.con_screen_pos ____= fbcon_screen_pos,
3303 ____.con_getxy _____= fbcon_getxy,
3304 ____.con_resize = fbcon_resize,
3305 ____.con_debug_enter____= fbcon_debug_enter,
3306 ____.con_debug_leave____= fbcon_debug_leave,
3307 };
我们不防以 fbcon_putcs 函数为例, 进一步分析, 其代码如下:
1256 static void fbcon_putcs(struct vc_data *vc, const unsigned short *s,
1257 ____________int count, int ypos, int xpos)
1258 {
1259 ____struct fb_info *info = registered_fb[con2fb_map[vc->vc_num]];
1260 ____struct display *p = &fb_display[vc->vc_num];
1261 ____struct fbcon_ops *ops = info->fbcon_par;
1262
1263 ____if (!fbcon_is_inactive(vc, info))
1264 ________ops->putcs(vc, info, s, count, real_y(p, ypos), xpos,
1265 ____________ get_color(vc, info, scr_readw(s), 1),
1266 ____________ get_color(vc, info, scr_readw(s), 0));
1267 }
它需要调用 info->fbcon_par->putcs(info 的数据结构是 struct fb_info),info->fbcon_par 初始化在函数是 fbcon_set_bitops,函数如下:
404 void fbcon_set_bitops(struct fbcon_ops *ops)
405 {
406 ____ops->bmove = bit_bmove;
407 ____ops->clear = bit_clear;
408 ____ops->putcs = bit_putcs;
409 ____ops->clear_margins = bit_clear_margins;
410 ____ops->cursor = bit_cursor;
411 ____ops->update_start = bit_update_start;
412 ____ops->rotate_font = NULL;
413
414 ____if (ops->rotate)
415 ________fbcon_set_rotate(ops);
416 }
因此会继续调用 bit_putcs,其最终会调用到 info->fbops->fb_imageblit(info, image);(info 的数据结构是 struct fb_info),info->fbops 的初始化函数是 cirrusfb_set_fbinfo 中, 该函数中有 info->fbops = &cirrusfb_ops;一句话, cirrusfb_ops 结构如下:
1973 static struct fb_ops cirrusfb_ops = {
1974 ____.owner______= THIS_MODULE,
1975 ____.fb_open____= cirrusfb_open,
1976 ____.fb_release_= cirrusfb_release,
1977 ____.fb_setcolreg___= cirrusfb_setcolreg,
1978 ____.fb_check_var___= cirrusfb_check_var,
1979 ____.fb_set_par_= cirrusfb_set_par,
1980 ____.fb_pan_display = cirrusfb_pan_display,
1981 ____.fb_blank___= cirrusfb_blank,
1982 ____.fb_fillrect____= cirrusfb_fillrect,
1983 ____.fb_copyarea____= cirrusfb_copyarea,
1984 ____.fb_sync____= cirrusfb_sync,
1985 ____.fb_imageblit___= cirrusfb_imageblit,
1986 };
因此最终会调用到 cirrusfb_imageblit 函数.

上面一个过程就是一个 console 写操作, 最终调到 cirrusfb 驱动中 cirrusfb_imageblit 过程.

四、xserver 显示(基于 fbmem)

xserver 下普通的显卡驱动, 通常会直接操作寄存器, 具体操作就是, 先 mmap(/dev/mem)io 空间的基址, 再通过基址加偏移的方式, 操作寄存器.  但 fbmem 是个例外, 其不用操作既存器, 而是通过 ioctl(/dev/fb0), 把这些操作丢给内核去做.  但是两者在都没有加速的情况下, framebuffer 操作方式是相同的, 都是将 framebuffer 区域通过 mmap 映射到用户态, 然后交给 xorg 中其他代码处理映射后地址.  五、结论 qemu 中 cirrus_linear_writeb 函数在 console 下调用很多次而在 Xserver 不被调用原因如下:  首先在 xserver 下, 我们将 framebuffer 区域(0x14000000 开始的一段区域)mmap 成了 guest 虚拟地址(通过调用/dev/fb0 mmap 函数), 也就说在 xserver 所有 frambuffer 的操作, 都是通过这个 gva. 其次 qema 中, 在函数 map_linear_vram 中, 通过 cpu_register_physical_memory(s->vga.map_addr, s->vga.map_end - s->vga.map_addr, s->vga.vram_offset);(将 0x14000000 这个 gpa 和一个 hva 建立起了联系) 因此在 xserver 下整个 framebuffer 操作全部成了内存操作, 不是 IO 操作, 过程是 gva->gpa->hva->hpa,不会回到 qemu 中, 当然也就不可能访问 qemu 中的 cirrus_linear_writeb 函数了.  再次在 console 下, 访 console 操作最终会调用到 cirrusfb_imageblit 函数, 在 cirrusfb_imageblit 中有这么一句话 memcpy(info->screen_base, image->data, size);其中 info->screen_base 就是 0x14000000remap 后(IO 空间), 因此会回到 qemu 中, 调用 cirrus_linear_writeb.  最后, 为什么这么关注 cirrus_linear_writeb 函数, 因为在 qemu 中操作 framebuffer 表现在往 s->vga.vram_ptr 中写或从 s->vga.vram_ptr 读, (s->vga.vram_ptr 就是我们说的 hva), 通过 2242 s->vram_offset = qemu_ram_alloc(NULL, "vga.vram", vga_ram_size); 2243 s->vram_ptr = qemu_get_ram_ptr(s->vram_offset);得到. 只有在 cirrus_linear_writeb 函数中, 才在往 s->vga.vram_ptr 这个 hva 写后, 通过 cpu_physical_memory_set_dirty 将这个区域标记, 而在我们更新屏幕是, dirty 的区域是我们更新的判断条件.