<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 中断模式](#1-中断模式)
    - [1.0.1. INTx vs. MSI](#101-intx-vs-msi)
    - [1.0.2. ivshmem-doorbell](#102-ivshmem-doorbell)
- [2. qemu 参数](#2-qemu-参数)
- [3. server](#3-server)
  - [3.1. server 初始化](#31-server-初始化)
  - [3.2. 启动 server](#32-启动-server)
  - [3.3. 监听所有 sockets](#33-监听所有-sockets)
- [4. QEMU 侧 socket 设备](#4-qemu-侧-socket-设备)
- [5. server 处理连接请求](#5-server-处理连接请求)
- [6. QEMU 侧 doorbell 设备初始化](#6-qemu-侧-doorbell-设备初始化)
- [7. QEMU 侧接收后续消息](#7-qemu-侧接收后续消息)
  - [7.1. 新连接](#71-新连接)
    - [7.1.1. 设置当前 peer 的中断](#711-设置当前-peer-的中断)
    - [7.1.2. 关联 doorbell 与 ioeventfd](#712-关联-doorbell-与-ioeventfd)
  - [7.2. 断连](#72-断连)
- [8. guest 启用 msix 功能](#8-guest-启用-msix-功能)
- [9. guest 读 pba table](#9-guest-读-pba-table)
- [10. guest 写 msix table](#10-guest-写-msix-table)
- [11. 写 doorbell 中断通知](#11-写-doorbell-中断通知)
- [12. peer vm 注入中断](#12-peer-vm-注入中断)
- [13. 内核驱动示例](#13-内核驱动示例)

<!-- /code_chunk_output -->

# 1. 中断模式

**非中断模式**直接把**虚拟 pci 设备**当做一个**共享内存**进行操作, **中断模式**则会操作**虚拟 pci 的寄存器**进行通信, 数据的传输都会触发一次**虚拟 pci 中断**并触发中断回调, 使接收方显式感知到数据的到来, 而不是一直阻塞在 read.

在虚拟机之间或者宿主机与虚拟机之间通过**共享内存进行通信**的情形下, **共享内存的两端**必须依赖**轮询方式**来实现**通知机制**. 这种方式是 ivshmem 提供的 `ivshmem-plain` 设备的使用方式.

**中断模式** 就是 `ivshmem-doorbell` 设备的**使用方式**, 它提供了**基于中断**的通知机制.

从代码分析和实际验证:

* **guest** 与 **guest** 之间可以实现中断与非中断 2 种模式下的通信

* **host** 与 **guest** 之间只支持**非中断模式**的通信.

### 1.0.1. INTx vs. MSI

传统的中断都有**专门的中断 pin**, 当中断信号产生时, 中断 PIN 电平产生变化(一般是拉低). INTx 就是传统的外部中断触发机制, 它使用**专门的通道**来产生控制信息. 然而 PCIe 并没有多根独立的中断 PIN, 于是使用特殊的信号来模拟中断 PIN 的置位和复位.

**MSI** 的全称是 `Message Signaled Interrupt`. MSI 出现在 PCI 2.2 和 PCIe 的规范中, 是一种内部中断信号机制. MSI 允许设备向一段指定的 MMIO 地址空间写一小段数据, 然后 chipset 以此产生相应的中断给 CPU.

从电气机械的角度, MSI 减少了对 interrupt pin 个数的需求, 增加了中断号的数量, **传统的 PCI 中断**只允许**每个 device** 拥有 **4 个中断**, 并且由于这些中断都是共享的, 大部分 device 都只有一个中断, **MSI** 允许**每个 device** 有1, 2, 4, 8, 16甚至 **32 个中断**.

使用 MSI 也有一点点性能上的优势. 使用传统的 PIN 中断, 当中断到来时, 程序去读内存获取数据时有可能会产生冲突. 其原因 device 的数据主要通过DMA来传输, 而在PIN中断到达时, DMA传输还未能完成, 此时cpu不能获取到数据, 只能空转. 而MSI不会存在这个问题, 因为MSI都是发生在DMA传输完成之后的.

### 1.0.2. ivshmem-doorbell

`ivshmem-doorbell` 提供了**两种中断方式**:

* 一种是**传统的**基于 **INTx** 的中断, 它主要使用 **BAR0** 的 `Interrupt Mask` 和 `Interrupt Status` 两个寄存器;

* 另一种是基于 `MSI-X` 的中断, 它主要使用 **BAR0** 的 **IVPosition** 和 **Doorbell** 两个寄存器.

由于具有了如上特性, ivshmem 在执行 **pin 中断**时(**revision 0** 时候), 则写 1 到 `Interrupt Status` **bit0** 触发对目标 guest 的中断.

如果使用 **Msi**, 则 ivshmem 设备支持**多 msi 向量**. 低 16 bit 写入值为 0 到 Guest 所支持的最大向量. 低 16 位写入的值就是目标 guest 上将会触发的msi向量. Msi 向量在 vm 启动时配置. Mis 不设置 status 位, 因此除了中断自身, 所有信息都通过共享内存区域通信. 由于设备支持多 msi 向量, 这样可以使用不同的向量来表示不同的事件. 这些向量的含义由用户来定.

使用共享的设备端叫做 peer. **IVPosition 寄存器**存储**该 peer 的数字标识符**(0-65535), 称做 `peer_id`. 该寄存器为**只读寄存器**.

而 Doorbell 寄存器为**只写寄存器**. ivshmem-doorbell 设备支持**多个中断向量**, **写入 Doorbell 寄存器**则触发**共享该内存**的**某个 peer 的某个中断**. Doorbell 为 32 位

* 低 16 位为 `peer_id`;

* 高 16 位为**中断向量号**(这里是从 0 开始的**顺序号**, 而**非** PCI 驱动在 Guest 虚拟机内部所申请的**向量号**).

# 2. qemu 参数

在 qemu 一端通过 `–chardev socket` 建立 **socket 连接**, 并通过 `-device ivshmem` 建立**共享内存设备**.

```
// doorbell 设备, 是个 socket 设备
-chardev socket,path=/tmp/ivshmem_socket,id=fg-doorbell \
// doorbell 设备
-device ivshmem-doorbell,chardev=fg-doorbell,vectors=8

// plain 设备, 是个 memory 设备
-object memory-backend-file,size=8M,share=true,mem-path=/dev/shm/shm3,id=shm3 \
-device ivshmem-plain,memdev=shm3,bus=pci.0,addr=0x1f,master=on
```

doorbell 设备的共享内存是通过 `ivshmem-server` 提供的

# 3. server

使用 `ivshmem-doorbell` 机制需要运行 `ivshmem-server`. 它是在 host 上运行的一个应用程序, 在 qemu 启动前会启动, 根据参数**创建共享内存**, 并通过**监听本地 UNIX DOMAIN SOCKET** 等待共享内存的 **peer** 来**连接**.

添加了 `ivshmem-doorbell` **设备**的 QEMU 进程会连接该 **socket**, 待 socket 连接建立后, server 会通过 socket 指派给每个 vm 一个 peer id 号(用来标识 vm), 并且将 id 号同 eventfd 文件描述符一起发给 qemu 进程(一个efd表示一个中断向量, msi下有多个efd). guests 之间通过 eventfd 来通知中断. 每个 guest 都在与自己 id 所绑定的 eventfd 上侦听, 并且使用其它 eventfd 来向其它 guest 发送中断.

server 代码参考 `contrib/ivshmem-server/*`

## 3.1. server 初始化

> 创建 socket 并监听

ivshmem_server 启动方式如下:

```
cd ./build/contrib/ivshmem-server
./ivshmem-server -l 4M -M fg-doorbell -n 8 -F -v
```

* `-l`: 共享内存大小, doorbell 设备的共享内存是通过 `ivshmem-server` 提供的

* `-M`: share memory 文件名, `-m` 表明是目录

* `-n`: 向量数目

* `-F`: 不是 daemonize

* `-v`: verbose

```cpp
int
main(int argc, char *argv[])
{
    ...
    // 初始化
    if (ivshmem_server_init(&server, args.unix_socket_path,
                            args.shm_path, args.use_shm_open,
                            args.shm_size, args.n_vectors, args.verbose) < 0) {
    }

    // 启动 server
    if (ivshmem_server_start(&server) < 0) {
    }
    // 监听所有
    ivshmem_server_poll_events(&server);
}
```

## 3.2. 启动 server

> 初始化 server socket_fd

```cpp
int
ivshmem_server_start(IvshmemServer *server)
{
    struct sockaddr_un s_un;
    int shm_fd, sock_fd;

    // -M shm文件
    if (server->use_shm_open) {
        IVSHMEM_SERVER_DEBUG(server, "Using POSIX shared memory: %s\n",
                             server->shm_path);
        shm_fd = shm_open(server->shm_path, O_CREAT | O_RDWR, S_IRWXU);
    // -m 目录名
    } else {
        gchar *filename = g_strdup_printf("%s/ivshmem.XXXXXX", server->shm_path);
        IVSHMEM_SERVER_DEBUG(server, "Using file-backed shared memory: %s\n",
                             server->shm_path);
        // 产生临时文件
        shm_fd = mkstemp(filename);
        unlink(filename);
        g_free(filename);
    }
    // 设置文件大小
    if (ivshmem_server_ftruncate(shm_fd, server->shm_size) < 0) {
    }
    ...
    // 创建一个 socket
    sock_fd = socket(AF_UNIX, SOCK_STREAM, 0);
    // local, 用于同一台机器上的进程间通信
    s_un.sun_family = AF_UNIX;
    // s_un.sun_path 默认是 /tmp/ivshmem_socket
    snprintf(s_un.sun_path, sizeof(s_un.sun_path), "%s",
            server->unix_sock_path);
    // 与本地文件进行绑定
    if (bind(sock_fd, (struct sockaddr *)&s_un, sizeof(s_un)) < 0)
    // 监听
    if (listen(sock_fd, IVSHMEM_SERVER_LISTEN_BACKLOG) < 0)

    server->sock_fd = sock_fd;
    server->shm_fd = shm_fd;
}
```

## 3.3. 监听所有 sockets

> server 的 socket fd 和所有 peer 的 socket fd

Server 端通过 select 侦听一个 qemu 上 socket 的连接.

`select()`: 查看指定 fd_set 中 socket 状态, 如果 fd_set 中有套接字**准备就绪**(触发(读、写或执行)), 则会返回, 返回值为**触发的套接字个数**.

`accept()`: 经过 创建套接字 `socket()`, 绑定 `bind()` 以及 `listen()` 之后, 将监听 socket 和客户端 socket 建立一个全新连接, 并返回 client 的 socket 信息, 以及新的 fd;

> 判断是否有客户端发起链接请求, 一般用 `select()`, 然后 `accept()`

```cpp
static int
ivshmem_server_poll_events(IvshmemServer *server)
{
    // fd 集
    fd_set fds;
    int ret = 0, maxfd;

    while (!ivshmem_server_quit) {
        // 清空 fds
        FD_ZERO(&fds);
        maxfd = 0;
        // 获取所有需要监听的 fds
        ivshmem_server_get_fds(server, &fds, &maxfd);
        // int select(int maxfd,fd_set *rdset,fd_set *wrset,fd_set *exset,struct timeval *timeout);
        // maxfd 是需要监视的最大的文件描述符值 + 1
        // rdset 是需要检测的可读文件描述符的集合
        // wrset 是需要检测的可写文件描述符的集合
        // exset 是需要检测的异常文件描述符的集合
        // timeval, 如果在这个时间内需要监视的描述符没有事件发生则函数返回, 返回值为0
        // 返回触发的套接字个数
        ret = select(maxfd, &fds, NULL, NULL, NULL);

        if (ret == 0) {
            continue;
        }
        // 遍历fds, 看是哪些个触发(可能多个同时触发)
        if (ivshmem_server_handle_fds(server, &fds, maxfd) < 0) {
        }
    }

    return ret;
}
```

获取所有 socket fds.

```cpp
void
ivshmem_server_get_fds(const IvshmemServer *server, fd_set *fds, int *maxfd)
{
    IvshmemServerPeer *peer;
    // 增加了 socket fd
    // server 自己的 socket fd
    FD_SET(server->sock_fd, fds);
    if (server->sock_fd >= *maxfd) {
        *maxfd = server->sock_fd + 1;
    }
    // 所有 peer 的 fd, 刚启动时候没有
    QTAILQ_FOREACH(peer, &server->peer_list, next) {
        // 增加了 socket fd
        FD_SET(peer->sock_fd, fds);
        if (peer->sock_fd >= *maxfd) {
            *maxfd = peer->sock_fd + 1;
        }
    }
}
```

遍历所有 socket fds, 看哪个有消息

```cpp
int
ivshmem_server_handle_fds(IvshmemServer *server, fd_set *fds, int maxfd)
{
    IvshmemServerPeer *peer, *peer_next;
    // server 的 socket fd
    if (server->sock_fd < maxfd && FD_ISSET(server->sock_fd, fds) &&
        // server 的 socket fd 用来 新连 的
        ivshmem_server_handle_new_conn(server) < 0 && errno != EINTR) {
    }
    // peer 的 socket fd
    QTAILQ_FOREACH_SAFE(peer, &server->peer_list, next, peer_next) {
        // peer 的 socket fd 来消息都是用来 断连的
        if (peer->sock_fd < maxfd && FD_ISSET(peer->sock_fd, fds)) {
            ivshmem_server_free_peer(server, peer);
        }
    }
    return 0;
}
```

# 4. QEMU 侧 socket 设备

> 连接 server

使用 qemu 参数 `–chardev socket`, 文件使用 ivshmem server 的

```
// 一个 socket 设备
-chardev socket,path=/tmp/ivshmem_socket,id=fg-doorbell
```

qemu 通过查找 chardev 注册类型 `register_types` 会调用 `qmp_chardev_open_socket->qmp_chardev_open_socket_client->qio_channel_socket_connect_sync->socket_connect`, 实现与 server 通信, 从而**建立 socket 连接**.

# 5. server 处理连接请求

**server** 收到 socket 请求后, 调用 `ivshmem_server_handle_new_conn` 会指派给每个 vm 一个 id 号, 并且将 id 号和**一系列 eventfd 文件描述符**(一个中断向量对应一个)一起发给 qemu 进程, qemu 间通过高效率的 eventfd 方式通信.

```cpp
typedef struct IvshmemServerPeer {
    // 链表
    QTAILQ_ENTRY(IvshmemServerPeer) next;
    // peer 的 socket fd
    int sock_fd;
    // peer id
    int64_t id;
    // 向量数组
    EventNotifier vectors[IVSHMEM_SERVER_MAX_VECTORS];
    // 向量数
    unsigned vectors_count;
} IvshmemServerPeer;

// contrib/ivshmem-server/ivshmem-server.c
static int
ivshmem_server_handle_new_conn(IvshmemServer *server)
{
    ...
    // 初始化连接, 返回新的fd, 用来和对端通信
    newfd = qemu_accept(server->sock_fd,
                        (struct sockaddr *)&unaddr, &unaddr_len);

    qemu_socket_set_nonblock(newfd);

    /* allocate new structure for this peer */
    peer = g_malloc0(sizeof(*peer));
    peer->sock_fd = newfd;
    // 确保 server->cur_id 是空的
    for (i = 0; i < G_MAXUINT16; i++) {
        if (ivshmem_server_search_peer(server, server->cur_id) == NULL) {
            break;
        }
        server->cur_id++;
    }
    // 新id, 从0开始, 永远递增, 也就是永不重复
    // 最后 server->cur_id 是当前 id +1, 下一个理论上肯定是 空的
    peer->id = server->cur_id++;

    // 一个向量生成一个 eventfd
    peer->vectors_count = server->n_vectors;
    for (i = 0; i < peer->vectors_count; i++) {
        if (event_notifier_init(&peer->vectors[i], FALSE) < 0) {
            ...
        }
    }

    // 给新连接的 client(peer->sock_fd) 发送消息
    // 1. 0(协议版本号), 不带 fd
    // 2. peer id, 不带 fd
    // 3. -1 并且带server->shm_fd(server的共享内存体fd)
    if (ivshmem_server_send_initial_info(server, peer) < 0) {
        ...
    }

    // 遍历 peer_list, 给其他所有peer都发送(新peer_id, 新client的所有向量对应的eventfd)
    QTAILQ_FOREACH(other_peer, &server->peer_list, next) {
        for (i = 0; i < peer->vectors_count; i++) {
            // msg 是 peer_id, 带有 fd
            // (3, fd1) (3, fd2) (3, fd3)
            ivshmem_server_send_one_msg(other_peer->sock_fd, peer->id,
                                        peer->vectors[i].wfd);
        }
    }

    // 给新client发送其他所有peer的(peer_id, client的所有向量对应的eventfd)
    QTAILQ_FOREACH(other_peer, &server->peer_list, next) {
        for (i = 0; i < peer->vectors_count; i++) {
            // 参数是 (peer_id, vector对应的fd)
            // msg 是 peer_id, 带有 fd
            // (0, fd1) (0, fd2) (0, fd3)
            // (1, fd1) (1, fd2) (1, fd3)
            // (2, fd1) (2, fd2) (2, fd3)
            ivshmem_server_send_one_msg(peer->sock_fd, other_peer->id,
                                        other_peer->vectors[i].wfd);
        }
    }

    // 这是中断设置消息
    // 将新client的peer_id和所有向量的eventfd发给新client自己
    for (i = 0; i < peer->vectors_count; i++) {
        // 参数是 (peer_id, vector对应的fd)
        // msg 是 peer_id, 带有 fd
        // (3, fd1) (3, fd2) (3, fd3)
        ivshmem_server_send_one_msg(peer->sock_fd, peer->id,
                                    event_notifier_get_fd(&peer->vectors[i]));
    }
    // 添加到server的peer链表尾部
    QTAILQ_INSERT_TAIL(&server->peer_list, peer, next);
    return 0;
}
```

# 6. QEMU 侧 doorbell 设备初始化

Qemu 侧同时定义 doorbell Pci 设备, 字符设备对应上面 socket 设备:

```
// doorbell 设备
-device ivshmem-doorbell,chardev=fg-doorbell,vectors=8
```

ivshmem doorbell 设备初始化时会接收来自 server 的信息.

```cpp
// hw/misc/ivshmem.c
ivshmem_doorbell_realize() -> ivshmem_common_realize()

static void ivshmem_common_realize(PCIDevice *dev, Error **errp)
{
    // BAR0 初始化
    memory_region_init_io(&s->ivshmem_mmio, OBJECT(s), &ivshmem_mmio_ops, s,
                          "ivshmem-mmio", IVSHMEM_REG_BAR_SIZE);

    /* region for registers*/
    pci_register_bar(dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY,
                     &s->ivshmem_mmio);

    // plain 设备
    // mem-path=/dev/shm/shm1
    if (s->hostmem != NULL) {
        ......
        // bar2 来自于传入的参数
    // doorbell 设备
    } else {
        ......
        // 预分配 16 个 peers
        resize_peers(s, 16);
        // 第一, 基本信息初始化
        // BAR2 来自于 server shm_fd
        ivshmem_recv_setup(s, &err);
        // 第二. 后续对server socket的read回调
        qemu_chr_fe_set_handlers(&s->server_chr, ivshmem_can_receive,
                                 ivshmem_read, NULL, NULL, s, NULL, true);
        // 第三
        if (ivshmem_setup_interrupts(s, errp) < 0) {
        }
    }
    ......
    vmstate_register_ram(s->ivshmem_bar2, DEVICE(s));
    // bar2 注册
    pci_register_bar(PCI_DEVICE(s), 2,
                     PCI_BASE_ADDRESS_SPACE_MEMORY |
                     PCI_BASE_ADDRESS_MEM_PREFETCH |
                     PCI_BASE_ADDRESS_MEM_TYPE_64,
                     s->ivshmem_bar2);
}
```

第一. 从 server 接收**初始化信息**. 版本号, peer id 和 server 的 `shm_fd`, 然后将 `shm_fd` 映射为 **BAR2**.

```cpp
static void ivshmem_recv_setup(IVShmemState *s, Error **errp)
{
    int64_t msg;
    ...
    msg = ivshmem_recv_msg(s, &fd, &err);
    // 获取第一个信息: 版本号
    // 版本号应该是 0
    if (msg != IVSHMEM_PROTOCOL_VERSION) {
    }
    // 获取第二个信息: peer id
    msg = ivshmem_recv_msg(s, &fd, &err);
    if (fd != -1 || msg < 0 || msg > IVSHMEM_MAX_PEERS) {
    }
    // 也就是设备的 vm_id
    s->vm_id = msg;

    // 理论上应该获取第三个信息: -1和(带有server的shm_fd)
    // server的shm_fd在fd变量中
    do {
        msg = ivshmem_recv_msg(s, &fd, &err);
        // 处理消息
        process_msg(s, msg, fd, &err);
    // -1 信息就直接退出
    } while (msg != -1);
}
```

`process_msg` 针对 (`-1`) 调用 `process_msg_shmem` 处理 sever 的 `shm_fd`

```cpp
static void process_msg_shmem(IVShmemState *s, int fd, Error **errp)
{
    struct stat buf;
    size_t size;
    // bar2 不应该已经初始化了
    if (s->ivshmem_bar2) {
        return;
    }
    // 获取server shm_fd的信息
    if (fstat(fd, &buf) < 0) {
    }
    // 文件大小,就是shm大小
    size = buf.st_size;

    // mmap这个shm_fd到BAR2
    // 也就是说doorbell设备的BAR2是server shm_fd
    if (!memory_region_init_ram_from_fd(&s->server_bar2, OBJECT(s), "ivshmem.bar2", size, RAM_SHARED, fd, 0, errp)) {
        return;
    }

    s->ivshmem_bar2 = &s->server_bar2;
}
```

第二. 设置了后续对 ivshmem server 的回调函数. 此时还没有处理 server 在 (-1) 之后发送的消息.

第三. 设置中断相关信息.

```cpp
static int ivshmem_setup_interrupts(IVShmemState *s, Error **errp)
{
    /* allocate QEMU callback data for receiving interrupts */
    // 第一
    s->msi_vectors = g_new0(MSIVector, s->vectors);

    if (ivshmem_has_feature(s, IVSHMEM_MSI)) {
        // 第二. MSIX cap 及其 BAR1 的设置
        // BAR1: memory request, 32 bit, non-prefetch
        if (msix_init_exclusive_bar(PCI_DEVICE(s), s->vectors, 1, errp)) {
            return -1;
        }
        // msix initialized (8 vectors)
        IVSHMEM_DPRINTF("msix initialized (%d vectors)\n", s->vectors);
        // 设置 dev->msix_entry_used[0-8]++, 表明在使用
        ivshmem_msix_vector_use(s);
    }

    return 0;
}
```

第一. vectors 默认是 1, 上面 qemu 定义 `ivshmem-doorbell` 设备时候设置的是 8.

```cpp
static Property ivshmem_doorbell_properties[] = {
    DEFINE_PROP_CHR("chardev", IVShmemState, server_chr),
    DEFINE_PROP_UINT32("vectors", IVShmemState, vectors, 1),
    DEFINE_PROP_BIT("ioeventfd", IVShmemState, features, IVSHMEM_IOEVENTFD,
                    true),
    DEFINE_PROP_ON_OFF_AUTO("master", IVShmemState, master, ON_OFF_AUTO_OFF),
    DEFINE_PROP_END_OF_LIST(),
};
```

第二. 给设备添加了 msix cap 并且注册了 BAR1.

# 7. QEMU 侧接收后续消息

server 后续还将(假设当前 peer 是 3):

* 给**其他所有 peer** 都发送(新peer_id, 新client的所有向量对应的eventfd)

> (3, fd1), (3, fd2), ..., (3, fd8)

* 给**新 client** 发送**其他所有 peer** 的(peer_id, 所有向量对应的eventfd)

> (0, fd1), (0, fd2), ..., (0, fd8)
> (1, fd1), (1, fd2), ..., (1, fd8)
> (2, fd1), (2, fd2), ..., (2, fd8)

* 中断消息, 将**新 client** 的(peer_id, 所有向量的 eventfd) 发给**新 client 自己**

> (3, fd1), (3, fd2), ..., (3, fd8)

QEMU 通过 `ivshmem_read` 接收 server 端发来的后续消息

```cpp
static void ivshmem_read(void *opaque, const uint8_t *buf, int size)
{
    ......
    // 获取 message 中的 fd
    fd = qemu_chr_fe_get_msgfd(&s->server_chr);
    // 处理
    process_msg(s, msg, fd, &err);
    if (err) {
        error_report_err(err);
    }
}
```

并调用 `process_msg` 进行处理.

```cpp
static void process_msg(IVShmemState *s, int64_t msg, int fd, Error **errp)
{
    IVSHMEM_DPRINTF("posn is %" PRId64 ", fd is %d\n", msg, fd);

    if (msg < -1 || msg > IVSHMEM_MAX_PEERS) {
        error_setg(errp, "server sent invalid message %" PRId64, msg);
        close(fd);
        return;
    }
    // msg 是 -1, 前面介绍过了
    if (msg == -1) {
        process_msg_shmem(s, fd, errp);
        return;
    }
    // 第一.
    if (msg >= s->nb_peers) {
        resize_peers(s, msg + 1);
    }
    // 有 fd
    // 第二.
    if (fd >= 0) {
        // msg 是 peer id
        process_msg_connect(s, msg, fd, errp);
    // 没有 fd
    // 第三.
    } else {
        process_msg_disconnect(s, msg, errp);
    }
}
```

第一. 根据需要重新调整该设备存的 `s->peers`(`Peer *peers`) 数组.

msg 是 peer id, 大于等于之前缓存的所有peer的数目(因为 peer id 是从0开始, 所以 msg+1) 时才调整

而在 `ivshmem_common_realize` doorbell 设备初始化时候调用 `resize_peers(s, 16);`, 预分配了 16 个

```cpp
static void resize_peers(IVShmemState *s, int nb_peers)
{
    int old_nb_peers = s->nb_peers;
    int i;

    assert(nb_peers > old_nb_peers);
    IVSHMEM_DPRINTF("bumping storage to %d peers\n", nb_peers);
    // 重新调整memory
    s->peers = g_renew(Peer, s->peers, nb_peers);
    s->nb_peers = nb_peers;

    for (i = old_nb_peers; i < nb_peers; i++) {
        // 每个peer的eventfds数组, 一个向量一个
        // peer 的向量数目最多不能超过当前设备的向量数目
        s->peers[i].eventfds = g_new0(EventNotifier, s->vectors);
        // 仅仅分配空间, 初始化在后面
        s->peers[i].nb_eventfds = 0;
    }
}
```

第二. 带有 fd 的话, 说明是有新的连接发过来的 msg.

## 7.1. 新连接

传入的参数是

* **posn**: peer id

* **fd**: 向量对应的 eventfd

```cpp
static void process_msg_connect(IVShmemState *s, uint16_t posn, int fd,
                                Error **errp)
{
    // 根据peer id找到缓存的 peer
    Peer *peer = &s->peers[posn];
    int vector;

    // peer 的向量数目不能大于当前设备的向量号
    if (peer->nb_eventfds >= s->vectors) {
        return;
    }
    // 向量号
    vector = peer->nb_eventfds++;

    IVSHMEM_DPRINTF("eventfds[%d][%d] = %d\n", posn, vector, fd);
    // fd是server传来的向量对应的eventfd
    // 传过来参数peer对应的EventNotifier 结构体初始化
    event_notifier_init_fd(&peer->eventfds[vector], fd);
    // 设置 fd 为非阻塞
    g_unix_set_fd_nonblocking(fd, true, NULL); /* msix/irqfd poll non block */

    // 当前 peer, 才设置中断
    if (posn == s->vm_id) {
        // 对当前peer的每个中断进行设置
        setup_interrupt(s, vector, errp);
        /* TODO do we need to handle the error? */
    }
    // 使用 ioeventfd, 默认打开
    if (ivshmem_has_feature(s, IVSHMEM_IOEVENTFD)) {
        //
        // 将fd(ioeventfd)与Guest IO地址(doorbell)关联起来
        ivshmem_add_eventfd(s, posn, vector);
    }
}
```

### 7.1.1. 设置当前 peer 的中断

* vector: 向量号

```cpp
static void setup_interrupt(IVShmemState *s, int vector, Error **errp)
{
    // 当前peer的vector对应的
    EventNotifier *n = &s->peers[s->vm_id].eventfds[vector];
    // kvm的msi是否通过irqfd实现
    bool with_irqfd = kvm_msi_via_irqfd_enabled() &&
        ivshmem_has_feature(s, IVSHMEM_MSI);
    PCIDevice *pdev = PCI_DEVICE(s);
    Error *err = NULL;

    IVSHMEM_DPRINTF("setting up interrupt for vector: %d\n", vector);
    // 没有启用irqfd的话, 无论msix是否已经启用
    // 通过 eventfd 实现
    if (!with_irqfd) {
        IVSHMEM_DPRINTF("with eventfd\n");
        // 设置了vector对应的fd的:
        // read函数: ivshmem_vector_notify
        // write函数: NULL
        // 第一
        watch_vector_notifier(s, n, vector);
    // 通过 irqfd 实现, 但是此时 msix 还没启用
    } else if (msix_enabled(pdev)) {
        IVSHMEM_DPRINTF("with irqfd\n");
        ivshmem_add_kvm_msi_virq(s, vector, &err);

        if (!msix_is_masked(pdev, vector)) {
            kvm_irqchip_add_irqfd_notifier_gsi(kvm_state, n, NULL,
                                               s->msi_vectors[vector].virq);
        }
    // 所以会走的这里
    } else {
        /* it will be delayed until msix is enabled, in write_config */
        IVSHMEM_DPRINTF("with irqfd, delayed until msix enabled\n");
    }
}
```

第一. `watch_vector_notifier()`. 如果 kvm 没有 irqfd 支持, 那就**使用 ioeventfd**. 调用 `qemu_set_fd_handler()` 注册了 ioeventfd, Linux 的 eventfd 机制使用了 `POLLIN` 通知方案, 所以仅仅设置了 read 函数(`ivshmem_vector_notify`).

这里使用 irqfd, 但是 msix 还没启用, 所以见下面

### 7.1.2. 关联 doorbell 与 ioeventfd

* posn: 传入的 peer id

* i: 向量号

```cpp
static void ivshmem_add_eventfd(IVShmemState *s, int posn, int i)
{
    // 将fd(ioeventfd)与对应的Guest IO地址关联起来, 最终调用 KVM_IOEVENTFD ioctl
    // guest 向 DOORBELL 写入的值必须等于((posn << 16) | i)才让kvm走ioeventfd
    // 见 Virtualization/2. 核心组件/4. KVM/3. 基本机制/通知机制/ioeventfd/
    memory_region_add_eventfd(&s->ivshmem_mmio,
                              DOORBELL,
                              4,
                              true,
                              (posn << 16) | i,
                              &s->peers[posn].eventfds[i]);
}
```

doorbell 寄存器与 ioeventfd 关联

ioeventfd: **传入的 peer id 的 vector 的 eventfd**

写入的值必须等于: `((peer id & 0xffff) << 16) | (vector & 0xffff)`, 符合 spec 中 doorbell 寄存器的要求

这里注册是为了**中断发送方**来写 doorbell.

## 7.2. 断连

第三. 没有 fd 的话, 说明是有的断连了

# 8. guest 启用 msix 功能

当 guest 写 MSI-X Capability Structure 的 Enable Bit 时触发, 调用 qemu 侧的 `ivshmem_write_config`

```cpp
static void ivshmem_write_config(PCIDevice *pdev, uint32_t address,
                                 uint32_t val, int len)
{
    IVShmemState *s = IVSHMEM_COMMON(pdev);
    int is_enabled, was_enabled = msix_enabled(pdev);

    pci_default_write_config(pdev, address, val, len);
    is_enabled = msix_enabled(pdev);
    // 使用了 kvm irqfd
    if (kvm_msi_via_irqfd_enabled()) {
        if (!was_enabled && is_enabled) {
            // enable irqfd
            ivshmem_enable_irqfd(s);
        } else if (was_enabled && !is_enabled) {
            // disable irqfd
            ivshmem_disable_irqfd(s);
        }
    }
}
```

只有使用 irqfd 时候才延迟到 enable msix 时候才设置, 否则使用 ioeventfd, 在 qemu 接收到 server 的消息时候设置

```cpp
static void ivshmem_enable_irqfd(IVShmemState *s)
{
    PCIDevice *pdev = PCI_DEVICE(s);
    int i;
    // 遍历当前vm的设备的所有向量
    for (i = 0; i < s->peers[s->vm_id].nb_eventfds; i++) {
        Error *err = NULL;

        ivshmem_add_kvm_msi_virq(s, i, &err);
    }
    // 将设备的所有msix_vector对应的
    // use_noifier,
    // release_notifier,
    // poll_notifier回调函数都进行记录
    if (msix_set_vector_notifiers(pdev,
                                  ivshmem_vector_unmask,
                                  ivshmem_vector_mask,
                                  ivshmem_vector_poll)) {
    }
    return;
}

static void ivshmem_add_kvm_msi_virq(IVShmemState *s, int vector,
                                     Error **errp)
{
    PCIDevice *pdev = PCI_DEVICE(s);
    KVMRouteChange c;
    int ret;
    // ivshmem_add_kvm_msi_virq vector:0
    // ......
    // ivshmem_add_kvm_msi_virq vector:7
    IVSHMEM_DPRINTF("ivshmem_add_kvm_msi_virq vector:%d\n", vector);
    assert(!s->msi_vectors[vector].pdev);

    c = kvm_irqchip_begin_route_changes(kvm_state);
    // 传入了 vector, 获取了 gsi
    ret = kvm_irqchip_add_msi_route(&c, vector, pdev);
    kvm_irqchip_commit_route_changes(&c);
    // gsi
    s->msi_vectors[vector].virq = ret;
    s->msi_vectors[vector].pdev = pdev;
}
```

# 9. guest 读 pba table

在设备初始化 `ivshmem_common_realize` 时候

`ivshmem_common_realize` -> `ivshmem_setup_interrupts` -> `msix_init_exclusive_bar` -> `msix_init`

```cpp
    memory_region_init_io(&dev->msix_pba_mmio, OBJECT(dev), &msix_pba_mmio_ops, dev,
                          "msix-pba", pba_size);
```

设置了读 pba table 的 ops

```cpp
msix_pba_mmio_read()
-> dev->msix_vector_poll_notifier()
    -> ivshmem_vector_poll()
```

```cpp
static void ivshmem_vector_poll(PCIDevice *dev,
                                unsigned int vector_start,
                                unsigned int vector_end)
{
    IVShmemState *s = IVSHMEM_COMMON(dev);
    unsigned int vector;
    // IVSHMEM: vector poll 0x5c42c55ff2e0 0-8
    IVSHMEM_DPRINTF("vector poll %p %d-%d\n", dev, vector_start, vector_end);

    vector_end = MIN(vector_end, s->vectors);

    // 第一
    for (vector = vector_start; vector < vector_end; vector++) {
        EventNotifier *notifier = &s->peers[s->vm_id].eventfds[vector];
        // 第二
        // 刚开始全部都被 mask
        if (!msix_is_masked(dev, vector)) {
            continue;
        }
        // 第三. 走到这里
        if (event_notifier_test_and_clear(notifier)) {
            // 暂时不到这里
            msix_set_pending(dev, vector);
        }
    }
}
```

第一. 遍历当前 vm 的所有 vector.

第二. 在设备初始化 `ivshmem_common_realize` 时候, 会 mask 掉所有 `PCI_MSIX_ENTRY_CTRL_MASKBIT`

`ivshmem_common_realize` -> `ivshmem_setup_interrupts` -> `msix_init_exclusive_bar` -> `msix_init` -> `msix_mask_all`

第三. 一直读 vector 对应的 eventfd.

```cpp
int event_notifier_test_and_clear(EventNotifier *e)
{
    int value;
    ssize_t len;
    char buffer[512];

    /* Drain the notify pipe.  For eventfd, only 8 bytes will be read.  */
    value = 0;
    do {
        len = read(e->rfd, buffer, sizeof(buffer));
        value |= (len > 0);
    } while ((len == -1 && errno == EINTR) || len == sizeof(buffer));
    return value;
}
```

将从文件描述符 fd 对应的文件中读到的数据存在 buf 缓冲区中, 每次读 count 字节, 同时文件指针会随着移动

read 返回:

* 当返回值大于 0 时: 实际读到的字节数

* 返回值 = 0:

  * 如果读的文件: 说明文件读完了

  * 如果从管道或 socket 中读: 说明对端关闭了

* 返回值为 -1: 说明发生了异常, 根据errno的值进一步判断

  * errno == EINTR 被信号中断

  * errno == EAGAIN(EWOULDBLOCK) **非阻塞方式读**, 并且没有数据

  * 其他值 代表 出现错误, 可以获得返回值, 然后利用 strerror(ret) 去打印错误信息

`(len == -1 && errno == EINTR)`, 发生异常, 被信号中断

# 10. guest 写 msix table

当 guest 对 MSI-X Table 中的 Message Data 的 Vector 地址进行写入时. 由于MSI-X Table存储在某个BAR中, 而BAR又分为可mmap的BAR和不可mmap的BAR, 现在一般都是用可mmap的BAR作为MSI-X Table的存储BAR.

而 MSI-X Table 存储在 BAR1 中.

```cpp
Guest配置MSIX Table:
=> 由于MMIO Exit而退到QEMU
=> address_space_rw
   => address_space_write
      => flatview_write
         ...(一系列的调用)
```

之后调用到:

```cpp
// 通过下面这些调用, 会对vector对应的use_noifier进行调用, 即ivshmem_vector_unmask
// 目的是对某vector对应的中断链路进行一次初始化
=> msix_table_mmio_write
   => msix_handle_mask_update
      => msix_fire_vector_notifier
         => ivshmem_vector_unmask
```

```cpp
static int ivshmem_vector_unmask(PCIDevice *dev, unsigned vector,
                                 MSIMessage msg)
{
    // 当前vm的vector对应的eventfd
    EventNotifier *n = &s->peers[s->vm_id].eventfds[vector];
    // 第一.
    IVSHMEM_DPRINTF("vector unmask %p %d\n", dev, vector);
    // 更新msi路由
    ret = kvm_irqchip_update_msi_route(kvm_state, v->virq, msg, dev);
    // 调用ioctl(KVM_SET_GSI_ROUTING)
    kvm_irqchip_commit_routes(kvm_state);
    // 第二. 申请注册irqfd, fd即n->rfd(vector对应的fd)
    ret = kvm_irqchip_add_irqfd_notifier_gsi(kvm_state, n, NULL, v->virq);
    v->unmasked = true;

    return 0;
}
```

第一. 这里 guest driver 仅仅 unmask 了 4 个.

```
IVSHMEM: vector unmask 0x5c42c55ff2e0 0
IVSHMEM: vector unmask 0x5c42c55ff2e0 1
IVSHMEM: vector unmask 0x5c42c55ff2e0 2
IVSHMEM: vector unmask 0x5c42c55ff2e0 3
```

第二. 获取当前 vm 的 vector 对应的 irqfd, 然后申请注册了 irqfd.


# 11. 写 doorbell 中断通知

`ivshmem-doorbell` 设备支持**多个中断向量**, **ivshmem-server** 会为 ivshmem 虚拟 PCI 设备支持的**每个中断向量**创建一个 **eventfd**, 并将**共享内存**以及为**所有客户端中断向量**所创建的 **eventfd** 都通过 **SCM_RIGHTS** 机制传递给**所有客户端进程**.

这样所有的 peer 便都具备了独立的**两两之间的通知通道**.

之后**在虚拟机内**通过触发 ivshmem 虚拟 PCI 设备的 DOORBELL 寄存器的写入, 虚拟机的 QEMU 进程便会通过 **DOORBELL 寄存器**中的 `peer_id` 和**中断向量号**来找到**相应的 eventfd**, 从而通知到对端的 QEMU 进程来**产生相应的 PCI 中断**.

前面 `process_msg_connect()` 阶段, 将 doorbell 寄存器地址与 ioeventfd 关联了, 根据 **peer** 和 **vector** 获取相应的 **ioeventfd**

```
((peer & 0xffff) << 16) | (vector & 0xffff)
```

~~kvm 会调用 `eventfd_signal` 触发一次 POLLIN 事件, 从而唤醒阻塞在 eventfd 上的读线程.~~

# 12. peer vm 注入中断

当 kvm 支持 irqfd 时候:

peer guest 写 msix table 时候, 调用 `ivshmem_vector_unmask()` 针对启用的 vector, 获取了 eventfd, 从而调用 `kvm_irqchip_add_irqfd_notifier_gsi` 申请注册成了 irqfd.

所以当另外一个 guest 调用 `eventfd_signal` 发送了信号, 那就会直接唤醒睡眠在 eventfd 上的任务, 唤醒后执行 `irqfd_wakeup` 函数, 在该函数中调度任务, 调用 `kvm_arch_set_irq_inatomic` 进行 (MSI 中断或 HV_SINT 中断) 的注入, 如果失败则调用 `irqfd_inject` 来注入中断.

VM0 vCPU0 -> VM1 vCPU0

这里肯定就是 `kvm_arch_set_irq_inatomic` -> `case KVM_IRQ_ROUTING_MSI`

如果 kvm 不支持 irqfd:

新连接时候, 调用了 `setup_interrupt()`, 已经注册了 ioeventfd.

基于 QEMU 的事件循环机制(`qemu_main_loop -> main_loop_wait -> os_host_main_loop_wait`), 当该 ioeventfd 在 kvm 侧有 `eventfd_signal`, 那 qemu 就调用该 read 函数(`ivshmem_vector_notify`).

# 13. 内核驱动示例

要**使用中断机制**, **用户态程序**是无能为力的, 需要**编写相应的 PCI 驱动**来实现.

本文通过一个简单的 PCI 驱动示例来说明 `ivshmem-doorbell` 的 MSI-X 中断机制的使用.

> `./my/ivpic.c`

`ivpci.c` 代码如下:

```cpp
#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/version.h>
#include <linux/err.h>
#include <linux/fs.h>
#include <linux/mm.h>
#include <linux/pci.h>
#include <linux/pci_regs.h>
#include <linux/cdev.h>
#include <linux/device.h>
#include <linux/uaccess.h>
#include <linux/interrupt.h>
#include <linux/ioctl.h>
#include <linux/sched.h>
#include <linux/wait.h>

#define DRV_NAME        "ivpci"
#define DRV_VERSION     "0.1"
#define PFX             "[IVPCI] "
#define DRV_FILE_FMT    DRV_NAME"%d"

#define IVPOSITION_OFF  0x08    /* VM ID */
#define DOORBELL_OFF    0x0c    /* Doorbell */

#define IOCTL_MAGIC         ('f')
#define IOCTL_RING          _IOW(IOCTL_MAGIC, 1, u32)
#define IOCTL_WAIT          _IO(IOCTL_MAGIC, 2)
#define IOCTL_IVPOSITION    _IOR(IOCTL_MAGIC, 3, u32)

static int g_max_devices = 2;
MODULE_PARM_DESC(g_max_devices, "number of devices can be supported");
module_param(g_max_devices, int, 0400);

struct ivpci_private {
    struct pci_dev      *dev;
    struct cdev         cdev;
    int                 minor;

    u8                  revision;
    u32                 ivposition;

    u8 __iomem          *base_addr;
    u8 __iomem          *regs_addr;

    unsigned int        bar0_addr;
    unsigned int        bar0_len;
    unsigned int        bar1_addr;
    unsigned int        bar1_len;
    unsigned int        bar2_addr;
    unsigned int        bar2_len;

    char                (*msix_names)[256];
    struct msix_entry   *msix_entries;
    int                 nvectors;
};

static int event_toggle;
// 使用wait_queue实现等待和通知
DECLARE_WAIT_QUEUE_HEAD(wait_queue);

/* store major device number shared by all ivshmem devices */
static dev_t g_ivpci_devno;
static int  g_ivpci_major;

static struct class *g_ivpci_class;

/* number of devices owned by this driver */
static int g_ivpci_count;

static struct ivpci_private *g_ivpci_devs;

static struct pci_device_id ivpci_id_table[] = {
    { 0x1af4, 0x1110, PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0},
    { 0 },
};
MODULE_DEVICE_TABLE(pci, ivpci_id_table);

static struct ivpci_private *ivpci_get_private(void)
{
    int i;

    for (i = 0; i < g_max_devices; i++) {
        if (g_ivpci_devs[i].dev == NULL) {
            return &g_ivpci_devs[i];
        }
    }

    return NULL;
}

static struct ivpci_private *ivpci_find_private(int minor)
{
    int i;
    for (i = 0; i < g_max_devices; i++) {
        if (g_ivpci_devs[i].dev != NULL && g_ivpci_devs[i].minor == minor) {
            return &g_ivpci_devs[i];
        }
    }

    return NULL;
}

static irqreturn_t ivpci_interrupt(int irq, void *dev_id)
{
    struct ivpci_private *ivpci_dev = dev_id;

    if (unlikely(ivpci_dev == NULL)) {
        return IRQ_NONE;
    }

    dev_info(&ivpci_dev->dev->dev, PFX "interrupt: %d\n", irq);

    event_toggle = 1;
    // 唤醒 wait_queue 上等待的进程
    wake_up_interruptible(&wait_queue);

    return IRQ_HANDLED;
}

static int ivpci_request_msix_vectors(struct ivpci_private *ivpci_dev, int n)
{
    int ret, i;

    ret = -EINVAL;

    dev_info(&ivpci_dev->dev->dev, PFX "request msi-x vectors: %d\n", n);

    ivpci_dev->nvectors = n;

    ivpci_dev->msix_entries = kmalloc(n * sizeof(struct msix_entry),
            GFP_KERNEL);
    if (ivpci_dev->msix_entries == NULL) {
        ret = -ENOMEM;
        goto error;
    }

    ivpci_dev->msix_names = kmalloc(n * sizeof(*ivpci_dev->msix_names),
            GFP_KERNEL);
    if (ivpci_dev->msix_names == NULL) {
        ret = -ENOMEM;
        goto free_entries;
    }

    for (i = 0; i < n; i++) {
        ivpci_dev->msix_entries[i].entry = i;
    }
    // 分配中断向量并设置了
    ret = pci_enable_msix_exact(ivpci_dev->dev, ivpci_dev->msix_entries, n);
    if (ret) {
        dev_err(&ivpci_dev->dev->dev, PFX "unable to enable msix: %d\n", ret);
        goto free_names;
    }

    for (i = 0; i < ivpci_dev->nvectors; i++) {
        snprintf(ivpci_dev->msix_names[i], sizeof(*ivpci_dev->msix_names),
                "%s%d-%d", DRV_NAME, ivpci_dev->minor, i);
        // 每个中断向量的处理函数: ivpci_interrupt
        ret = request_irq(ivpci_dev->msix_entries[i].vector,
                ivpci_interrupt, 0, ivpci_dev->msix_names[i], ivpci_dev);

        if (ret) {
            dev_err(&ivpci_dev->dev->dev, PFX "unable to allocate irq for " \
                    "msix entry %d with vector %d\n", i,
                    ivpci_dev->msix_entries[i].vector);
            goto release_irqs;
        }
        // irq for msix entry: 0, vector: 30
        // ...
        // irq for msix entry: 3, vector: 33
        dev_info(&ivpci_dev->dev->dev,
                PFX "irq for msix entry: %d, vector: %d\n",
                i, ivpci_dev->msix_entries[i].vector);
    }

    return 0;

release_irqs:
    for ( ; i > 0; i--) {
        free_irq(ivpci_dev->msix_entries[i - 1].vector, ivpci_dev);
    }
    pci_disable_msix(ivpci_dev->dev);

free_names:
    kfree(ivpci_dev->msix_names);

free_entries:
    kfree(ivpci_dev->msix_entries);

error:
    return ret;
}

static void ivpci_free_msix_vectors(struct ivpci_private *ivpci_dev)
{
    int i;

    for (i = ivpci_dev->nvectors; i > 0; i--) {
        free_irq(ivpci_dev->msix_entries[i - 1].vector, ivpci_dev);
    }
    pci_disable_msix(ivpci_dev->dev);

    kfree(ivpci_dev->msix_names);
    kfree(ivpci_dev->msix_entries);
}

static int ivpci_open(struct inode *inode, struct file *filp)
{
    int minor = iminor(inode);
    struct ivpci_private *ivpci_dev;

    ivpci_dev = ivpci_find_private(minor);
    filp->private_data = (void *) ivpci_dev;
    BUG_ON(filp->private_data == NULL);

    dev_info(&ivpci_dev->dev->dev, PFX "open ivpci\n");

    return 0;
}

static int ivpci_mmap(struct file *filp, struct vm_area_struct *vma)
{
    int ret;
    unsigned long len;
    unsigned long off;
    unsigned long start;
    struct ivpci_private *ivpci_dev;

    ivpci_dev = (struct ivpci_private *)filp->private_data;
    BUG_ON(ivpci_dev == NULL);
    BUG_ON(ivpci_dev->base_addr == NULL);

    dev_info(&ivpci_dev->dev->dev, PFX "mmap ivpci bar2\n");

    /* `vma->vm_start` and `vma->vm_end` had aligned to page size */
    WARN_ON(offset_in_page(vma->vm_start));
    WARN_ON(offset_in_page(vma->vm_end));

    off = vma->vm_pgoff << PAGE_SHIFT;
    start = ivpci_dev->bar2_addr;

    /* Align up to page size. */
    len = PAGE_ALIGN((start & ~PAGE_MASK) + ivpci_dev->bar2_len);
    start &= PAGE_MASK;

    dev_info(&ivpci_dev->dev->dev, PFX "mmap vma pgoff: %lu, 0x%0lx - 0x%0lx," \
            " aligned length: %lu\n", vma->vm_pgoff, vma->vm_start,
            vma->vm_end, len);

    if (vma->vm_end - vma->vm_start + off > len) {
        dev_err(&ivpci_dev->dev->dev,
                PFX "mmap overflow the end, %lu - %lu + %lu > %lu",
                vma->vm_end, vma->vm_start, off, len);
        ret = -EINVAL;
        goto error;
    }

    off += start;
    vma->vm_pgoff = off >> PAGE_SHIFT;
    vma->vm_flags |= VM_IO|VM_SHARED|VM_DONTEXPAND|VM_DONTDUMP;

    if (io_remap_pfn_range(vma, vma->vm_start, off >> PAGE_SHIFT,
                vma->vm_end - vma->vm_start, vma->vm_page_prot)) {
        dev_err(&ivpci_dev->dev->dev, PFX "mmap bar2 failed\n");
        ret = -ENXIO;
        goto error;
    }

    ret = 0;

error:
    return ret;
}

static ssize_t ivpci_read(struct file *filp, char *buffer, size_t len,
        loff_t *poffset)
{
    int bytes_read = 0;
    unsigned long offset;
    struct ivpci_private *ivpci_dev;

    ivpci_dev = (struct ivpci_private *)filp->private_data;

    BUG_ON(ivpci_dev == NULL);
    BUG_ON(ivpci_dev->base_addr == NULL);


    offset = *poffset;
    dev_info(&ivpci_dev->dev->dev, PFX "read ivpci bar2, offset: %lu\n",
            offset);

    /* beyoud the end */
    if (len > ivpci_dev->bar2_len - offset) {
        len = ivpci_dev->bar2_len - offset;
    }

    if (len == 0) {
        return 0;
    }

    bytes_read = copy_to_user(buffer, (char *)ivpci_dev->base_addr + offset,
            len);
    if (bytes_read > 0) {
        dev_err(&ivpci_dev->dev->dev,
                PFX "read ivpci bar2, copy_to_user() failed: %d\n", bytes_read);
        return -EFAULT;
    }

    *poffset += len;
    return len;
}

static long ivpci_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
    int ret;
    struct ivpci_private *ivpci_dev;
    u16 ivposition;
    u16 vector;
    u32 value;

    ivpci_dev = (struct ivpci_private *)filp->private_data;

    BUG_ON(ivpci_dev == NULL);
    BUG_ON(ivpci_dev->base_addr == NULL);

    switch (cmd) {
    case IOCTL_RING:
        if (copy_from_user(&value, (u32 *) arg, sizeof(value)) ) {
            dev_err(&ivpci_dev->dev->dev, PFX "copy from user failed");
            return -1;
        }
        vector = value & 0xffff;
        ivposition = value & 0xffff0000 >> 16;
        dev_info(&ivpci_dev->dev->dev,
                PFX "ring doorbell: value: %u(0x%x), vector: %u, peer id: %u\n",
                value, value, vector, ivposition);
        writel(value & 0xffffffff, ivpci_dev->regs_addr + DOORBELL_OFF);
        break;

    case IOCTL_WAIT:
        dev_info(&ivpci_dev->dev->dev, PFX "wait for interrupt\n");
        // 该进程在 wait_queue 上等待
        ret = wait_event_interruptible(wait_queue, (event_toggle == 1));
        if (ret == 0) {
            dev_info(&ivpci_dev->dev->dev, PFX "wakeup\n");
            event_toggle = 0;
        } else if (ret == -ERESTARTSYS) {
            dev_err(&ivpci_dev->dev->dev, PFX "interrupted by signal\n");
            return ret;
        } else {
            dev_err(&ivpci_dev->dev->dev, PFX "unknown failed: %d\n", ret);
            return ret;
        }
        break;

    case IOCTL_IVPOSITION:
        dev_info(&ivpci_dev->dev->dev, PFX "get ivposition: %u\n",
                ivpci_dev->ivposition);
        if (copy_to_user((u32 *) arg, &ivpci_dev->ivposition, sizeof(u32))) {
            dev_err(&ivpci_dev->dev->dev, PFX "copy to user failed");
            return -1;
        }
        break;

    default:
        dev_err(&ivpci_dev->dev->dev, PFX "bad ioctl command: %d\n", cmd);
        return -1;
    }

    return 0;
}

static long ivpci_write(struct file *filp, const char *buffer, size_t len,
        loff_t *poffset)
{
    int bytes_written = 0;
    unsigned long offset;
    struct ivpci_private *ivpci_dev;

    ivpci_dev = (struct ivpci_private *)filp->private_data;

    BUG_ON(ivpci_dev == NULL);
    BUG_ON(ivpci_dev->base_addr == NULL);

    dev_info(&ivpci_dev->dev->dev, PFX "write ivpci bar2\n");

    offset = *poffset;

    if (len > ivpci_dev->bar2_len - offset) {
        len = ivpci_dev->bar2_len - offset;
    }

    if (len == 0) {
        return 0;
    }

    bytes_written = copy_from_user(ivpci_dev->base_addr + offset, buffer, len);
    if (bytes_written > 0) {
        return -EFAULT;
    }

    *poffset += len;
    return len;
}

static loff_t ivpci_lseek(struct file *filp, loff_t offset, int origin)
{
    loff_t retval = -1;
    struct ivpci_private *ivpci_dev;

    ivpci_dev = (struct ivpci_private *)filp->private_data;

    BUG_ON(ivpci_dev == NULL);
    BUG_ON(ivpci_dev->base_addr == NULL);

    dev_info(&ivpci_dev->dev->dev,
            PFX "lseek ivpci bar2, offset: %llu, origin: %d\n", offset, origin);

    switch (origin) {
    case 1:
        offset += filp->f_pos;
        /* fall through */
    case 0:
        retval = offset;
        if (offset > ivpci_dev->bar2_len) {
            offset = ivpci_dev->bar2_len;
        }
        filp->f_pos = offset;
    }
    return retval;
}

static int ivpci_release(struct inode *inode, struct file *filp)
{
    struct ivpci_private *ivpci_dev;
    ivpci_dev = (struct ivpci_private *)filp->private_data;

    BUG_ON(ivpci_dev == NULL);
    BUG_ON(ivpci_dev->base_addr == NULL);

    dev_info(&ivpci_dev->dev->dev, PFX "release ivpci\n");

    return 0;
}

static struct file_operations ivpci_ops = {
    .owner          = THIS_MODULE,
    .open           = ivpci_open,
    .mmap           = ivpci_mmap,
    .unlocked_ioctl = ivpci_ioctl,
    .read           = ivpci_read,
    .write          = ivpci_write,
    .llseek         = ivpci_lseek,
    .release        = ivpci_release,
};

static int ivpci_probe(struct pci_dev *pdev, const struct pci_device_id *id)
{
    int ret;
    struct ivpci_private *ivpci_dev;
    dev_t devno;
    // probing for device: 0000:00:1f.0
    dev_info(&pdev->dev, PFX "probing for device: %s\n", pci_name(pdev));

    if (g_ivpci_count >= g_max_devices) {
        dev_err(&pdev->dev, PFX "reach the maxinum number of devices, " \
                "please adapt the `g_max_devices` value, reload the driver\n");
        ret = -1;
        goto out;
    }
    // 启用设备
    ret = pci_enable_device(pdev);
    if (ret < 0) {
        dev_err(&pdev->dev, PFX "unable to enable device: %d\n", ret);
        goto out;
    }

    /* Reserved PCI I/O and memory resources for this device */
    ret = pci_request_regions(pdev, DRV_NAME);
    if (ret < 0) {
        dev_err(&pdev->dev, PFX "unable to reserve resources: %d\n", ret);
        goto disable_device;
    }

    ivpci_dev = ivpci_get_private();
    BUG_ON(ivpci_dev == NULL);

    pci_read_config_byte(pdev, PCI_REVISION_ID, &ivpci_dev->revision);
    // device 241:0, revision: 1
    dev_info(&pdev->dev, PFX "device %d:%d, revision: %d\n", g_ivpci_major,
            ivpci_dev->minor, ivpci_dev->revision);

    /* Pysical address of BAR0, BAR1, BAR2 */
    // 各个 bar 的物理地址
    ivpci_dev->bar0_addr = pci_resource_start(pdev, 0);
    ivpci_dev->bar0_len = pci_resource_len(pdev, 0);
    ivpci_dev->bar1_addr = pci_resource_start(pdev, 1);
    ivpci_dev->bar1_len = pci_resource_len(pdev, 1);
    ivpci_dev->bar2_addr = pci_resource_start(pdev, 2);
    ivpci_dev->bar2_len = pci_resource_len(pdev, 2);
    // BAR0: 0x81141100@0x100
    dev_info(&pdev->dev, PFX "BAR0: 0x%0x@0x%0x\n", ivpci_dev->bar0_addr,
            ivpci_dev->bar0_len);
    // BAR1: 0x0@0x0
    dev_info(&pdev->dev, PFX "BAR1: 0x%0x@0x%0x\n", ivpci_dev->bar1_addr,
            ivpci_dev->bar1_len);
    // BAR2: 0x800000@0x800000
    dev_info(&pdev->dev, PFX "BAR2: 0x%0x@0x%0x\n", ivpci_dev->bar2_addr,
            ivpci_dev->bar2_len);
    // BAR0 映射
    //ivpci_dev->regs_addr = ioremap(ivpci_dev->bar0_addr, ivpci_dev->bar0_len);
    ivpci_dev->regs_addr = pci_iomap(pdev, 0, 0x100);
    if (!ivpci_dev->regs_addr) {
        dev_err(&pdev->dev, PFX "unable to ioremap bar0, size: %d\n",
                ivpci_dev->bar0_len);
        goto release_regions;
    }
    // BAR2 映射
    //ivpci_dev->base_addr = ioremap(ivpci_dev->bar2_addr, ivpci_dev->bar2_len);
    ivpci_dev->base_addr = pci_iomap(pdev, 2, 0);
    if (!ivpci_dev->base_addr) {
        dev_err(&pdev->dev, PFX "unable to ioremap bar2, size: %d\n",
                ivpci_dev->bar2_len);
        goto iounmap_bar0;
    }
    // BAR2 map: 00000000c306fc7d
    dev_info(&pdev->dev, PFX "BAR2 map: %p\n", ivpci_dev->base_addr);

    /*
     * Create character device file.
     */
    cdev_init(&ivpci_dev->cdev, &ivpci_ops);
    ivpci_dev->cdev.owner = THIS_MODULE;

    devno = MKDEV(g_ivpci_major, ivpci_dev->minor);
    // 添加字符设备
    ret = cdev_add(&ivpci_dev->cdev, devno, 1);
    if (ret < 0) {
        dev_err(&pdev->dev, PFX "unable to add chrdev %d:%d to system: %d\n",
                g_ivpci_major, ivpci_dev->minor, ret);
        goto iounmap_bar2;
    }
    // 创建设备文件
    if (device_create(g_ivpci_class, NULL, devno, NULL, DRV_FILE_FMT,
                ivpci_dev->minor) == NULL)
    {
        dev_err(&pdev->dev, PFX "unable to create device file: %d:%d\n",
                g_ivpci_major, ivpci_dev->minor);
        goto delete_chrdev;
    }

    ivpci_dev->dev = pdev;
    pci_set_drvdata(pdev, ivpci_dev);
    // revision为1, 才支持 MSI-X
    if (ivpci_dev->revision == 1) {
        /* Only process the MSI-X interrupt. */
        ivpci_dev->ivposition = ioread32(ivpci_dev->regs_addr + IVPOSITION_OFF);
        // device ivposition: 1, MSI-X: yes
        dev_info(&pdev->dev, PFX "device ivposition: %u, MSI-X: %s\n",
                ivpci_dev->ivposition,
                (ivpci_dev->ivposition == 0) ? "no": "yes");

        if (ivpci_dev->ivposition != 0) {
            // 仅仅分配了4个中断向量并设置
            ret = ivpci_request_msix_vectors(ivpci_dev, 4);
            if (ret != 0) {
                goto destroy_device;
            }
        }
    }

    g_ivpci_count++;
    dev_info(&pdev->dev, PFX "device probed: %s\n", pci_name(pdev));
    return 0;

destroy_device:
    devno = MKDEV(g_ivpci_major, ivpci_dev->minor);
    device_destroy(g_ivpci_class, devno);
    ivpci_dev->dev = NULL;

delete_chrdev:
    cdev_del(&ivpci_dev->cdev);

iounmap_bar2:
    iounmap(ivpci_dev->base_addr);

iounmap_bar0:
    iounmap(ivpci_dev->regs_addr);

release_regions:
    pci_release_regions(pdev);

disable_device:
    pci_disable_device(pdev);

out:
    pci_set_drvdata(pdev, NULL);
    return ret;
}

static void ivpci_remove(struct pci_dev *pdev)
{
    int devno;
    struct ivpci_private *ivpci_dev;

    dev_info(&pdev->dev, PFX "removing ivshmem device: %s\n", pci_name(pdev));

    ivpci_dev = pci_get_drvdata(pdev);
    BUG_ON(ivpci_dev == NULL);

    ivpci_free_msix_vectors(ivpci_dev);

    ivpci_dev->dev = NULL;

    devno = MKDEV(g_ivpci_major, ivpci_dev->minor);
    device_destroy(g_ivpci_class, devno);

    cdev_del(&ivpci_dev->cdev);

    iounmap(ivpci_dev->base_addr);
    iounmap(ivpci_dev->regs_addr);

    pci_release_regions(pdev);
    pci_disable_device(pdev);
    pci_set_drvdata(pdev, NULL);
}

static struct pci_driver ivpci_driver = {
    .name       = DRV_NAME,
    .id_table   = ivpci_id_table,
    .probe      = ivpci_probe,
    .remove     = ivpci_remove,
};

static int __init ivpci_init(void)
{
    int ret, i, minor;

    pr_info(PFX "*********************************************************\n");
    pr_info(PFX "module loading\n");
    //int alloc_chrdev_region(dev_t *dev, unsigned baseminor, unsigned count, const char *name)
    // baseminor: 次设备号的起始
    // count: 申请次设备号的个数
    // name: 执行cat /proc/devices显示的名称
    // 自动分配 主次设备号
    ret = alloc_chrdev_region(&g_ivpci_devno, 0, g_max_devices, DRV_NAME);
    if (ret < 0) {
        pr_err(PFX "unable to allocate major number: %d\n", ret);
        goto out;
    }
    // 设备数据
    g_ivpci_devs = kzalloc(sizeof(struct ivpci_private) * g_max_devices,
            GFP_KERNEL);
    if (g_ivpci_devs == NULL) {
        goto unregister_chrdev;
    }
    // 次设备号
    minor = MINOR(g_ivpci_devno);
    for (i = 0; i < g_max_devices; i++) {
        g_ivpci_devs[i].minor = minor++;
    }
    // 创建 class
    g_ivpci_class = class_create(THIS_MODULE, DRV_NAME);
    if (g_ivpci_class == NULL) {
        pr_err(PFX "unable to create the struct class\n");
        goto free_devs;
    }
    // 主设备号
    g_ivpci_major = MAJOR(g_ivpci_devno);
    // major: 241, minor: 0
    pr_info(PFX "major: %d, minor: %d\n", g_ivpci_major, MINOR(g_ivpci_devno));
    // 注册 ivshmem 设备的驱动结构
    ret = pci_register_driver(&ivpci_driver);
    if (ret < 0) {
        pr_err(PFX "unable to register driver: %d\n", ret);
        goto destroy_class;
    }

    pr_info(PFX "module loaded\n");
    return 0;

destroy_class:
    class_destroy(g_ivpci_class);

free_devs:
    kfree(g_ivpci_devs);

unregister_chrdev:
    unregister_chrdev_region(g_ivpci_devno, g_max_devices);

out:
    return -1;
}

static void __exit ivpci_exit(void)
{
    pci_unregister_driver(&ivpci_driver);

    class_destroy(g_ivpci_class);

    kfree(g_ivpci_devs);

    unregister_chrdev_region(g_ivpci_devno, g_max_devices);

    pr_info(PFX "module unloaded\n");
    pr_info(PFX "*********************************************************\n");
}

/************************************************
 * Just for eliminating the compiling warnings.
 ************************************************/
#define my_module_init(initfn)					\
    static inline initcall_t __inittest(void)		\
    { return initfn; }					\
    int init_module(void) __cold __attribute__((alias(#initfn)));

#define my_module_exit(exitfn)					\
    static inline exitcall_t __exittest(void)		\
    { return exitfn; }					\
    void cleanup_module(void) __cold __attribute__((alias(#exitfn)));

my_module_init(ivpci_init);
my_module_exit(ivpci_exit);

MODULE_AUTHOR("haiwei-li@outlook.com");
MODULE_LICENSE("GPL");
MODULE_DESCRIPTION("Demo PCI driver for ivshmem device");
MODULE_VERSION(DRV_VERSION);
```

简单解释一下代码逻辑.

当**驱动被加载**时, 会调用 `ivpci_init`.

* 我们希望**所有的 ivshmem 设备**使用**相同的设备 major 号**, 因而在这里调用 `alloc_chrdev_region` 申请足够的 major 和 minor 号;

* 接着创建出足够的 ivshmem 设备所需的数据结构 `struct ivpci_private`, 并将 minor 号存储在这些结构中;

* 接着, 创建**设备文件 class**, 然后**注册** ivshmem 设备的**驱动结构**.

当 ivshmem 设备被检测到, probe 函数 `ivpci_probe` 会被调用.

* 我们调用 `pci_enable_device` **启用该设备**;

* 并将 PCI 设备的 **3 个 BAR 空间**映射到内存空间, 方便后续访问;

* 然后**创建对应的设备文件**, 给**用户态程序**提供访问接口.

* QEMU-2.6.0 之后, ivshmem 虚拟 PCI 设备配置空间的 revision 字段为 1, 这样才支持 MSI-X 中断机制, 所以我们只处理 revision 为 1 的 ivshmem 设备.

* 接下来, 函数 `ivpci_request_msix_vectors` 调用 `pci_enable_msix` 分配中断向量, 然后依次调用 `request_irq` 注册我们的中断处理程序: `ivpci_interrupt`.

示例程序里简单使用 **waitqueue** 来实现**等待和通知**. 当**用户态程序**以 `IOCTL_WAIT` 命令调用 ioctl 时, 就让该进程在 waitqueue 上进行**等待**. 当**中断触发**时, 我们的中断处理程序会调用 `wake_up_interruptible` 唤醒 waitqueue 上所等待的进程.

**设备文件**的文件操作的 mmap 实现可以把设备 **BAR2** 空间映射到用户空间, 与上一篇介绍的用户程序直接 mmap sysfs 的 resource2 文件作用是一致的.

示例的**用户态代码**, `./my/ioctl.c`

`ioctl.c` 如下:

```cpp
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <errno.h>
#include <fcntl.h>
#include <sys/ioctl.h>
#include <linux/types.h>
#include <assert.h>

#define IOCTL_MAGIC         ('f')
#define IOCTL_RING          _IOW(IOCTL_MAGIC, 1, __u32)
#define IOCTL_WAIT          _IO(IOCTL_MAGIC, 2)
#define IOCTL_IVPOSITION    _IOR(IOCTL_MAGIC, 3, __u32)

static void usage(void)
{
    printf("Usage: \n"  \
           "  ioctl <devfile> ivposition\n"    \
           "  ioctl <devfile> wait\n"          \
           "  ioctl <devfile> ring <peer_id> <vector_id>\n");
}

int main(int argc, char **argv)
{
    int fd, arg, ret, vector, peer;

    if (argc < 3) {
        usage();
        return -1;
    }

    fd = open(argv[1], O_RDWR);
    assert(fd != -1);

    ret = ioctl(fd, IOCTL_IVPOSITION, &peer);
    printf("IVPOSITION: %d\n", peer);

    if (strcmp(argv[2], "ivposition") == 0) {
        return 0;

    } if (strcmp(argv[2], "wait") == 0) {
        if (argc != 3) {
            usage();
            return -1;
        }
        printf("wait:\n");
        ret = ioctl(fd, IOCTL_WAIT, &arg);

    } else if (strcmp(argv[2], "ring") == 0) {
        if (argc != 5) {
            usage();
            return -1;
        }
        printf("ring:\n");
        peer = atoi(argv[3]);
        vector = atoi(argv[4]);

        arg = ((peer & 0xffff) << 16) | (vector & 0xffff);
        printf("arg: 0x%x\n", arg);

        ret = ioctl(fd, IOCTL_RING, &arg);

    } else {
        printf("Invalid command: %s\n", argv[2]);
        usage();
        return -1;
    }

    printf("ioctl finished, returned value: %d\n", ret);

    close(fd);

    return 0;
}
```

Makefile 如下:

```makefile
CONFIG_MODULE_SIG=n

obj-m += ivpci.o
all:
    make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules
    gcc -g -O0 ioctl.c -o ioctl
clean:
    make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean
    rm -rf ioctl
```

接下来, 在宿主机上运行 `ivshmem-server`:

```
sudo -u qemu /tmp/ivshmem-server -l 4M -M fg-doorbell -n 8 -F -v

cd contrib/ivshmem-server
./ivshmem-server -l 4M -M fg-doorbell -n 8 -F -v
*** Example code, do not use in production ***
Using POSIX shared memory: fg-doorbell
create & bind socket /tmp/ivshmem_socket
```

由于 `ivshmem-server` 分配的第一个 `peer_id` 为 0, 而当 ivshmem 设备的 **ivposition** 为 **0** 时, **无法判断**是不支持 `MSI-X` 中断还是分配的 `peer_id` 为 0. 所以我们可以先调用 ivshmem-client 程序来先连接 `ivshmem-server` 来占据 0 号 `peer_id`:

```
# cd build/contrib/ivshmem-client
# ./ivshmem-client
dump: dump peers (including us)
int <peer> <vector>: notify one vector on a peer
int <peer> all: notify all vectors of a peer
int all: notify all vectors of all peers (excepting us)
cmd> listen on server socket 3

cmd> dump
our_id = 0
  vector 0 is enabled (fd=5)
  vector 1 is enabled (fd=6)
  vector 2 is enabled (fd=7)
  vector 3 is enabled (fd=8)
  vector 4 is enabled (fd=9)
  vector 5 is enabled (fd=10)
  vector 6 is enabled (fd=11)
  vector 7 is enabled (fd=12)
cmd>
```

`ivshmem-server` 会多 log 是:

```
accept()=5
new peer id = 0
peer->sock_fd=5
```

接下来**启动两台虚拟机**, 先装载驱动模块:

```
insmod ./ivpci.ko
```

dmesg log 如下:

```
[  284.850013] [IVPCI] *********************************************************
[  284.850015] [IVPCI] module loading
[  284.850054] [IVPCI] major: 241, minor: 0
[  284.850088] [IVPCI] module loaded
```

再分别给两台虚拟机各动态添加一个 `ivshmem-plain` 和一个 `ivshmem-doorbell` 设备.

添加 `ivshmem-plain` 设备:

```
# nc -U /tmp/mon_test
object_add memory-backend-file,size=8M,share=true,mem-path=/dev/shm/shm3,id=shm3
device_add ivshmem-plain,memdev=shm3,bus=pci.0,addr=0x1f,master=on
```

`ivshmem-server` 没有增加任何 log, qemu 增加 log

```
IVSHMEM: using hostmem
```

虚拟机 dmesg 增加 log 如下:

```
[  710.661048] pci 0000:00:1f.0: [1af4:1110] type 00 class 0x050000
[  710.661322] pci 0000:00:1f.0: reg 0x10: [mem 0x00000000-0x000000ff]
[  710.661654] pci 0000:00:1f.0: reg 0x18: [mem 0x00000000-0x007fffff 64bit pref]
[  710.663840] pci 0000:00:1f.0: BAR 2: assigned [mem 0x380000800000-0x380000ffffff 64bit pref]
[  710.664808] pci 0000:00:1f.0: BAR 0: assigned [mem 0x81140100-0x811401ff]
[  710.665432] ivpci 0000:00:1f.0: [IVPCI] probing for device: 0000:00:1f.0
[  710.665517] ivpci 0000:00:1f.0: [IVPCI] device 241:0, revision: 1
[  710.665521] ivpci 0000:00:1f.0: [IVPCI] BAR0: 0x81140100@0x100
[  710.665524] ivpci 0000:00:1f.0: [IVPCI] BAR1: 0x0@0x0
[  710.665527] ivpci 0000:00:1f.0: [IVPCI] BAR2: 0x800000@0x800000
[  710.665562] ivpci 0000:00:1f.0: [IVPCI] BAR2 map: 00000000721d394b
[  710.666562] ivpci 0000:00:1f.0: [IVPCI] device ivposition: 0, MSI-X: no
[  710.666568] ivpci 0000:00:1f.0: [IVPCI] device probed: 0000:00:1f.0
```

通过 `cat /proc/devices`, 可以看到 241 的 ivpci

```
# cat /proc/devices
Character devices:
......
241 ivpci
```

同时, 查看 PCI 设备, 没有 `MSI-X` 的 capability.

```
# ll /dev/ivpci*
crw------- 1 root root 241, 0 May 31 08:05 /dev/ivpci0

# lspci
00:1f.0 RAM memory: Red Hat, Inc. Inter-VM shared memory (rev 01)

# lspci -vvv -s 00:1f.0
00:1f.0 RAM memory: Red Hat, Inc. Inter-VM shared memory (rev 01)
        Subsystem: Red Hat, Inc. QEMU Virtual Machine
        Physical Slot: 31
        Control: I/O+ Mem+ BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap- 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
        Region 0: Memory at 81140100 (32-bit, non-prefetchable) [size=256]
        Region 2: Memory at 380000800000 (64-bit, prefetchable) [size=8M]
        Kernel driver in use: ivpci
```

`cat /proc/interrupts` 也没有该设备使用的中断




添加 `ivshmem-doorbell` 设备:

```
# nc -U /tmp/mon_test
chardev-add socket,path=/tmp/ivshmem_socket,id=fg-doorbell
device_add ivshmem-doorbell,chardev=fg-doorbell,vectors=8
```

添加 `chardev-add` 时候, `ivshmem-server` 会多 log 是:

```
// vm1 添加时候多的 log
accept()=14
new peer id = 1
peer->sock_fd=5
peer->sock_fd=14

// vm2 添加时候多的 log
accept()=23
new peer id = 2
peer->sock_fd=5
peer->sock_fd=14
peer->sock_fd=23
```

`client` 查看所有 peer:

```
cmd> dump
our_id = 0
  vector 0 is enabled (fd=5)
  vector 1 is enabled (fd=6)
  vector 2 is enabled (fd=7)
  vector 3 is enabled (fd=8)
  vector 4 is enabled (fd=9)
  vector 5 is enabled (fd=10)
  vector 6 is enabled (fd=11)
  vector 7 is enabled (fd=12)
// vm1 的 peer
peer_id = 1
  vector 0 is enabled (fd=13)
  vector 1 is enabled (fd=14)
  vector 2 is enabled (fd=15)
  vector 3 is enabled (fd=16)
  vector 4 is enabled (fd=17)
  vector 5 is enabled (fd=18)
  vector 6 is enabled (fd=19)
  vector 7 is enabled (fd=20)
// vm2 的 peer
peer_id = 2
  vector 0 is enabled (fd=21)
  vector 1 is enabled (fd=22)
  vector 2 is enabled (fd=23)
  vector 3 is enabled (fd=24)
  vector 4 is enabled (fd=25)
  vector 5 is enabled (fd=26)
  vector 6 is enabled (fd=27)
  vector 7 is enabled (fd=28)
```

qemu 和 dmesg 此时还没有 log 增加.

`device_add` 时候, `ivshmem-server` 没有额外 log.

qemu 增加 log 如下:

```
IVSHMEM: using shared memory server (socket = unix:)
IVSHMEM: bumping storage to 16 peers
IVSHMEM: posn is -1, fd is 29
IVSHMEM: msix initialized (8 vectors)
IVSHMEM: posn is 0, fd is 30
IVSHMEM: eventfds[0][0] = 30
IVSHMEM: posn is 0, fd is 31
IVSHMEM: eventfds[0][1] = 31
IVSHMEM: posn is 0, fd is 32
IVSHMEM: eventfds[0][2] = 32
IVSHMEM: posn is 0, fd is 33
IVSHMEM: eventfds[0][3] = 33
IVSHMEM: posn is 0, fd is 34
IVSHMEM: eventfds[0][4] = 34
IVSHMEM: posn is 0, fd is 35
IVSHMEM: eventfds[0][5] = 35
IVSHMEM: posn is 0, fd is 36
IVSHMEM: eventfds[0][6] = 36
IVSHMEM: posn is 0, fd is 37
IVSHMEM: eventfds[0][7] = 37
IVSHMEM: posn is 1, fd is 38
IVSHMEM: eventfds[1][0] = 38
IVSHMEM: posn is 1, fd is 39
IVSHMEM: eventfds[1][1] = 39
IVSHMEM: posn is 1, fd is 40
IVSHMEM: eventfds[1][2] = 40
IVSHMEM: posn is 1, fd is 41
IVSHMEM: eventfds[1][3] = 41
IVSHMEM: posn is 1, fd is 42
IVSHMEM: eventfds[1][4] = 42
IVSHMEM: posn is 1, fd is 43
IVSHMEM: eventfds[1][5] = 43
IVSHMEM: posn is 1, fd is 44
IVSHMEM: eventfds[1][6] = 44
IVSHMEM: posn is 1, fd is 45
IVSHMEM: eventfds[1][7] = 45
IVSHMEM: posn is 2, fd is 46
IVSHMEM: eventfds[2][0] = 46
IVSHMEM: setting up interrupt for vector: 0
IVSHMEM: with irqfd, delayed until msix enabled
IVSHMEM: posn is 2, fd is 47
IVSHMEM: eventfds[2][1] = 47
IVSHMEM: setting up interrupt for vector: 1
IVSHMEM: with irqfd, delayed until msix enabled
IVSHMEM: posn is 2, fd is 48
IVSHMEM: eventfds[2][2] = 48
IVSHMEM: setting up interrupt for vector: 2
IVSHMEM: with irqfd, delayed until msix enabled
IVSHMEM: posn is 2, fd is 49
IVSHMEM: eventfds[2][3] = 49
IVSHMEM: setting up interrupt for vector: 3
IVSHMEM: with irqfd, delayed until msix enabled
IVSHMEM: posn is 2, fd is 50
IVSHMEM: eventfds[2][4] = 50
IVSHMEM: setting up interrupt for vector: 4
IVSHMEM: with irqfd, delayed until msix enabled
IVSHMEM: posn is 2, fd is 51
IVSHMEM: eventfds[2][5] = 51
IVSHMEM: setting up interrupt for vector: 5
IVSHMEM: with irqfd, delayed until msix enabled
IVSHMEM: posn is 2, fd is 52
IVSHMEM: eventfds[2][6] = 52
IVSHMEM: setting up interrupt for vector: 6
IVSHMEM: with irqfd, delayed until msix enabled
IVSHMEM: posn is 2, fd is 53
IVSHMEM: eventfds[2][7] = 53
IVSHMEM: setting up interrupt for vector: 7
IVSHMEM: with irqfd, delayed until msix enabled
IVSHMEM: ivshmem_add_kvm_msi_virq vector:0
IVSHMEM: ivshmem_add_kvm_msi_virq vector:1
IVSHMEM: ivshmem_add_kvm_msi_virq vector:2
IVSHMEM: ivshmem_add_kvm_msi_virq vector:3
IVSHMEM: ivshmem_add_kvm_msi_virq vector:4
IVSHMEM: ivshmem_add_kvm_msi_virq vector:5
IVSHMEM: ivshmem_add_kvm_msi_virq vector:6
IVSHMEM: ivshmem_add_kvm_msi_virq vector:7
IVSHMEM: vector poll 0x5d073de6f400 0-8
IVSHMEM: vector unmask 0x5d073de6f400 0
IVSHMEM: vector unmask 0x5d073de6f400 1
IVSHMEM: vector unmask 0x5d073de6f400 2
IVSHMEM: vector unmask 0x5d073de6f400 3
```

dmesg 增加 log 如下:

```
[ 1900.758415] pci 0000:00:06.0: [1af4:1110] type 00 class 0x050000
[ 1900.758682] pci 0000:00:06.0: reg 0x10: [mem 0x00000000-0x000000ff]
[ 1900.758840] pci 0000:00:06.0: reg 0x14: [mem 0x00000000-0x00000fff]
[ 1900.759038] pci 0000:00:06.0: reg 0x18: [mem 0x00000000-0x003fffff 64bit pref]
[ 1900.761696] pci 0000:00:06.0: BAR 2: assigned [mem 0x380000400000-0x3800007fffff 64bit pref]
[ 1900.762617] pci 0000:00:06.0: BAR 1: assigned [mem 0x81144000-0x81144fff]
[ 1900.763083] pci 0000:00:06.0: BAR 0: assigned [mem 0x81140200-0x811402ff]
[ 1900.764446] ivpci 0000:00:06.0: [IVPCI] probing for device: 0000:00:06.0
[ 1900.764528] ivpci 0000:00:06.0: [IVPCI] device 241:1, revision: 1
[ 1900.764532] ivpci 0000:00:06.0: [IVPCI] BAR0: 0x81140200@0x100
[ 1900.764536] ivpci 0000:00:06.0: [IVPCI] BAR1: 0x81144000@0x1000
[ 1900.764538] ivpci 0000:00:06.0: [IVPCI] BAR2: 0x400000@0x400000
[ 1900.764575] ivpci 0000:00:06.0: [IVPCI] BAR2 map: 0000000054fa5536
[ 1900.765844] ivpci 0000:00:06.0: [IVPCI] device ivposition: 1, MSI-X: yes
[ 1900.765848] ivpci 0000:00:06.0: [IVPCI] request msi-x vectors: 4
[ 1900.767713] ivpci 0000:00:06.0: [IVPCI] irq for msix entry: 0, vector: 30
[ 1900.767900] ivpci 0000:00:06.0: [IVPCI] irq for msix entry: 1, vector: 31
[ 1900.768079] ivpci 0000:00:06.0: [IVPCI] irq for msix entry: 2, vector: 32
[ 1900.768249] ivpci 0000:00:06.0: [IVPCI] irq for msix entry: 3, vector: 33
[ 1900.768252] ivpci 0000:00:06.0: [IVPCI] device probed: 0000:00:06.0
```

可以看到两个 ivshmem 设备都被检测到, 驱动只对 `0000:00:06.0` 设备进行了中断处理.

同时, `/dev` 下看到 ivpci 设备

```
# ll /dev/ivpci*
crw------- 1 root root 241, 0 May 31 07:21 /dev/ivpci0
crw------- 1 root root 241, 1 May 31 07:40 /dev/ivpci1
```

通过 lspci 查看该设备, 可以看到 `MSI-X` 的 Capabilities 结构显示支持 `MSI-X` 机制

```
# lspci
00:06.0 RAM memory: Red Hat, Inc. Inter-VM shared memory (rev 01)
00:1f.0 RAM memory: Red Hat, Inc. Inter-VM shared memory (rev 01)

# lspci -vvv -s 00:06.0
00:06.0 RAM memory: Red Hat, Inc. Inter-VM shared memory (rev 01)
        Subsystem: Red Hat, Inc. QEMU Virtual Machine
        Physical Slot: 6
        Control: I/O+ Mem+ BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+
        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
        Region 0: Memory at 81140200 (32-bit, non-prefetchable) [size=256]
        Region 1: Memory at 81144000 (32-bit, non-prefetchable) [size=4K]
        Region 2: Memory at 380000400000 (64-bit, prefetchable) [size=4M]
        Capabilities: [40] MSI-X: Enable+ Count=8 Masked-
                Vector table: BAR=1 offset=00000000
                PBA: BAR=1 offset=00000800
        Kernel driver in use: ivpci

# lspci -vvv -s 00:1f.0
00:1f.0 RAM memory: Red Hat, Inc. Inter-VM shared memory (rev 01)
        Subsystem: Red Hat, Inc. QEMU Virtual Machine
        Physical Slot: 31
        Control: I/O+ Mem+ BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap- 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
        Region 0: Memory at 81140100 (32-bit, non-prefetchable) [size=256]
        Region 2: Memory at 380000800000 (64-bit, prefetchable) [size=8M]
        Kernel driver in use: ivpci
```

查看系统的中断信息, 可以看到 30-33 的中断由我们的驱动来处理:

```
# cat /proc/interrupts
           CPU0       CPU1
 30:          0          0   PCI-MSI 114688-edge      ivpci1-0
 31:          0          0   PCI-MSI 114689-edge      ivpci1-1
 32:          0          0   PCI-MSI 114690-edge      ivpci1-2
 33:          0          0   PCI-MSI 114691-edge      ivpci1-3
```

在其中一台虚拟机上执行用户态程序, 进程被阻塞, `peer_id` 为 2:

```
# ./ioctl /dev/ivpci1 wait
IVPOSITION: 2
wait:

```

可以看到此时进程进入睡眠状态:

```
# ps aux | grep -i ioctl
root         967  0.0  0.0   2776   936 pts/0    S+   08:19   0:00 ./ioctl /dev/ivpci1 wait
root         969  0.0  0.1   7004  2188 ttyS0    R+   08:20   0:00 grep --color=auto -i ioctl
```

查看当前机器的 dmesg

```
[ 1087.379217] ivpci 0000:00:06.0: [IVPCI] open ivpci
[ 1087.379226] ivpci 0000:00:06.0: [IVPCI] get ivposition: 2
[ 1087.379318] ivpci 0000:00:06.0: [IVPCI] wait for interrupt
```

在另一台机器上执行用户态程序触发对端 2 的 1 号中断:

```
# ./ioctl /dev/ivpci1 ring 2 1
IVPOSITION: 1
ring:
arg: 0x20001
ioctl finished, returned value: 0
```

查看当前机器的 dmesg:

```
[ 1318.627275] ivpci 0000:00:06.0: [IVPCI] open ivpci
[ 1318.627286] ivpci 0000:00:06.0: [IVPCI] get ivposition: 1
[ 1318.627370] ivpci 0000:00:06.0: [IVPCI] ring doorbell: value: 131073(0x20001), vector: 1, peer id: 1
[ 1318.640719] ivpci 0000:00:06.0: [IVPCI] release ivpci
```

此时, peer_id 为 2 的虚拟机上的用户态进程将从睡眠状态返回:

```
# ./ioctl /dev/ivpci1 wait
IVPOSITION: 2
wait:
ioctl finished, returned value: 0
```

此时 dmesg 是:

```
[ 1325.028912] ivpci 0000:00:06.0: [IVPCI] interrupt: 31
[ 1325.028988] ivpci 0000:00:06.0: [IVPCI] interrupt: 31
...
[ 1325.029089] ivpci 0000:00:06.0: [IVPCI] interrupt: 31
[ 1325.029105] ivpci 0000:00:06.0: [IVPCI] interrupt: 31
[ 1325.029105] ivpci 0000:00:06.0: [IVPCI] wakeup
[ 1325.029122] ivpci 0000:00:06.0: [IVPCI] interrupt: 31
[ 1325.029146] ivpci 0000:00:06.0: [IVPCI] interrupt: 31
[ 1325.029157] ivpci 0000:00:06.0: [IVPCI] interrupt: 31
[ 1325.029158] ivpci 0000:00:06.0: [IVPCI] release ivpci
[ 1325.029175] ivpci 0000:00:06.0: [IVPCI] interrupt: 31
[ 1325.029193] ivpci 0000:00:06.0: [IVPCI] interrupt: 31
...
[ 1325.042035] ivpci 0000:00:06.0: [IVPCI] interrupt: 31
[ 1325.042047] ivpci 0000:00:06.0: [IVPCI] interrupt: 31
```

关闭一个虚拟机或者移除设备, 另外一个虚拟机的 qemu log

```
IVSHMEM: posn is 1, fd is -1
IVSHMEM: posn 1 has gone away
```

`ivshmem-server` log

```
peer->sock_fd=5
peer->sock_fd=14
free peer 1
peer->sock_fd=23
```

client 端查看 peer

```
cmd> dump
our_id = 0
  vector 0 is enabled (fd=5)
  vector 1 is enabled (fd=6)
  vector 2 is enabled (fd=7)
  vector 3 is enabled (fd=8)
  vector 4 is enabled (fd=9)
  vector 5 is enabled (fd=10)
  vector 6 is enabled (fd=11)
  vector 7 is enabled (fd=12)
peer_id = 2
  vector 0 is enabled (fd=21)
  vector 1 is enabled (fd=22)
  vector 2 is enabled (fd=23)
  vector 3 is enabled (fd=24)
  vector 4 is enabled (fd=25)
  vector 5 is enabled (fd=26)
  vector 6 is enabled (fd=27)
  vector 7 is enabled (fd=28)
```

另外一个虚拟机也关机, `ivshmem-server` log 为

```
peer->sock_fd=5
peer->sock_fd=23
free peer 2
```

client 也关闭, `ivshmem-server` log 为

```
peer->sock_fd=5
free peer 0
```


使用中断机制, 可以避免各 peer 通过轮询进行消息通知, 性能更优. 但 ivshmem-server 的实现以及 ivshmem 机制的服务端与客户端之间的协议设计本身的生产可用性还是有所不足. 比如上边提到的第一个 peer_id 为 0. 而 peer_id 的分配机制本身所携带的信息过少, ivshmem-server 没有足够的信息来区分哪个设备的 peer_id 各是多少. 在实际生产应用中, 需要独立实现注册中心来收集各设备 peer_id. 这样还需要注册中心与虚拟机内的程序、宿主机上的辅助程序都要具备网络连通性. 这本身会对云平台的网络结构带来比较大的依赖. 在生产环境中使用中断机制建议对 ivshmem-server 进行较大完善.


