
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [NUMA 和 UMA](#numa-和-uma)
- [1 numastat](#1-numastat)
- [2 numad](#2-numad)
  - [2.1 numad 的参数](#21-numad-的参数)
  - [2.1 numad 相关操作](#21-numad-相关操作)
    - [2.1.1 启动和退出 numad](#211-启动和退出-numad)
    - [2.1.2 numad 将 QEMU 进程绑定到 numa 节点](#212-numad-将-qemu-进程绑定到-numa-节点)
      - [2.1.2.1 auto NUMA balancing: 没有 numad](#2121-auto-numa-balancing-没有-numad)
      - [2.1.2.2 打开 numad](#2122-打开-numad)
- [3 numactl](#3-numactl)
  - [3.1 numactl 主要用法](#31-numactl-主要用法)
  - [3.2 numactl 示例](#32-numactl-示例)
    - [3.2.1 查看 numa 节点信息](#321-查看-numa-节点信息)
    - [3.2.2 numactl 启动客户机](#322-numactl-启动客户机)
      - [3.2.2.1 只运行在一个节点上](#3221-只运行在一个节点上)
      - [3.2.2.2 均匀占用节点资源](#3222-均匀占用节点资源)
- [4 小结](#4-小结)

<!-- /code_chunk_output -->

# NUMA 和 UMA

NUMA(Non\-Uniform Memory Access, 非统一内存访问架构)是相对于 UMA(Uniform Memory Access)而言的. 早年的计算机架构都是 UMA, 如图 7\-2 所示. 所有的 CPU 处理单元(Processor)均质地通过共享的总线访问内存, 所有 CPU 访问所有内存单元的速度是一样的. 在多处理器的情形下, 多个任务会被分派在各个处理器上并发执行, 则它们**竞争内存资源**的情况会非常频繁, 从而引起效率的下降.

![](./images/2019-05-28-21-14-56.png)

所以, 随着多处理器架构的逐渐普及以及数量的不断增长, NUMA 架构兴起, 如图 7\-3 所示. 处理器与内存被划分成一个个的节点(node), 处理器访问自己节点内的内存会比访问其他节点的内存快.

![](./images/2019-05-28-21-15-17.png)

Intel Xeon 系列平台从**2007 年**的**Nehalem**那一代开始, 就支持 NUMA 架构了. 现在主流的 E5、E7 系列 Xeon 平台, 通常是 2 个、4 个 NUMA node 的.

# 1 numastat

numastat 用来查看**某个(些)进程**或者**整个系统**的**内存消耗**在**各个 NUMA 节点的分布情况**.

它的典型输出如下:

```
[root@kvm-host ~]# numastat
                           node0           node1
numa_hit                72050204        55925951
numa_miss                      0               0
numa_foreign                   0               0
interleave_hit             38068           39139
local_node              71816493        54027058
other_node                233711         1898893
```

- **numa\_hit**表示成功地从**该节点**分配到的**内存页数**.
- **numa\_miss**表示成功地从该节点分配到的**内存页数**, 但其本意是希望从别的节点分配, 失败以后退而求其次从该节点分配.
- numa\_foreign 与 numa\_miss 互为"影子", **每个 numa\_miss**都来自**另一个节点的 numa\_foreign**.
- interleave\_hit, 有时候内存请求是没有 NUMA 节点偏好的, 此时会**均匀分配自各个节点(interleave**), 这个数值就是这种情况下从该节点分配出去的内存页面数.
- local\_node 表示分配给运行在**同一节点的进程**的**内存页数**.
- other\_node 与上面相反. **local\_node**值**加上 other\_node**值就是**numa\_hit 值**.

以上数值**默认**都是**内存页数**, 要看具体**多少 MB**, 可以通过加上\-**n 参数**实现.

numastat 还可以**只看某些进程**, 甚至只要名字片段匹配. 比如看 QEMU 进程的内存分布情况, 可以通过"**numastat qemu**"即可. 比如:

```
[root@kvm-host ~]# numastat qemu

Per-node process memory usage (in MBs) for PID 73658 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                         0.00           21.96           21.96
Stack                        0.00            8.37            8.37
Private                      0.03         1538.17         1538.20
----------------  --------------- --------------- ---------------
Total                        0.03         1568.49         1568.52
```

更多参数及用法, 大家可以通过 man numastat 命令了解.

# 2 numad

numad 是一个可以**自动管理 NUMA 亲和性**(affinity)的**工具**(同时也是一个后台进程).

- 它**实时监控 NUMA 拓扑结构(topology**)和**资源使用**,
- 并**动态调整**.
- 同时它还可以在**启动一个程序前**, 提供**NUMA 优化建议**.

与 numad 功能类似, Kernel 的**auto NUMA balancing**(/proc/sys/kernel/**numa\_balancing**)也是进行**动态 NUMA 资源的调节**.

numad 启动后会**覆盖**Kernel 的 auto NUMA balancing 功能.

numad 与 THP 和 KSM 都有些纠葛, 下面讲到.

## 2.1 numad 的参数

numad 比较复杂, 它有很多参数进行**精细化控制**, 下面是几个重要的.

- \-p\<pid\>, \-x\<pid\>, \-r\<pid\>, 分别指定 numad**针对哪些 pid**以及**不针对哪些 pid**进行**自动的 NUMA 资源优化**.

numad 自己**内部**维护一个**inclusive list**和一个**exclusive list**; \-p\<pid\>、\-x\<pid\>就是分别往这两个 list 里面**添加进程 id**; \-r\<pid\>就是从这两个**list 里面移除**.

在 numad**首次启动**时候, 可以**重复多个**\-p 或者\-x; 启动后, 每次调用 numad 只能跟一个\-p、\-x 或者\-r 参数.

**默认没有这些指定**的话, numad 会对系统**所有进程进行 NUMA 资源优化**.

- \-S 0\/1, **0**表示只对**inclusive list**里面的进程进行 NUMA 优化; **1**表示对**除 exclusive list**以外的**所有进程**进行优化. 通常, \-S 与\-p、\-x 搭配使用.

- \-R\<cpu\_list\>, reserve, 指定一些 CPU 是 numad 不能染指的, numad**不会**在自动优化 NUMA 资源的时候把**进程放到这些 CPU 上**去运行.

- \-t\<百分比\>, 它指示**逻辑(logical)CPU**(比如 Intel HyperThread 打开时)运算能力对于它的**HW core**的比例.

这个值关系到**numad 内部调配资源**时的计算, **默认是 20%**.

- \-u\<百分比\>, numad 最多能消耗**每个 NUMA 节点多少资源**(CPU 和内存), 默认是**85%**.

numad 毕竟**不能取代内核调度器**, 并不能接管系统里所有 route 的调度, 所以, 留有余地是必须的. 但当你确定将一个 node 专属(dedicate)给一个进程时, 也可以设置\-u 100%, 甚至超过 100%, 但要小心.

- \-C 0\/1, 是否将 NUMA 节点的 inactive file cache 作为 free memory 对待. 默认为 1, 表示进程的 inactive file cache 不纳入 NUMA 优化的考量, 即如果一个进程还有一些 inactive file cache 留在另一个节点上, numad 也不会把它搬过来.

- \-K 0\/1, 控制是否将 interleaved memory 合并到一个 NUMA 节点. 默认是合并的, 但要注意, 合并到一个节点并不一定有最好的 performance, 而应该根据实际的 work\-load 来决定. 比如, 如果一个系统里主要就是一个大型的数据库应用程序(大量内存访问且地址随机), \-K 1 禁止 numad 合并 interleaved memory 反而有更好的性能.

- \-m\<百分比\>, 它是一个阈值, 表示当内存中在本地节点的数量达到它所属进程的内存总量的多少时, numad 停止对该进程的 NUMA 优化.

- \-i\<最小间隔: 最大间隔\>, 最小值可以省略. 它设置 numad 2 次扫描系统情况的时间间隔. 通常用它来终止(退出)numad, -i 0.

- \-H\<时间间隔\>, 它设置(**override)透明大页**(见 7.2 节)的**扫描间隔时间**.

**默认地**, numad 会将/sys/kernel/mm/tranparent\_hugepage/khugepaged/**scan\_sleep\_millisecs**值从默认的**10000 毫秒**缩短为**1000 毫秒**, 因为**更激进的透明大页合并**更有利于 numad 将页面在**NUMA 节点之间迁移**.

- \-w\<NCPUS\[:MB]\>, 它就是 numad**一次性地运行一下**(而**不是作为系统后台 daemon**)以供咨询: "我有一个应用程序将要运行, 它会需要 NCPUS 个 CPU, M 兆内存, numad, 你告诉我该把它放到哪个 NUMA 节点上运行好啊?"**numad**此时会**返回**一个**合适的 NUMA node list**, 这个 list 可以作为后面**numactl**(下面介绍)的**参数**.

另外, **与 THP 的关联**, 上面\-**H 参数**已涉及.

**与 KSM 的关联**, 在于/sys/kernel/mm/ksm/**merge\_nodes**最好**设置为 0**, 禁止 KSM 跨 NUMA 节点地同页合并.

## 2.1 numad 相关操作

我们通过几个典型用例来了解上面部分重要参数的用法.

### 2.1.1 启动和退出 numad

1)**启动和退出 numad**; 退出通过"\-**i 0**"完成.

```
[root@kvm-host ~]# numad

[root@kvm-host ~]# ps aux | grep numad
root   175821  0.2  0.0  19860  572 ?      Ssl  20:44  0:00 numad
root   175827  0.0  0.0 112652  960 pts/2  S+   20:44  0:00 grep --color=auto numad

[root@kvm-host ~]# numad -i 0      #退出 numad
[root@kvm-host ~]# ps aux | grep numad
root   175836  0.0  0.0 112652 960 pts/2    S+  20:44  0:00 grep --color=auto numad
```

### 2.1.2 numad 将 QEMU 进程绑定到 numa 节点

通过**numad**将 QEMU 进程**搬到一个 NUMA 节点**上.

#### 2.1.2.1 auto NUMA balancing: 没有 numad

先看看**没有 numad**的时候, 内核的**auto NUMA balancing**会是怎样的行为.

查看当前情况, 并启动一个客户机.

```
[root@kvm-host ~]# cat /proc/sys/kernel/numa_balancing
1

[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot
```

启动后, 查看 numastat. 可以看到, **客户机**所使用的内存, 在**两个节点上都有分布**.

```
[root@kvm-host ~]# numastat qemu-system

Per-node process memory usage (in MBs) for PID 61898 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                        19.80            0.36           20.17
Stack                        8.23            0.11            8.34
Private                    127.71          893.88         1021.60
----------------  --------------- --------------- ---------------
Total                      155.75          894.36         1050.11
```

接下来在**客户机里进行编译内核的行为**, 并时不时地在宿主机中查看其 numastat 的状况.

```
[root@kvm-guest linux-4.9]# make -j 4
```

随着**客户机的运行**, 可以看到**两个节点**依然各自分布着一些**内存占用**.

```
[root@kvm-host ~]# numastat qemu-system

Per-node process memory usage (in MBs) for PID 61898 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                        20.69            1.71           22.40
Stack                       10.41            6.21           16.63
Private                   1192.11         2469.65         3661.76
----------------  --------------- --------------- ---------------
Total                     1223.21         2477.57         3700.79
```

#### 2.1.2.2 打开 numad

接下来我们**打开 numad**, 并重做上面的实验.

可以看到, 一开始**打开 numad 之前**, 客户机(QEMU 进程)的内存**分布于两个节点**上, 随着客户机的运行, **numad**把内存都搬到**一个节点**上了.

```
[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot

[root@kvm-host ~]# ps aux | grep -i qemu
root  61898  130  0.7 10604948 996800 pts/1 Sl+  15:08   0:19 qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot
root  61959  0.0  0.0 112652  976 pts/8  S+  15:09  0:00 grep --color=auto -i qemu

[root@kvm-host ~]# numad -S 0 -p 64686

[root@kvm-host ~]# numastat 64686

Per-node process memory usage (in MBs) for PID 64686 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                         5.05           16.94           21.98
Stack                        0.06            2.06            2.12
Private                    137.43         1121.89         1259.32
----------------  --------------- --------------- ---------------
Total                      142.54         1140.89         1283.43
[root@kvm-host ~]# numastat 64686

Per-node process memory usage (in MBs) for PID 64686 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                         0.00           22.09           22.09
Stack                        0.00           20.58           20.58
Private                      0.00         2105.27         2105.27
----------------  --------------- --------------- ---------------
Total                        0.00         2147.93         2147.93
```

# 3 numactl

如果说**numad**是事后(**客户机起来以后**)**调节 NUMA 资源分配**, 那么**numactl**则是**主动**地在程序起来时候就**指定好它的 NUMA 节点**.

numactl 其实不止它名字表示的那样

- **设置 NUMA 相关亲和性**,
- 它还可以**设置共享内存/大页文件系统的内存策略**,
- 以及**进程的 CPU 和内存的亲和性**.

## 3.1 numactl 主要用法

它的主要用法如下:

```
numactl  [  --all  ]  [ --interleave nodes ] [ --preferred node ] [ --membind nodes ] [ --cpunodebind nodes ] [ --physcpubind cpus ] [ --localalloc ] [--] command {arguments...}
```

它的一些主要参数如下:
- \-\-hardware, 列出来目前系统中**可用的 NUMA 节点**, 以及它们**之间的距离**(distance).
- \-\-membind, 确保**command 执行**时候**内存都是从指定的节点**上分配; 如果该节点没有足够内存, 返回失败.
- \-\-cpunodebind, 确保 command**只在指定 node 的 CPU**上面执行.
- \-\-phycpubind, 确保 command**只在指定的 CPU**上执行.
- \-\-localalloc, 指定**内存只从本地节点**上分配.
- \-\-preferred, 指定一个偏好的节点, command 执行时内存**优先从这个节点分配**, 不够的话才从别的节点分配.

其他还有更多参数, 读者可以通过"man numactl"命令了解.

## 3.2 numactl 示例

我们还是通过一个简单的例子来了解一下 numactl 的一般用法.

### 3.2.1 查看 numa 节点信息

通过\-\-hardware, 我们可以看到, 笔者系统上有两个 NUMA 节点, 相互间的 distance 为 21.

```
[root@kvm-host ~]# numactl --hardware
available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65
node 0 size: 65439 MB
node 0 free: 43948 MB
node 1 cpus: 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87
node 1 size: 65536 MB
node 1 free: 53230 MB
node distances:
node   0   1
  0:  10  21
  1:  21  10
```

### 3.2.2 numactl 启动客户机

#### 3.2.2.1 只运行在一个节点上

我们用 numactl 来**控制启动一个客户机**, 让它**只运行在节点 1**上, 然后通过 numastat 来确认. 可以看到, 这个 qemu\-system\-x86\_64 进程, 一开始就被绑定到了节点 1 上.

```
[root@kvm-host ~]# numactl --membind=1 --cpunodebind=1 -- qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot --daemonize
[root@kvm-host ~]# numastat qemu-system

Per-node process memory usage (in MBs) for PID 72511 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                         0.00           23.03           23.03
Stack                        0.00           10.41           10.41
Private                      0.00          979.96          979.96
----------------  --------------- --------------- ---------------
Total                        0.00         1013.41         1013.41
```

#### 3.2.2.2 均匀占用节点资源

我们如果想让客户机均匀地占用两个节点的资源, 可以使用\-\-interleave 参数.

```
[root@kvm-host ~]# numactl --interleave=0,1 -- qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot --daemonize
[root@kvm-host ~]# numastat qemu-system

Per-node process memory usage (in MBs) for PID 73119 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                        11.35           11.55           22.89
Stack                        6.12            2.13            8.25
Private                    470.18          467.83          938.01
----------------  --------------- --------------- ---------------
Total                      487.65          481.50          969.15
```

# 4 小结

综上, 我们介绍了**numastat**、**numad**、**numactl**三个常用的 NUMA 控制、查看的工具.

- **numad**可以在程序(包括客户机的 QEMU 进程)起来以后, **事后调节**、**优化其 NUMA 资源**.
- **numactl**则是在**一开始**就指定好这个命令的**NUMA 政策**.
- **numastat**则可以用来方便地**查看当前系统**或**某个进程的 NUMA 资源**使用分布情况.

在想要专属地让**某个客户机**得到**优先服务**的时候, 我们可以把**KSM 关闭**, 通过**numactl**将客户机**QEMU 进程绑定在某个 node 或某些 CPU**上.

当我们想要**更高的客户机密度**, 而**不考虑特别的服务质量**的时候, 我们可以通过**numactl \-\-all**, 同时**打开 KSM**, **关掉 numad**.
