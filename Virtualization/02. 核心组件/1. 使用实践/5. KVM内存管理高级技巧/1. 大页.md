
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 大页的介绍](#1-大页的介绍)
  - [1.1. 大页的优点](#11-大页的优点)
  - [1.2. 内核的支持](#12-内核的支持)
  - [1.3. 系统大页信息查看](#13-系统大页信息查看)
    - [1.3.1. /proc/meminfo](#131-procmeminfo)
    - [1.3.2. /proc/sys/vm/ 下](#132-procsysvm-下)
    - [1.3.3. /sys/kernel/mm/hugepages/ 目录](#133-syskernelmmhugepages-目录)
- [2. KVM 虚拟化对大页的利用](#2-kvm-虚拟化对大页的利用)
  - [2.1. 一般应用程序使用大页的方式](#21-一般应用程序使用大页的方式)
  - [2.2. KVM 对大页的使用](#22-kvm-对大页的使用)
  - [2.3. KVM 使用大页操作](#23-kvm-使用大页操作)
    - [2.3.1. 检查硬件是否支持大页](#231-检查硬件是否支持大页)
    - [2.3.2. 检查系统是否支持 hugetlbfs](#232-检查系统是否支持-hugetlbfs)
    - [2.3.3. 检查大页池有足够大页](#233-检查大页池有足够大页)
    - [2.3.4. 创建 hugetlbfs 的挂载点](#234-创建-hugetlbfs-的挂载点)
    - [2.3.5. 创建客户机](#235-创建客户机)
  - [2.4. 2MB 大页的相关命令](#24-2mb-大页的相关命令)
  - [2.5. 大页的优缺点](#25-大页的优缺点)

<!-- /code_chunk_output -->

# 1. 大页的介绍

x86(包括 x86\-32 和 x86\-64)架构的 CPU**默认使用 4KB 大小的内存页面**, 但是它们也支持较大的内存页, 如 x86\-64 系统就支持 2MB 及 1GB 大小的大页(Huge Page). Linux 2.6 及以上的内核都支持 Huge Page.

## 1.1. 大页的优点

如果在系统中使用了 Huge Page, 则内存页的数量会减少, 从而需要更少的页表(Page Table), 节约了页表所占用的内存数量, 并且所需的地址转换也减少了, TLB 缓存失效的次数就减少了, 从而提高了内存访问的性能.

另外, 由于**地址转换所需的信息**一般保存在**CPU 的缓存**中, Huge Page 的使用让**地址转换信息减少**, 从而**减少了 CPU 缓存**的使用, **减轻了 CPU 缓存的压力**, 让 CPU 缓存能**更多**地用于应用程序的**数据缓存**, 也能够在整体上提升系统的性能.

## 1.2. 内核的支持

**编译内核**时候, 下面这些 Config 选项与 Huge Page 相关, 需要在内核配置文件中使能.

```
CONFIG_HUGETLBFS=y
CONFIG_HUGETLB_PAGE=y
```

内核**启动**时候的**参数**中, 与大页相关的如下:

- **default_hugepagesz**, 表示**默认的大页的大小**, 可以是**2MB**或者**1GB**.

- **hugepages**, 表示内核启动后给系统准备的**大页的数量**.

- **hugepagesz**, 表示内核启动后给系统准备的**大页的大小**, 可以是 2MB 或者 1GB.

hugepages 和 hugepagesz 可以**组合交替出现！！！**, 表示不同大小的大页**分别准备多少！！！**.

如下, 我们以 hugepages=64、hugepagesz=1G、hugepages=16 组合启动内核, 故意**省略**掉第一组中**hugepagesz**的设置

可以看到, 前面**不指定 hugepagesz**的情况, 分配的**64 个大页是 2MB！！！** 的, 后面指定**1GB 大小**, 分配**16 个大页**也是成功的.

```
[root@kvm-host ~]# cat /proc/cmdline
BOOT_IMAGE=/vmlinuz-3.10.0-514.el7.x86_64 root=/dev/mapper/rhel-root ro crashkernel= auto rd.lvm.lv=rhel/root rd.lvm.lv=rhel/swap rhgb quiet LANG=en_US.UTF-8 intel_iommu=on hugepages=64 hugepagesz=1G hugepages=16

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
64

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages
16
```

## 1.3. 系统大页信息查看

内核启动之后, **系统大页信息**主要从以下面几个**处查看与配置**.

### 1.3.1. /proc/meminfo

(1)/**proc/meminfo**里面的一些信息

/proc/meminfo 里面有这些信息: **HugePages_Total**, **HugePages_Free**, **HugePages_Rsvd**, **HugePages_Surp**, **Hugepagesize**.

但要注意, 它只体现"**default_hugepagesz**"的 huge page 信息.

如按笔者上面的**启动项启动**后, 查看/proc/meminfo, 发现**1GB 的 Huge Page 信息**并**没有体现**在这里, 如下所示.

```
[root@kvm-host ~]# cat /proc/meminfo
HugePages_Total:      64
HugePages_Free:       64
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
```

### 1.3.2. /proc/sys/vm/ 下

(2)/**proc/sys/vm**/下面的一些选项

在/proc/sys/vm 目录下我们可以看到一些与大页相关的**节点文件**, 通过读取它们的值(cat 命令), 就可以获得当前系统中与大页相关的实时信息.

```
[root@kvm-host ~]# ls -l /proc/sys/vm/*huge*
-rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/hugepages_treat_as_movable
-rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/hugetlb_shm_group
-rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/nr_hugepages
-rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/nr_hugepages_mempolicy
-rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/nr_overcommit_hugepages
```

这些主要参数介绍如下:

- **hugepages_treat_as_movable**: 这个参数设置为**非 0**时, 表示允许 hugepage 从**ZONE_MOVABLE**(前面内存热插拔里提到过)里面分配. 但这**有利有弊**: 虽然这**扩大了 hugepage 的来源**, 但它也增加了**内存热拔出**时的**失败几率**, 如果当时这个**hugepage 正在被用**, 哪怕只用到了一小块, 它也不能当时被换出, 这样整个内存条也就不能拔出. 更极端的情况: 系统中没有其他的大页可以供这个大页的内容迁移, 则内存热拔出一直会失败. 所以, 这个参数**默认是 0**.

- **hugetlb_shm_group**: **SysV 共享内存段**(shared memory segment)的 id, 该内存段将使用大页. 我们常用的 POSIX 共享内存的方法**用不到这个**.

- **nr_hugepages_mempolicy**: 通常与**NUMA mempolicy(内存策略**)相关. 后面详细讲到.

- **nr_hugepages**: 就是**大页的数目**.

- **nr_overcommit_hugepages**: 在 nr_hugepages 数目以外, 在 nr_hugepages 数目不够的时候, 还可以**动态补充多少大页**.

读者可以这样去理解:

- **大页资源**放在一个**池(huge page pool**)中, **nr_hugepages**指定的就是**恒定的大页数目(persistent hugepage**), 它们是**不会被拆分成小页**的, 即使系统需要小页的时候.

- 而在**persistent hugepage**不够的时候, 系统可以**向大页池中补充大页**, 这些补充的大页由系统中**空闲且物理连续的小页**拼凑而来, 最多允许拼凑**nr_overcommit_hugepages 个大页**. 当这些额外拼凑来的大页后来被释放出来的时候, 它们又会**被解散成小页**, 释放回**小页池**.

### 1.3.3. /sys/kernel/mm/hugepages/ 目录

(3)/sys/kernel/mm/hugepages/目录

其实, 现在的**kernel**已经**逐渐废弃**通过/**proc 文件系统去配置大页**, 而转向/**sys 文件系统**.

在/sys/kernel/mm/hugepages 目录下, 我们可以看到上面两种方式重复的地方, 同时又有比它们更详尽的地方.

首先, /sys/kernel/mm/hugepages 目录下按不同大页大小分子目录. 如笔者环境中, 配置了 2MB 和 1GB 的大页.

```
[root@gerrylee ~]# ll /sys/kernel/mm/hugepages/
总用量 0
drwxr-xr-x 2 root root 0 5 月  21 10:13 hugepages-1048576kB
drwxr-xr-x 2 root root 0 5 月  21 10:13 hugepages-2048kB
```

每个目录下面会有这些文件 free_hugepages、nr_hugepages、nr_hugepages_mempolicy、nr_overcommit_hugepages、resv_hugepages、surplus_hugepages,

其中 nr_hugepages、nr_hugepages_mempolicy、nr_overcommit_hugepages 在前面已经介绍过了.

- **free_hugepages**, **大页池**中有多少**空闲的大页**.

- **resv_hugepages**, **被保留的暂时未分配**出去的大页.

- **surplus_hugepages**, 具体的**额外分配来的大页**(nr_overcommit_hugepages 指示的是这个数目的上限).

上述 6 个/sys FS 的控制文件中, nr_hugepages、nr_hugepages_mempolicy、nr_over\-commit_hugepages 是**可读可写**的, 也就是说, 可以通过它们在系统起来以后**动态！！！地调整大页池的大小**.

前面提到的**内核启动参数 hugepages**等, 是指示系统起来时候**预分配大页池**. 起来以后, 因为存在内存碎片, 分配大页**成功机会没有系统启动时候大**.

我们以 2MB 大页为例, 当前池中是有 64 个大页, 我们可以把它动态地调整为 128. 具体的设置如下所示:

```
[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages
64

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
64

[root@kvm-host ~]# echo 128 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages
128

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
128
```

# 2. KVM 虚拟化对大页的利用

## 2.1. 一般应用程序使用大页的方式

**操作系统**之上的**应用程序**(包括**QEMU 创建的客户机**)要利用上**大页**, 无非下面**3 种途径**之一.

- **mmap**系统调用使用**MAP_HUGETLB flag**创建一段内存映射.
- **shmget**系统调用使用**SHM_HUGETLB flag**创建一段共享内存区.
- 使用**hugetlbfs**创建一个**文件挂载点**, 这个挂载目录之内的**文件**就都是**使用大页**的了.

## 2.2. KVM 对大页的使用

在**KVM 虚拟化环境**中, 就是使用**第 3 种方法**: 创建**hugetlbfs 的挂载点**, 通过"\-**mem\-path FILE**"选项, 让客户机使用宿主机的 huge page 挂载点作为它**内存的 backend**.

另外, 还有一个参数"\-**mem\-prealloc**"是让宿主机在启动客户机时就**全部分配好客户机的内存**, 而不是在客户机实际用到更多内存时才按需分配. \-**mem-prealloc**必须在有"\-**mem\-path**"参数时**才能使用**. 提前分配好内存的好处是客户机的内存访问速度更快, 缺点是客户机启动时就得到了所有的内存, 从而使得宿主机的内存很快减少(而不是根据客户机的需求而动态地调整内存分配).

## 2.3. KVM 使用大页操作

下面我们就以 1GB 大页为例(2MB 的类似, 读者自行实验), 说明怎样让 KVM 客户机用上大页. 因为 RHEL 7.3 自带的 3.10 内核 mount \-t hugetlbfs \-o min_size 参数的要求, 为了向读者展示它对 reserve huge page 的作用, 本节使用较新的 4.9.6 内核.

### 2.3.1. 检查硬件是否支持大页

1)检查**硬件是否支持大页**. 检查方法如下:

```
[root@kvm-host ~]# cat /proc/cpuinfo | grep flags | uniq | grep pdpe1gb | grep pae | grep pse
```

目前, 几乎所有的 Intel 处理器都有上述对大页的硬件支持. 如果没有硬件的支持, 内核将无法使用大页.

### 2.3.2. 检查系统是否支持 hugetlbfs

2)检查**系统是否支持 hugetlbfs**(7.1.1 节提到的 kernel Config 选项要打开).

```
[root@gerrylee linux]# cat /proc/filesystems | grep hugetlbfs
nodev	hugetlbfs
```

如果没有 hugetlbfs 的支持, 虚拟机也是无法使用大页的. 这时需要参照 7.1.1 节打开**内核选项**并**重新编译宿主机内核**并**重新启动**.

### 2.3.3. 检查大页池有足够大页

3)检查并确保**大页池(huge page pool)中有足够大页**, 本例创建**16 个 1GB 大页的 pool**.

系统起来时候已经通过 Kernel boot param(**hugepages=, hugepagesz**=)指定了, 那么此时应该已经有了.

本例因为**启动时候没有！！！指定大页**, 所以**此时才创建大页池**.

```
[root@kvm-host ~]   # cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages
0

[root@kvm-host ~]   # echo 16 > /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages

[root@kvm-host ~]   # cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages
16

[root@kvm-host ~]   # cat /sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages
16
```

### 2.3.4. 创建 hugetlbfs 的挂载点

4)创建**hugetlbfs 的挂载点**. 我们指定 min_size=4G, 可以看到**大页池**里就给它**保留(reserve)了 4 个 1GB 大页**, 虽然还没有正式分配出去.

```
[root@kvm-host ~]# mount -t hugetlbfs -o pagesize=1G,size=8G,min_size=4G nodev /mnt/1G-hugepage/

[root@kvm-host ~]# mount | grep /mnt/1G-hugepage
nodev on /mnt/1G-hugepage type hugetlbfs (rw,relatime,pagesize=1G,size=8G,min_size=4G)

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages
16

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages
16

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/resv_hugepages
4
```

### 2.3.5. 创建客户机

5)**启动客户机**让其**使用 hugepage 的内存**, 使用"\-mem\-path""\-mem\-prealloc"参数.

可以看到, 当客户机创建以后, 大页池的空闲页只剩 8 个, 其中包括之前创建 hugetlbfs 时候预留的 4 个大页, 此时一共被分配出去 8 个 1GB 大页给客户机.

```
[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -smp 4 -m 8G -mem-path /mnt/1G-hugepage -mem-prealloc -drive file=./rhel7.img,if=virtio,media=disk -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/resv_hugepages
0

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages
8

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages
16
```

至此, 如果在客户机中运行的应用程序具备使用 Huge Page 的能力, 那么就可以在客户机中使用 Huge Page, 从而带来性能的提升.

## 2.4. 2MB 大页的相关命令

我们还可以同时创建**2MB 大页池**, 创建客户机, 并故意给客户机分配内存大于池内 persistent huge page 数量而出现 over commit 的现象.

相关命令行总结如下:

```
[root@kvm-host ~]# echo 2048 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
2048

[root@kvm-host ~]# echo 2048 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages
2048

[root@kvm-host ~]# mkdir /mnt/2M-hugepage

[root@kvm-host ~]# mount -t hugetlbfs -o pagesize=2M,size=8G nodev /mnt/2M-hugepage/

[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -smp 4 -m 8G -mem-path /mnt/2M-hugepage/ -mem-prealloc -drive file=./rhel7.img,if=virtio,media=disk -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot
```

这个客户机创建以后, 通过下面的命令行可以看到, 2MB 大页池中的 free hugepage 为 0 了, 并且 surplus hugepage 为 2048. 因为客户机需要 8G 内存, 而大页池中本来只有 2048 个 2MB 大页, 同时我们事先设置了 nr_overcommit_hugepages 为 2048, 即允许它即时拼凑最多 2048 个大页.

```
[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
4096

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages
0

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages
2048

[root@kvm-host ~]# cat /sys/kernel/mm/hugepages/hugepages-2048kB/surplus_hugepages
2048
```

## 2.5. 大页的优缺点

总的来说, 对于内存访问密集型的应用, 在 KVM 客户机中使用 Huge Page 是可以较明显地提高客户机性能的.

不过, 它也有一个缺点, 使用**Huge Page**的**内存不能被换出(swap out**), 也**不能使用 ballooning 方式自动增长**.
