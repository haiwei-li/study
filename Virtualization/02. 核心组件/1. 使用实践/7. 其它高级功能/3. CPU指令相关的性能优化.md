
本节介绍一些与性能优化相关的新指令, 如 AVX 和 XSAVE 指令的支持、AES 新指令的支持. 最后介绍完全暴露宿主机 CPU 的特性给客户机.
9.3.1　AVX/AVX2/AVX512
AVX(Advanced Vector Extensions, 高级矢量扩展)是 Intel 和 AMD 的 x86 架构指令集的一个扩展, 它最早是 Intel 在 2008 年 3 月提出的指令集, 在 2011 年 Intel 发布 Sandy Bridge 处理器时开始第一次正式支持 AVX, 随后 AMD 最新的处理器也支持 AVX 指令集. Sandy Bridge 是 Intel 处理器微架构从 Nehalem(2009 年)后的革新, 而引入 AVX 指令集是 Sandy Bridge 一个较大的新特性, 甚至有人将 AVX 指令认为 Sandy Bridge 中最大的亮点, 其重要性堪比 1999 年 PentiumⅢ处理中引入的 SSE(流式 SIMD 扩展)指令集. AVX 中的新特性有: 将向量化宽度从 128 位提升到 256 位, 且将 XMM0~XMM15 寄存器重命名为 YMM0~YMM15; 引入了三操作数、四操作数的 SIMD 指令格式; 弱化了 SIMD 指令中对内存操作对齐的要求, 支持灵活的不对齐内存地址访问.
向量就是多个标量的组合, 通常意味着 SIMD(单指令多数据), 就是一个指令同时对多个数据进行处理, 达到很大的吞吐量. 早期的超级计算机大多都是向量机, 而随着图形图像、视频、音频等多媒体的流行, PC 处理器也开始向量化. x86 上最早出现的是 1996 年的 MMX(多媒体扩展)指令集, 之后是 1999 年的 SSE 指令集, 它们分别是 64 位向量和 128 位向量, 比超级计算机用的要短得多, 所以叫作"短向量". Sandy Bridge 的 AVX 指令集将向量化宽度扩展到了 256 位, 原有的 16 个 128 位 XMM 寄存器扩充为 256 位的 YMM 寄存器, 可以同时处理 8 个单精度浮点数和 4 个双精度浮点数. AVX 的 256 位向量仅支持浮点, 到了 AVX2(Haswell 平台开始引入)大多数整数运算指令已经支持 256 位向量. 而最新的 AVX512(Skylake 平台开始引入)已经扩展到 512 位向量.
最新的 AVX512 已经不是一个单一的指令, 而是一个指令家族(family). 它目前包含(以后还会继续扩充)AVX-512F, AVX-512CD, AVX-512ER, AVX-512PF, AVX-512BW, AVX-512DQ, AVX-512VL. 并且, 这些指令集在不同的硬件平台上只部分出现, 且编译选项不同, 会分别编译出不同指令集的二进制文件.
CPU 硬件方面, Intel 的 Xeon Phi x200(Knights Landing, 2016 年发布)、Xeon X5-E26xx v5(2017 年下半年发布), 都支持最新的 AVX512. 在编译器方面, GCC 4.9 版本(binutil 2.4.0)、ICC(Intel C/C++Compiler)15.2 版本都已经支持 AVX512. 截至笔者写作时, MS Visual Studio 还没有支持.
在操作系统方面, Linux 内核 3.19、RHEL7.3 已经支持 AVX512. 不过在应用程序方面, 目前只有较少的软件提供了对 AVX512 的支持, 因为新的指令集的应用需要一定的时间, 就像当年 SSE 指令集出来后, 也是过了几年后才被多数软件支持. 但对 AVX2 的支持已经比较普遍, 比如 QEMU 自身, 在笔者的 Xeon E5-2699 v4 平台上编译的, 就已经支持了 AVX2 优化.

[root@kvm-host qemu]# cat config-host.mak | grep -i avxCONFIG_AVX2_OPT=y

从网上的这篇文章 http://www.sisoftware.eu/2016/02/24/future-performance-with-avx512-in-sandra-2016-sp1/, 我们可以看到, 从 SSE -> AVX2 -> AVX512, 每一代都有至少 60+%乃至最多 2.35 倍的多媒体处理性能提升.
9.3.2　XSAVE 指令集
另外, XSAVE 指令集(包括 XGETBV、XSETBV、XSAVE、XSAVEC、XSAVEOPT、XSAVES、XRSTOR、XSAVES 等)是在 Intel Nehalem 以后的处理器中陆续引入、用于保存和恢复处理器扩展状态的, 这些扩展状态包括但不限于上节提到的 AVX 特性相关的寄存器. 在 KVM 虚拟化环境中, 客户机的动态迁移需要保存处理器状态, 然后在迁移后恢复处理器的执行状态, 如果有 AVX 指令要执行, 在保存和恢复时也需要 XSAVE(S)/XRSTOR(S)指令的支持.
下面介绍一下如何在 KVM 中为客户机提供 AVX、XSAVE 特性.
1)检查宿主机中 AVX、XSAVE 指令系列的支持, Intel Broadwell 硬件平台支持到 AVX2、XSAVE、XSAVEOPT.

[root@kvm-host ~]# cat /proc/cpuinfo | grep -E "xsave|avx" | uniqflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb pln pts dtherm intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local

2)启动客户机, 将 AVX、XSAVE 特性提供给客户机使用(-cpu host, 完全暴露宿主机 CPU 特性, 详见 9.3.4 节). 命令行操作如下:

[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot

3)在客户机中, 查看 QEMU 提供的 CPU 信息中是否支持 AVX 和 XSAVE. 命令行如下:

[root@kvm-guest ~]# cat /proc/cpuinfo | grep -E "xsave|avx" | uniqflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat

由上面输出可知, 客户机已经检测到 CPU 有 AVX 和 XSAVE 的指令集支持了, 如果客户机中有需要使用到它们的程序, 就可正常使用, 从而提高程序执行的性能.
由于 XSAVES、AVX512 都是 Skylake 以后的 CPU 才支持的, 所以笔者的环境中没有看到它们.
9.3.3　AES 新指令
AES(Advanced Encryption Standard, 高级加密标准)是一种用于对电子数据进行加密的标准, 它在 2001 年就被美国政府正式接纳和采用. 软件工业界广泛采用 AES 对个人数据、网络传输数据、公司内部 IT 基础架构等进行加密保护. AES 的区块长度固定为 128 位, 密钥长度则可以是 128 位、192 位或 256 位. 随着大家对数据加密越来越重视, 以及 AES 应用越来越广泛, 并且 AES 算法用硬件实现的成本并不高, 一些硬件厂商(如 Intel、AMD 等)都在自己的 CPU 中直接实现了针对 AES 算法的一系列指令, 从而提高 AES 加解密的性能.
AESNI(Advanced Encryption Standard new instructions, AES 新指令)是 Intel 在 2008 年 3 月提出的在 x86 处理器上的指令集扩展. 它包含了 7 条新指令, 其中 6 条指令是在硬件上对 AES 的直接支持, 另外一条是对进位乘法的优化, 从而在执行 AES 算法的某些复杂的、计算密集型子步骤时, 使程序能更好地利用底层硬件, 减少计算所需的 CPU 周期, 提升 AES 加解密的性能. Intel 公司从 Westmere 平台开始就支持 AESNI.
目前有不少的软件都已经支持 AESNI 了, 如 OpenSSL(1.0.l 版本以上)、Oracle 数据库(11.2.0.2 版本以上)、7-Zip(9.1 版本以上)、Libgcrypt(1.5.0 以上)、Linux 的 Crypto API 等. 在 KVM 虚拟化环境中, 如果客户机支持 AESNI(如 RHEL6、7 的内核都支持 AESNI), 而且在客户机中用到 AES 算法加解密, 那么将 AESNI 的特性提供给客户机使用, 会提高客户机的性能.
笔者在 KVM 的客户机中对 AESNI 进行了测试, 对比在使用 AESNI 和不使用 AESNI 的情况下对磁盘进行加解密的速度. 下面介绍一下 AESNI 的配置和测试过程及测试结果.
1)在进行测试之前, 检查硬件平台是否支持 AESNI. 一般来说, 如果 CPU 支持 AESNI, 则会默认暴露到操作系统中去. 而一些 BIOS 中的 CPU configuration 下有一个"AESNI Intel"这样的选项, 也需要查看并且确认打开 AESNI 的支持. 不过, 在设置 BIOS 时需要注意, 笔者曾经在一台 Romley-EP 的 BIOS 中设置了"Advanced" -> "Processor Configuration" -> "AES-NI Defeature"的选项, 需要看清楚这里是设置"Defeature"而不是"Feature", 所以这个"AES-NI Defeature"应该设置为"disabled"(其默认值也是"disabled"), 表示打开 AESNI 功能. 而 BIOS 中没有 AESNI 相关的任何设置之时, 就需要到操作系统中加载"aesni_intel"等模块, 来确认硬件是否提供了 AESNI 的支持.
2)需要保证在内核中将 AESNI 相关的配置项目编译为模块或直接编译进内核. 当然, 如果不是通过内核来使用 AESNI 而是直接应用程序的指令使用它, 则该步对内核模块的检查来说不是必要的. RHEL 7.3 的内核关于 AES 的配置如下:

CONFIG_CRYPTO_AES=yCONFIG_CRYPTO_AES_X86_64=yCONFIG_CRYPTO_AES_NI_INTEL=mCONFIG_CRYPTO_CAMELLIA_AESNI_AVX_X86_64=mCONFIG_CRYPTO_CAMELLIA_AESNI_AVX2_X86_64=mCONFIG_CRYPTO_DEV_PADLOCK_AES=m

3)在宿主机中, 查看/proc/cpuinfo 中的 AESNI 相关的特性, 并查看"aseni_intel"这个模块加载情况(如果没有, 需加载). 命令行操作如下:

[root@kvm-host ~]# cat /proc/cpuinfo | grep aes | uniqflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb pln pts dtherm intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local[root@kvm-host ~]# lsmod | grep aesaesni_intel            69884  0 lrw                    13286  1 aesni_intelglue_helper            13990  1 aesni_intelablk_helper            13597  1 aesni_intelcryptd                 20359  3 ghash_clmulni_intel,aesni_intel,ablk_helper

4)启动 KVM 客户机, 默认 QEMU 启动客户机时, 没有向客户机提供 AESNI 的特性, 可以用"-cpu host"或"-cpu qemu64, +aes"选项来暴露 AESNI 特性给客户机使用. 当然, 由于前面提及一些最新的 CPU 系列是支持 AESNI 的, 所以也可用"-cpu Westmere""-cpu SandyBridge"这样的参数提供相应的 CPU 模型, 从而提供对 AESNI 特性的支持. 注意, 笔者在启动客户机时, 使用了 7.4 节讲到的 numactl 技巧, 将客户机绑定在 node1 上执行, 进一步提高性能. 另外, 笔者给客户机配备了 16 个 vCPU 和 48G 内存, 因为后面测试中要用到 20G 大小的 ramdisk.

[root@kvm-host ~]# numactl --membind=1 --cpunodebind=1 -- qemu-system-x86_64 -enable-kvm -smp 16 -m 48G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0

5)在客户机中可以看到 aes 标志在/proc/cpuinfo 中也是存在的, 然后像宿主机中那样确保"aesni_intel"模块被加载, 再执行使用 AESNI 测试程序, 即可得出使用了 AESNI 的测试结果. 当然, 为了衡量 AESNI 带来的性能提升, 还需要做对比测试, 即不添加"-cpu host"等参数启动客户机, 从而没有 AESNI 特性的测试结果. 注意, 如果在 QEMU 启动命令行启动客户机时带了 AESNI 参数, 一些程序使用 AES 算法时可能会自动加载"aesni_intel"模块, 所以, 为了确保没有 AESNI 的支持, 也可以用"rmod aesni_intel"命令移除该模块, 再找到 aesni-intel.ko 文件并将其删除, 以防被自动加载.
笔者用下面的脚本在 Xeon E5-2699 v4(Broadwell)平台上测试了有无 AESNI 支持对客户机加解密运算性能的影响. 宿主机和客户机内核都是 RHEL7.3 自带的.
测试原理是: 用 cryptsetup 命令对 ramdisk 进行一层加密封装, 然后对封装后的有加密层的磁盘设备进行读写以比较性能. 加密封装我们采用 aes-ecb-plain、aes-cbc-plain、aes-ctr-plain、aes-lrw-plain、aes-pcbc-plain、aes-xts-plain 这 6 种 AES 算法分别测试. 由于 cryptsetup 会调用 Linux 内核的 Crypto API(前面已提到它是支持 AESNI 的), 所以测试中会用上 AESNI 的指令. 测试脚本的内容如下, 笔者是 RHEL7 的环境, 系统默认 mount 了 tmpfs(一种 ramfs)在/run 目录. 读者可能需要根据自己的 Linux 发行版的情况, 自行 mount 或者修改路径.

#!/bin/bashCRYPT_DEVICE_NAME='crypt0'SYS_RAMDISK_PATH='/run'CIPHER_SET='aes-ecb-plain aes-cbc-plain aes-ctr-plain aes-lrw-plain aes-pcbc-plain aes-xts-plain'# create a crypt device using cryptsetup# $1 -- create crypt device name# $2 -- backend real ramfs device# $3 -- encryption algorithmcreate_dm(){    echo 123456 | cryptsetup open --type=plain $2 $1 -c $3return $?}# remove the device-mapper using cryptsetupremove_dm(){    if [ -b /dev/mapper/$CRYPT_DEVICE_NAME ]; then        cryptsetup close $CRYPT_DEVICE_NAME &> /dev/null    fi}remove_dm# create raw backend ramdiskecho -n "Create raw ramdisk with 20GB size, and its raw writing speed is: "dd if=/dev/zero of=$SYS_RAMDISK_PATH/ramdisk bs=1G count=20if [ $? -ne 0 ]; then    echo "Create raw ramdisk fail, exit"exit 1fiecho "Now start testing..."for cipher in $CIPHER_SETdocreate_dm crypt0 $SYS_RAMDISK_PATH/ramdisk $cipherif [ ! -b /dev/mapper/$CRYPT_DEVICE_NAME ]; then    echo "creating dm with $cipher encryption failed, exit"    rm -fr $SYS_RAMDISK_PATH/ramdisk    exit 1firm -fr $cipher*.txt# do read and write action for several times, so that you can calculate their average value.for i in $(seq 10); do    # the following line is for read action        echo "Start reading test with $cipher, round $i"        dd if=/dev/mapper/$CRYPT_DEVICE_NAME of=/dev/null bs=1G count=20 >> $cipher-read.txt 2>&1    # the following line is for write action        echo "Start write test with $cipher, round $i"        dd of=/dev/mapper/$CRYPT_DEVICE_NAME if=/dev/zero bs=1G count=20 >> "$cipher"-write.txt 2>&1doneremove_dmdonerm -fr $SYS_RAMDISK_PATH/ramdisk

该脚本执行过程中, 会分别以 aes-ecb-plain、aes-cbc-plain、aes-ctr-plain、aes-lrw-plain、aes-pcbc-plain、aes-xts-plain 加密算法加密 ramdisk(路径为/run/ramdisk), 虚拟的加密层硬盘为/dev/mapper/crypt0. 然后对这个虚拟磁盘分别进行 10 次读写操作, 读写的速度保存在诸如 aes-cbc-plain-read.txt、aes-cbc-plain-write.txt 等文件中. 最后, 笔者对这 10 次速度求平均值, 得到图 9-5、图 9-6. 可以看到, 读速率方面, 有 AESNI 比没有 AESNI 快 40%~91%, 写速率要快 16%~50%.
另外, 在执行脚本中"cryptsetup open"命令时, 有可能会出现如下的错误:

[root@kvm-guest ~]# echo 123456 | cryptsetup create crypt0 /dev/ram0 -c aes-ctr-plaindevice-mapper: reload ioctl on  failed: No such file or directory


图 9-5　AESNI 在各 AES 加密算法下读性能优势


图 9-6　AESNI 在各 AES 加密算法下写性能优势
这有可能是内核配置的问题, 缺少一些加密相关的模块. 在本示例中, 需要确保内核配置中有如下的配置项:

CONFIG_CRYPTO_CBC=mCONFIG_CRYPTO_CTR=mCONFIG_CRYPTO_CTS=mCONFIG_CRYPTO_ECB=mCONFIG_CRYPTO_LRW=mCONFIG_CRYPTO_PCBC=mCONFIG_CRYPTO_XTS=mCONFIG_CRYPTO_FPU=m

除了笔者这个实验以外, 读者还可以在网上搜索到很多关于 AESNI 性能优势的文章, 都显示了有了 AESNI 的硬件辅助, AES 加解密获得了显著的性能提升. 比如 Intel AES-NI Performance Testing on Linux*/Java*Stack, 显示了 AES128 和 AES256 加解密在有 AESNI 情况下的明显性能优势, 如图 9-7、图 9-8 所示.

图 9-7　AES128/256 利用 AESNI 的加密性能提升

图 9-8　AES128/256 利用 AESNI 的解密性能提升
本节首先对 AESNI 新特性进行介绍, 然后对如何让 KVM 客户机也使用 AESNI 的步骤进行了介绍. 最后展示了笔者曾经某次对于 AESNI 在 KVM 客户机中的实验数据. 相信读者对 AESNI 有了较好的认识, 在实际应用环境中如果物理硬件平台支持 ASENI, 且客户机中用到 AES 相关的加解密算法, 可以考虑将 AESNI 特性暴露给客户机使用, 以便提高加解密性能.
9.3.4　完全暴露宿主机 CPU 特性
在 5.2.4 节中介绍过, QEMU 提供 qemu64 作为默认的 CPU 模型. 对于部分 CPU 特性, 也可以通过"+"号来添加一个或多个 CPU 特性到一个基础的 CPU 模型之上, 如前面介绍的 AESNI, 可以选用"-cpu qemu64, +aes"参数来让客户机支持 AESNI.
当需要客户机尽可能地使用宿主机和物理 CPU 支持的特性时, QEMU 也提供了"-cpu host"参数来尽可能多地暴露宿主机 CPU 特性给客户机, 从而在客户机中可以看到和使用 CPU 的各种特性(如果 QEMU/KVM 同时支持该特性). 在笔者的 E5-2699 v4(Broadwell)平台上, 对"-cpu host"参数的效果演示如下.
1)在 KVM 宿主机中查看 CPU 信息. 命令行如下:

[root@kvm-host ~]# cat /proc/cpuinfo <!-- 省略其余逻辑 CPU 信息的输出 -->processor   : 87vendor_id   : GenuineIntelcpu family   : 6model      : 79model name   : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHzstepping   : 1microcode   : 0xb00001dcpu MHz      : 2029.414cache size   : 56320 KBphysical id   : 1siblings   : 44core id      : 28cpu cores   : 22apicid      : 121initial apicid   : 121fpu      : yesfpu_exception   : yescpuid level   : 20wp      : yesflags      : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb pln pts dtherm intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_localbogomips   : 4403.45clflush size   : 64cache_alignment   : 64address sizes   : 46 bits physical, 48 bits virtualpower management:

2)用"-cpu host"参数启动一个 RHEL 7.3 客户机系统. 命令行如下:

[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot

3)在客户机中查看 CPU 信息. 命令行如下:

[root@kvm-guest ~]# cat /proc/cpuinfo processor   : 0vendor_id   : GenuineIntelcpu family   : 6model      : 79model name   : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHzstepping   : 1microcode   : 0x1cpu MHz      : 2194.916cache size   : 4096 KBphysical id   : 0siblings   : 1core id      : 0cpu cores   : 1apicid      : 0initial apicid   : 0fpu      : yesfpu_exception   : yescpuid level   : 13wp      : yesflags      : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch arat fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveoptbogomips   : 4389.83clflush size   : 64cache_alignment   : 64address sizes   : 40 bits physical, 48 bits virtualpower management:

由上面客户机中 CPU 信息中可知, 客户机看到的 CPU 模型与宿主机中一致, 都是"E5-2699 v4@2.20GHz"; CPUID 等级信息(cpuid level)也与宿主机一致, 在 CPU 特性标识(flags)中, 也有了"aes""xsave""xsaveopt""avx""avx2""rdrand""smep""smap"等特性. 这些较高级的特性是默认的 qemu64 CPU 模型中没有的, 说明"-cpu host"参数成功地将宿主机的特性尽可能多地提供给客户机使用了.
当然, "-cpu host"参数也并没有完全让客户机得到与宿主机同样多的 CPU 特性, 这是因为 QEMU/KVM 对于其中的部分特性没有模拟和实现, 如"EPT""VPID"等 CPU 特性目前就不能暴露给客户机使用. 另外, 尽管"-cpu host"参数尽可能多地暴露宿主机 CPU 特性给客户机, 可以让客户机用上更多的 CPU 功能, 也能提高客户机的部分性能, 但同时, "-cpu host"参数也给客户机的动态迁移增加了障碍. 例如, 在 Intel 的 SandyBridge 平台上, 用"-cpu host"参数让客户机使用了 AVX 新指令进行运算, 此时试图将客户机迁移到没有 AVX 支持的 Intel Westmere 平台上去, 就会导致动态迁移的失败. 所以, "-cpu host"参数尽管向客户机提供了更多的功能和更好的性能, 还是需要根据使用场景谨慎使用.
