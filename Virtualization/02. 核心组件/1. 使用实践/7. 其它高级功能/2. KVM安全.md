
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->



<!-- /code_chunk_output -->

如今, 计算机信息安全越来越受到人们的重视, 在计算机相关的各种学术会议、论文、期刊中, "安全"(security)一词被经常提及. 因为 KVM 是 Linux 系统中的一个虚拟化相关的模块, QEMU 是 Linux 系统上一个普通的用户空间进程, 所以 Linux 系统上的各种安全技术、策略对 QEMU/KVM 都是适用的. 在本节中, 笔者根据经验选择了其中的一些 KVM 的安全技术来介绍一下.

9.2.1　SMEP/SMAP/MPX

有一些安全渗透(exploitation)攻击, 会诱导系统在最高执行级别(ring0)上访问在用户空间(ring3)中的数据和执行用户空间的某段恶意代码, 从而获得系统的控制权或使系统崩溃.

SMEP(Supervision Mode Execution Protection, 监督模式执行保护)是 Intel 在 2012 年发布的代号为"Ivy Bridge"的新一代 CPU 上提供的一个安全特性. 当控制寄存器 CR4 寄存器的 20 位(第 21 位)被设置为 1 时, 表示 SMEP 特性是打开状态. SMEP 特性让处于管理模式(supervisor mode, 当前特权级别 CPL＜3)的程序不能获取用户模式(user mode, CPL=3)可以访问的地址空间上的指令, 如果管理模式的程序试图获取并执行用户模式的内存上的指令, 则会发生一个错误(fault), 不能正常执行. 在 SMEP 特性的支持下, 运行在管理模式的 CPU 不能执行用户模式的内存页, 这就较好地阻止前面提到的那种渗透攻击. 而且由于 SMEP 是 CPU 的一个硬件特性, 这种安全保护非常方便和高效, 其带来的性能开销几乎可以忽略不计.

SMAP(Supervisor Mode Access Prevention)是对 SMEP 的更进一步的补充, 它将保护进一步扩展为 Supervisor Mode(典型的如 Linux 内核态)代码不能直接访问(读或写)用户态的内存页(SMEP 是禁止执行权限). 类似的, 当它被触犯时, 也会产生一个访页异常(Page Fault). SMAP 于 Intel Broadwell 平台中开始引入.

MPX(Memory Protection Extensions)是对软件(包括内核态的代码和用户态的代码都支持)指针越界的保护. 它引入了新的寄存器记录软件运行时一个指针的合法范围, 从而保证指针不会越界(无论有意或者无意). 与 SMEP 和 SMAP 不同的是, 它还引入了新指令, 所以要它起作用, 不仅需要硬件的支持, 也需要编译器的支持, 以及软件运行时用到的库里面代码的支持. MPX 从 Intel Skylake 平台以后引入.

基本上, 内核(包括 KVM)自 3.16 版本以后对 SMEP/SMAP/MPX 的支持都已经完备了. 一些 Linux 发行版(如 RHEL 7.2/7.3)尽管使用的是以 Linux 3.10 为基础的内核, 但是也都向后移植(backport)了这些的特性. 下面我们以 SMEP 为例, 介绍一下如何让 KVM 客户机也可以使用这个安全特性的步骤. 对于 SMAP 和 MPX 读者可以自行实践.

1) 检查宿主机中的 CPU 对 SMEP 特性的支持. 命令行如下所示:

```
[root@kvm-host ~]# cat /proc/cpuinfo | grep "flag" | uniq | grep smepflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb pln pts dtherm intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local
```

2) 加上"-cpu host"参数来启动客户机. 命令行如下:

```
[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot -name SMEP_guest
```

3) 在客户机中检查其 CPU 是否有 SMEP 特性的支持. 命令行如下:

```
[root@kvm-guest ~]# cat /proc/cpuinfo | grep "flags" | uniq | grep smepflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology eagerfpu pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat
```

由上面的输出信息可知, 客户机中已经能够检测到 SMEP 特性了, 从而使客户机成为一个有 Intel CPU 的 SMEP 特性支持的、较为安全的环境.

9.2.2　控制客户机的资源使用——cgroups

在 KVM 虚拟化环境中, 每个客户机操作系统使用系统的一部分物理资源(包括处理器、内存、磁盘、网络带宽等). 当一个客户机对资源的消耗过大时(特别是 QEMU 启动客户机时没有能够控制磁盘或网络 I/O 的选项), 它可能会占用该系统的大部分资源, 此时, 其他的客户机对相同资源的请求就会受到严重影响, 可能会导致其他客户机响应速度过慢甚至失去响应. 为了让所有客户机都能够按照预先的比例来占用物理资源, 我们需要对客户机能使用的物理资源进行控制. 第 5 章中介绍过, 通过 qemu 命令行启动客户机时, 可以用"-smp num"参数来控制客户机的 CPU 个数, 使用"-m size"参数来控制客户机的内存大小. 不过, 这些都是比较粗粒度的控制, 例如, 不能控制客户机仅使用 1 个 CPU 的 50%的资源, 而且对磁盘 I/O、网络 I/O 等并没有直接的参数来控制. 由于每个客户机就是宿主机 Linux 系统上的一个普通 QEMU 进程, 所以可以通过控制 QEMU 进程使用的资源来达到控制客户机的目的.

1. cgroups 简介

cgroups(即 control groups, 控制群组)是 Linux 内核中的一个特性, 用于限制、记录和隔离进程组(processgroups)对系统物理资源的使用. cgroups 最初是由 Google 的一些工程师(Paul Menage、Rohit Seth 等)在 2006 年以"进程容器"(process container)的名字实现的, 在 2007 年被重命名为"控制群组"(Control Groups), 然后被合并到 Linux 内核的 2.6.24 版本中. 在加入 Linux 内核的主干之后, cgroups 越来越成熟, 有很多新功能和控制器(controller)被加入进去, 其功能也越来越强大. cgroups 为不同的用户场景提供了一个统一的接口, 这些用户场景包括对单一进程的控制, 也包括 OpenVZ、LXC(Linux Containers)等操作系统级别的虚拟化技术. 一些较新的比较流行的 Linux 发行版, 如 RHEL、Fedora、SLES、Ubuntu 等, 都提供了对 cgroups 的支持.

cgroups 提供了如下一些功能:

1) 资源限制(Resource limiting), 让进程组被设置为使用资源数量不能超过某个界限. 如内存子系统可以为进程组设定一个内存使用的上限, 一旦进程组使用的内存达到限额, 如果再申请内存就会发生缺乏内存的错误(即: OOM, out of memory).

2) 优先级控制(Prioritization), 让不同的进程组有不同的优先级. 可以让一些进程组占用较大的 CPU 或磁盘 I/O 吞吐量的百分比, 另一些进程组占用较小的百分比.

3) 记录(Accounting), 衡量每个进程组(包括 KVM 客户机)实际占用的资源数量, 可以用于对客户机用户进行收费等目的. 如使用 cpuacct 子系统记录某个进程组使用的 CPU 时间.

4) 隔离(Isolation), 对不同的进程组使用不同的命名空间(namespace), 不同的进程组之间不能看到相互的进程、网络连接、文件访问等信息, 如使用 ns 子系统就可以使不同的进程组使用不同的命名空间.

5) 控制(Control), 控制进程组的暂停、添加检查点、重启等, 如使用 freezer 子系统可以将进程组挂起和恢复.

cgroups 中有如下几个重要的概念, 理解它们是了解 cgroups 的前提条件.

1) 任务(task): 在 cgroups 中, 一个任务就是 Linux 系统上的一个进程或线程. 可以将任务添加到一个或多个控制群组中.

2) 控制群组(control group): 一个控制群组就是按照某种标准划分的一组任务(进程). 在 cgroups 中, 资源控制都是以控制群组为基本单位来实现的. 一个任务可以被添加到某个控制群组, 也可以从一个控制群组转移到另一个控制群组. 一个控制群组中的进程可以使用以控制群组为单位分配的资源, 同时也受到以控制群组为单位而设定的资源限制.

3) 层级体系(hierarchy): 简称"层级", 控制群组被组织成有层级关系的一棵控制群组树. 控制群组树上的子节点控制群组是父节点控制群组的孩子, 可以继承父节点控制群组的一些特定属性. 每个层级需要被添加到一个或多个子系统中, 受到子系统的控制.

4) 子系统(subsytem): 一个子系统就是一个资源控制器(resource controller), 如 blkio 子系统就是控制对物理块设备的 I/O 访问的控制器. 子系统必须附加到一个层级上才能起作用, 一个子系统附加到某个层级以后, 该子系统会控制这个层级上的所有控制群组.

目前 cgroups 中主要有如下 10 个子系统可供使用.

·blkio: 这个子系统为块设备(如磁盘、固态硬盘、U 盘等)设定读写 I/O 的访问设置限制.

·cpu: 这个子系统通过使用进程调度器提供了对控制群组中的任务在 CPU 上执行的控制.

·cpuacct: 这个子系统为控制群组中的任务所实际使用的 CPU 资源自动生成报告.

·cpuset: 这个子系统为控制群组中的任务分配独立 CPU 核心(在多核系统)和内存节点.

·devices: 这个子系统可以控制一些设备允许或拒绝来自某个控制群组中的任务的访问.

·freezer: 这个子系统用于挂起或恢复控制群组中的任务.

·hugetlb: 这个子系统用于控制对大页(见 7.1 节)的使用.

·memory: 这个子系统为控制群组中任务能使用的内存设置限制, 并能自动生成那些任务所使用的内存资源的报告.

·net_cls: 这个子系统使用类别识别符(classid)标记网络数据包, 允许 Linux 流量控制程序(traffic controller)识别来自某个控制群组中任务的数据包.

·net_prio: 这个子系统使得其名下 cgroups 里面的任务, 可以就不同的接口设置从该接口出去的包的优先级.

·perf_event: 这个子系统主要用于对系统中进程运行的性能监控、采样和分析等.

·pids: 这个子系统用于控制 cgroups 中可以派生(通过 fork、clone)出来的子进程、子线程的数量(pid 的数量).

在 Redhat 系列的系统中, 如 RHEL 7.3 系统的 libcgroup 这个 RPM 包提供了 lssubsys 工具在命令包中查看当前系统支持哪些子系统. 命令行操作如下:

```
[root@kvm-host ~]# uname -aLinux kvm-host 3.10.0-514.el7.x86_64 #1 SMP Wed Oct 19 11:24:13 EDT 2016 x86_64 x86_64 x86_64 GNU/Linux[root@kvm-host ~]# lssubsys cpusetcpu,cpuacctmemorydevicesfreezernet_cls,net_prioblkioperf_eventhugetlbpids
```

cgroups 中层级体系的概念与 Linux 系统中的进程模型有相似之处. 在 Linux 系统中, 所有进程也是组成树状的形式(可以用"pstree"命令查看), 除 init 以外的所有进程都是一个公共父进程, 即 init 进程的子进程. init 进程是由 Linux 内核在启动时执行的, 它会启动其他进程(当然普通进程也可以启动自己的子进程). 除 init 进程外的其他进程从其父进程那里继承环境变量(如$PATH 变量)和其他一些属性(如打开的文件描述符). 与 Linux 进程模型类似, cgroups 的层级体系也是树状分层结构的, 子节点控制群组继承父节点控制群组的一些属性. 尽管有类似的概念和结构, 但它们之间也一些区别. 最主要的区别是, 在 Linux 系统中可以同时存在 cgroups 的一个或多个相互独立的层级, 而且此时 Linux 系统中只有一个进程树模型(因为它们有一个相同的父进程 init 进程). 多个独立的层级的存在也是有其必然性的, 因为每个层级都会给添加到一个或多个子系统下.

图 9-3 展示了 cgroups 模型的一个示例, 其中, cpu、memory、blkio、net_cls 是 4 个子系统, Cgroup Hierarchy A~Cgroup Hierarchy C 是 3 个相互独立的层级, 含有"cg"的就是控制群组(包括 cpu_mem_cg、blk_cg、cg1、cg4 等), qemu-kvm 是一个任务(其 PID 为 8201). cpu 和 memory 子系统同时附加到 Cgroup Hierarchy A 这个层级上, blkio、net_cls 子系统分别附加到 Cgroup Hierarchy B 和 Cgroup Hierarchy C 这两个层级上. qemu-kvm 任务被添加到这 3 个层级之中, 它分别在 cg1、cg3、cg4 这 3 个控制群组中.

cgroups 中的子系统、层级、控制群组、任务之间的关系, 至少有如下几条规则需要遵循.

1) 每个层级可以附加一个或多个子系统.

在图 9-3 中, Cgroup Hierarchy A 就有 cpu、memory 两个子系统, 而 Cgroup Hierarchy B 只有 blkio 一个子系统.

图 9-3　一个 cgroups 模型的示例

2) 只要其中的一个层级已经附加上一个子系统, 任何一个子系统都不能被添加到两个或多个层级上.
在图 9-3 中, memory 子系统就不能同时添加到层级 A 和层级 B 之上.

3) 一个任务不能同时是同一个层级中的两个或多个控制群组中的成员. 一个任务一旦成为同一个层级中的第 2 个控制群组中的成员, 它必须先从第 1 个控制群组中移除.

在图 9-3 中, qemu-kvm 任务就不能同时是层级 A 中的 cg1 和 cg2 这两个控制群组的共同成员.

4) 派生(fork)出来的一个任务会完全继承它父进程的 cgroups 群组关系. 当然, 也可以再次调整派生任务的群组关系, 使其与它的父进程不同.

如图 9-3 所示, 如果 PID 为 8201 的 qemu-kvm 进程派生出一个子进程, 则该子进程也默认是 cg1、cg3、cg4 这 3 个控制群组的成员.

5) 在每次初始化一个新的层级时, 该系统中所有的任务(进程或线程)都将添加该层级默认的控制群组中成为它的成员. 该群组被称为根控制群组(root cgroup), 在创建层级时自动创建, 之后在该层级中创建的所有其他群组都是根控制群组的后代.

2.cgroups 操作示例

在了解了 cgroups 的基本功能和原理后(关于 cgroups 更多描述可以参考有关文档), 本节将介绍如何实际操作 cgroups 来控制 KVM 客户机的资源使用. Linux 内核提供了一个统一的 cgroups 接口来访问多个子系统(如 cpu、memory、blkio 等). 在编译 Linux 内核时, 需要对 cgroups 相关的项目进行配置, 以下是内核中与 cgroups 相关的一些重要配置.

```
CONFIG_CGROUP_SCHED=yCONFIG_CGROUPS=y# CONFIG_CGROUP_DEBUG is not setCONFIG_CGROUP_NS=yCONFIG_CGROUP_FREEZER=yCONFIG_CGROUP_DEVICE=yCONFIG_CPUSETS=yCONFIG_PROC_PID_CPUSET=yCONFIG_CGROUP_CPUACCT=yCONFIG_RESOURCE_COUNTERS=yCONFIG_CGROUP_MEM_RES_CTLR=yCONFIG_CGROUP_MEM_RES_CTLR_SWAP=yCONFIG_BLK_CGROUP=y# CONFIG_DEBUG_BLK_CGROUP is not setCONFIG_CGROUP_PERF=yCONFIG_SCHED_AUTOGROUP=yCONFIG_MM_OWNER=y# CONFIG_SYSFS_DEPRECATED_V2 is not setCONFIG_RELAY=yCONFIG_NAMESPACES=yCONFIG_UTS_NS=yCONFIG_IPC_NS=yCONFIG_USER_NS=yCONFIG_PID_NS=yCONFIG_NET_NS=yCONFIG_NET_CLS_CGROUP=yCONFIG_NETPRIO_CGROUP=y
```

在实际操作过程中, 可以通过以下几个方式来使用 cgroups.

1) 手动地访问 cgroups 的虚拟文件系统.

2) 使用 libcgroup 软件包中的 cgcreate、cgexec、cgclassify 等工具来创建和管理 cgroups 控制群组.

3) 通过一些使用 cgroups 的规则引擎, 通常是系统的一个守护进程来读取 cgroups 相关的配置文件, 然后对任务进程相应的设置. 例如, 在 RHEL 6.3 系统中, 如果安装了 libcgroup RPM 包, 就会有 cgconfig 这个服务可以使用(用"service cgconfig start"命令启动它), 其配置文件默认为/etc/cgconfig.conf 文件.

4) 通过其他一些软件来间接使用 cgroups, 如使用操作系统虚拟化技术(LXC 等)、libvirt 工具等.

下面举个 KVM 虚拟化实际应用中的例子: 一个系统中运行着两个客户机(它们的磁盘镜像文件都在宿主机的本地存储上), 每个客户机中都运行着 MySQL 数据库服务, 其中一个数据库的优先级很高(需要尽可能快地响应), 另一个优先级不高(慢一点也无所谓). 我们知道, 数据库服务是磁盘 I/O 密集型服务, 所以这里通过 cgroups 的 blkio 子系统来设置两个客户机对磁盘 I/O 读写的优先级, 从而使优先级高的客户机能够占用更多的宿主机中的 I/O 资源. 采用手动读写 cgroups 虚拟文件系统的方式实现这个需求的操作步骤如下:

1) 启动这两个客户机和其中的 MySQL 服务器, 让其他应用开始使用 MySQL 服务. 这里不再写出启动过程. 假设, 优先级高的客户机在 qemu 命令行启动时加上了"-name high_prio"参数来指定其名称, 而优先级低的客户机用"-name low_prio"参数. 在它们启动时为它们取不同的名称, 仅仅是为了在后面的操作中方便区别两个客户机.

2) 在 blkio 的层级下, 创建高优先级和低优先级两个群组. 命令行操作如下:

```
[root@kvm-host ~]# cd /sys/fs/cgroup/blkio[root@kvm-host blkio]# mkdir high_prio[root@kvm-host blkio]# mkdir low_prio [root@kvm-host blkio]# ls -l high_prio/total 0-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.io_merged-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.io_merged_recursive-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.io_queued-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.io_queued_recursive-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.io_service_bytes-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.io_service_bytes_recursive-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.io_serviced-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.io_serviced_recursive-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.io_service_time-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.io_service_time_recursive-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.io_wait_time-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.io_wait_time_recursive-rw-r--r-- 1 root root 0 Mar 12 15:22 blkio.leaf_weight-rw-r--r-- 1 root root 0 Mar 12 15:22 blkio.leaf_weight_device--w------- 1 root root 0 Mar 12 15:22 blkio.reset_stats-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.sectors-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.sectors_recursive-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.throttle.io_service_bytes-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.throttle.io_serviced-rw-r--r-- 1 root root 0 Mar 12 15:22 blkio.throttle.read_bps_device-rw-r--r-- 1 root root 0 Mar 12 15:22 blkio.throttle.read_iops_device-rw-r--r-- 1 root root 0 Mar 12 15:22 blkio.throttle.write_bps_device-rw-r--r-- 1 root root 0 Mar 12 15:22 blkio.throttle.write_iops_device-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.time-r--r--r-- 1 root root 0 Mar 12 15:22 blkio.time_recursive-rw-r--r-- 1 root root 0 Mar 12 15:22 blkio.weight-rw-r--r-- 1 root root 0 Mar 12 15:22 blkio.weight_device-rw-r--r-- 1 root root 0 Mar 12 15:22 cgroup.clone_children--w--w--w- 1 root root 0 Mar 12 15:22 cgroup.event_control-rw-r--r-- 1 root root 0 Mar 12 15:22 cgroup.procs-rw-r--r-- 1 root root 0 Mar 12 15:22 notify_on_release-rw-r--r-- 1 root root 0 Mar 12 15:22 tasks[root@kvm-host blkio]# cat high_prio/tasks
```

刚创建好的时候, 以 high_prio 为例, 它这个 cgroups 里面没有 task.

3) 分别将高优先级和低优先级的客户机的 QEMU 进程(及其子进程、子线程)的 tgid 移动到相应的控制群组下面. 下面以高优先级客户机为例, 低优先级客户机类似处理.

```
[root@kvm-host ~]# ps -eLf | grep " high_prio"root      89477   6270  89477  0    6 15:17 pts/1    00:00:04 qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0,mac=52:54:00:12:45:78 -netdev bridge,id=nic0,br=virbr0 -snapshot -name high_prioroot      89477   6270  89478  0    6 15:17 pts/1    00:00:00 qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0,mac=52:54:00:12:45:78 -netdev bridge,id=nic0,br=virbr0 -snapshot -name high_prioroot      89477   6270  89485  1    6 15:17 pts/1    00:00:14 qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0,mac=52:54:00:12:45:78 -netdev bridge,id=nic0,br=virbr0 -snapshot -name high_prioroot      89477   6270  89486  1    6 15:17 pts/1    00:00:13 qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0,mac=52:54:00:12:45:78 -netdev bridge,id=nic0,br=virbr0 -snapshot -name high_prioroot      89477   6270  89487  0    6 15:17 pts/1    00:00:09 qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0,mac=52:54:00:12:45:78 -netdev bridge,id=nic0,br=virbr0 -snapshot -name high_prioroot      89477   6270  89488  1    6 15:17 pts/1    00:00:11 qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0,mac=52:54:00:12:45:78 -netdev bridge,id=nic0,br=virbr0 -snapshot -name high_prioroot      91337  44121  91337  0    1 15:35 pts/4    00:00:00 grep --color=auto  high_prio[root@kvm-host blkio]# echo 89477 > high_prio/cgroup.procs
```

将控制群组正常分组后, 分别查看 high_prio 和 low_prio 两个控制群组中的任务, 命令行如下. 其中, 89477 是高优先级客户机 qemu 的进程 ID(也是这个进程组的 tgid), 89478、89485、89486 等都是 ID 为 89477 进程的子线程的 ID. 低优先级客户机对应的 low_prio 群组中的信息也与此类似.

```
[root@kvm-host blkio]# cat high_prio/tasks 894778947889485894868948789488[root@kvm-host blkio]# cat low_prio/tasks 89563895648957189572895738957491766
```

4) 分别设置高低优先级的控制群组中块设备 I/O 访问的权重. 这里假设高低优先级的比例为 10∶1. 设置权重的命令行如下:

```
[root@kvm-host blkio]# echo 1000 > high_prio/blkio.weight[root@kvm-host blkio]# echo 100 > low_prio/blkio.weight
```

顺便提一下, "blkio.weight"是在一个控制群组中设置块设备 I/O 访问相对比例(即权重)的参数, 其取值范围是 100~1000 的整数.

5) 块设备 I/O 访问控制的效果分析. 假设宿主机系统中磁盘 I/O 访问的最大值是每秒写入 66 MB, 除客户机的 QEMU 进程之外的其他所有进程的磁盘 I/O 访问可以忽略不计, 那么在未设置两个客户机的块设备 I/O 访问权重之前, 在满负载的情况下, 高优先级和低优先级的客户机对实际磁盘的写入速度会同时达到约 33 MB/s. 而在通过 10∶1 的比例设置了它们的权重之后, 高优先级客户机对磁盘的写入速度可以达到约 60 MB/s, 而低优先级的客户机可达到约 6 MB/s.

在 KVM 虚拟化环境中, 通过使用 cgroups, 系统管理员可以对客户机进行细粒度的控制, 包括资源分配、优先级调整、拒绝某个资源的访问、监控系统资源利用率. 硬件资源可以根据不同的客户机进行"智能"的分组, 从整体上提高了资源利用效率, 从某种角度来说, 也可以提高各个客户机之间的资源隔离性, 从而提升 KVM 虚拟机的安全性.

9.2.3　SELinux 和 sVirt

1.SELinux 简介

SELinux(Security-Enhanced Linux)是一个 Linux 内核的一个安全特性, 通过使用 Linux 内核中的 Linux 安全模块(Linux Security Modules, LSM), 它提供一种机制来支持访问控制的安全策略, 如美国国防部风格的强制访问控制(Mandatory Access Controls, MAC). 简单地说, SELinux 提供了一种安全访问机制的体系, 在该体系中进程只能访问其任务中所需要的那些文件. SELinux 中的许多概念是来自美国国家安全局(National Security Agency, NSA)的一些早期项目. SELinu 特性是从 2003 年发布 Linux 内核 2.6 版本开始进入内核主干开发树中的. 在一些 Linux 发型版中, 可以通过使用 SELinux 配置的内核和在用户空间的管理工具来使用 SELinux, 目前 RHEL、Fedora、CentOS、OpenSuse、SLES、Ubuntu 等发行版中都提供对 SELinux 的支持.

使用 SELinux 可以减轻恶意攻击、恶意软件等带来的灾难损失, 并提供对机密性、完整性有较高要求的信息的安全保障. 普通的 Linux 和传统的 UNIX 操作系统一样, 在资源访问控制上采用自由访问控制(Discretionary Access Controls, DAC)策略, 只要符合规定的权限(如文件的所有者和文件属性等), 就可以访问资源. 在这种传统的安全机制下, 一些通过 setuid 或 setgid 的程序就可能产生安全隐患, 甚至有些错误的配置可能引发很大的安全漏洞, 让系统处于脆弱的、容易被攻击的状态. 而 SELinux 是基于强制访问控制(MAC)策略的, 应用程序或用户必须同时满足传统的 DAC 和 SELinux 的 MAC 访问控制, 才能进行资源的访问操作, 否则会被拒绝或返回失败. 但这个过程不影响其他的应用程序, 从而保持了系统的安全性.

强制访问控制(MAC)模式为每一个应用程序都提供了一个虚拟"沙箱", 只允许应用程序执行它设计需要的且在安全策略中明确允许的任务, 对每个应用程序只分配它正常工作所需的对应特权. 例如, Web 服务器可能只能读取网站发布目录中的文件, 并监听指定的网络端口. 即使攻击者将其攻破, 他们也无法执行在安全策略中没有明确允许的任何活动, 即使这个进程在超级用户(root)下运行也不行. 传统 Linux 的权限控制仍然会在系统中存在, 并且当文件被访问时, 此权限控制将先于 SELinux 安全策略生效. 如果传统的权限控制拒绝本次访问, 则该访问直接被拒绝, 在整个过程无须 SELinux 参与. 但是, 如果传统 Linux 权限允许访问, SELinux 此时将进一步对其进行访问控制检查, 并根据其源进程和目标对象的安全上下文来判断允许本次访问还是拒绝访问.

2.sVirt 简介

在非虚拟化环境中, 每台服务器都是物理硬件上隔离的, 一般通过网络来进行相互的通信. 如果一台服务器被攻击了, 一般只会使该服务器不能正常工作, 当然针对网络的攻击也可能影响其他服务器. 在 KVM 这样的虚拟化环境中, 有多个客户机服务器运行在一个宿主机上, 它们使用相同的物理硬件, 如果一个客户机被攻击了, 攻击者可能会利用被攻陷的服务器发起对宿主机的攻击, 如果 Hypervisor 有 bug, 则可能会让攻击很容易地从一个客户机蔓延到其他客户机上.

在传统的文件权限管理中, 一般是将用户分为所有者(owner)、与所有者相同的用户组(group)和其他用户(others). 在虚拟化应用中, 如果使用一个用户账号启动了多个客户机, 那么当其中一个客户机 QEMU 进程有异常行为时, 若它对其他客户机的镜像文件有读写权限, 可能会让其他客户机也暴露在不安全的环境中. 当然可以使一个客户机对应一个用户账号, 使用多个账号来分别启动多个不同的客户机, 以便使客户机镜像访问权限隔离, 而当一个宿主机中有多达数十个客户机时, 就需要对应数十个用户账号, 这样管理和使用起来都会非常不方便.

sVirt 是 Redhat 公司开发的针对虚拟化环境的一种安全技术, 它主要集成了 SELinux 和虚拟化. sVirt 通过对虚拟化客户机使用强制访问控制来提高安全性, 它可以阻止因为 Hypervisor 的 bug 而导致的从一台客户机向宿主机或其他客户机的攻击. sVirt 让客户机之间相互隔离得比较彻底, 而且即使某个客户机被攻陷了, 也能够限制它发起进一步攻击的能力.
sVirt 还是通过 SELinux 来起作用的, 图 9-4 展示了 sVirt 阻止来自一个客户机的攻击.

图 9-4　宿主机中的 sVirt(SELinux)阻止来自客户机的攻击

sVirt 框架允许将客户机及其资源都打上唯一的标签, 如 QEMU 进程有一个标签(tag1), 其对应的客户机镜像也使用这个标签(tag1), 而其他客户机的标签不能与这个标签(tag1)重复. 一旦被打上标签, 就可以很方便地应用规则来拒绝不同客户机之间的访问, SELinux 可以控制仅允许标签相同的 QEMU 进程访问某个客户机镜像.

3.SELinux 和 sVirt 的配置和操作示例

由于 Redhat 公司是 SELinux 的主要开发者之一, 更是 sVirt 最主要的开发者和支持者, 因此 SELinux 和 sVirt 在 Redhat 公司支持的系统中使用得最为广泛, RHEL、Fedora 的较新的版本都有对 sVirt 的支持. 因为 sVirt 是以 SELinux 为基础, 并通过 SELinux 来真正实现访问控制, 所以本节以 RHEL 7.3 系统为例, 介绍 sVirt 的配置和使用也完全与 SELinux 相关.

(1)SELinux 相关的内核配置

查看 RHEL 7.3 系统内核配中 SELinux 相关的内容, 命令行如下:

```
[root@kvm-host ~]# grep -i selinux /boot/config-3.10.0-514.el7.x86_64 CONFIG_SECURITY_SELINUX=yCONFIG_SECURITY_SELINUX_BOOTPARAM=yCONFIG_SECURITY_SELINUX_BOOTPARAM_VALUE=1CONFIG_SECURITY_SELINUX_DISABLE=yCONFIG_SECURITY_SELINUX_DEVELOP=yCONFIG_SECURITY_SELINUX_AVC_STATS=yCONFIG_SECURITY_SELINUX_CHECKREQPROT_VALUE=1# CONFIG_SECURITY_SELINUX_POLICYDB_VERSION_MAX is not setCONFIG_DEFAULT_SECURITY_SELINUX=yCONFIG_DEFAULT_SECURITY="selinux"
```

这里的"CONFIG_SECURITY_SELINUX_BOOTPARAM_VALUE=1"表示在内核启动时默认开启 SELinux. 也可以在 Grub 引导程序的内核启动行中添加"selinux="参数来修改内核启动时的 SELinux 状态: "selinux=0"参数表示在内核启动时关闭 SELinux, "selinux=1"参数表示在内核启动时开启 SELinux.

(2)SELinux 和 sVirt 相关的软件包

RHEL 7.3 中用 rpm 命令查询与 SELinux、sVirt 相关的主要软件包, 命令行如下:

```
[root@kvm-host ~]# rpm -qa | grep selinuxlibselinux-python-2.5-6.el7.x86_64libselinux-2.5-6.el7.x86_64libselinux-devel-2.5-6.el7.x86_64selinux-policy-3.13.1-102.el7.noarchselinux-policy-targeted-3.13.1-102.el7.noarchlibselinux-utils-2.5-6.el7.x86_64[root@kvm-host ~]# rpm -q policycoreutilspolicycoreutils-2.5-8.el7.x86_64
```

其中, 最重要的就是 libselinux、selinux-policy、selinux-policy-targeted 这 3 个 RPM 包, 它们提供了 SELinux 在用户空间的库文件和安全策略配置文件. 而 libselinux-utils RPM 包则提供了设置和管理 SELinux 的一些工具, 如 getenforce、setenforce 等命令行工具. policycoreutils RPM 包提供了一些查询和设置 SELinux 策略的工具, 如 setfiles、fixfiles、sestatus、setsebool 等命令行工具. 另外, 因为 RHEL 中 SELinux 和 sVirt 的安全策略是根据 libvirt 来配置的, 所以一般也需要安装 libvirt 软件包, 这样才能更好地发挥 sVirt 保护虚拟化安全的作用.

(3)SELinux 和 sVirt 的配置文件

SELinux 和 sVirt 的配置文件一般都在/etc/selinux 目录之中, 如下:

```
[root@kvm-host ~]# ls /etc/selinux/config  final  semanage.conf  targeted  tmp[root@kvm-host ~]# ls -l /etc/selinux/config -rw-r--r--. 1 root root 546 Sep 26 19:14 /etc/selinux/config
```

其中最重要的配置文件是"/etc/selinux/config"文件, 在该配置文件中修改设置之后需要重启系统才能生效. 该配置文件的一个示例如下:

```
# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:#     enforcing - SELinux security policy is enforced.#     permissive - SELinux prints warnings instead of enforcing.#     disabled - No SELinux policy is loaded.SELINUX=enforcing# SELINUXTYPE= can take one of these two values:#     targeted - Targeted processes are protected,#     mls - Multi Level Security protection.SELINUXTYPE=targeted
```

其中"SELINUX="项用于设置 SELinux 的状态, 有 3 个值可选: "enforcing"是生效状态, 它会禁止违反规则策略的行为; "permissive"是较为宽松的状态, 它会在发现违反 SELinux 策略的行为时发出警告(而不是阻止该行为), 管理员在看到警告后可以考虑是否修改该策略来满足目前的安全需求; "disabled"是关闭状态, 任何 SELinux 策略都不会生效.

"SELINUXTYPE="项用于设置 SELinux 的策略, 有两个值可选: 一个是目标进程保护策略(targeted), 另一个是多级安全保护策略(mls). 目标进程保护策略仅针对部分已经定义为目标的系统服务和进程执行 SELinux 策略, 会执行"/etc/selinux/targeted"目录中各个具体策略; 而多级安全保护策略是严格分层级定义的安全策略. 一般来说, 选择其默认的"targeted"目标进程保护策略即可.

SELinux 的状态和策略的类型决定了 SELinux 工作的行为和方式, 而具体策略决定具体的进程访问文件时的安全细节. 在目标进程工作模式(targeted)下, 具体的策略配置在"/etc/selinux/targeted"目录中详细定义. 执行如下命令行可查找与 sVirt 直接相关的配置文件:

```
[root@kvm-host targeted]# pwd/etc/selinux/targeted [root@kvm-host targeted]# grep -i svirt * -ractive/file_contexts:/var/lib/kubelet(/.*)?   system_u:object_r:svirt_sandbox_file_t:s0active/file_contexts:/var/lib/docker/vfs(/.*)?   system_u:object_r:svirt_sandbox_file_t:s0active/homedir_template:HOME_DIR/\.libvirt/qemu(/.*)?   system_u:object_r:svirt_home_t:s0<!-- 此处省略几十行输出信息 -->Binary file policy/policy.30 matches
```

"virtual_domain_context"文件配置了客户机的标签, 而"virtual_image_context"文件配置了客户机镜像的标签, "customizable_types"中增加了"svirt_image_t"这个自定义类型.

(4)SELinux 相关的命令行工具

在 RHEL 7.3 中, 命令行查询和管理 SELinux 的工具主要是由"libselinux-utils"和"policycoreutils"这两个 RPM 包提供的. 其中部分命令行工具的使用示例如下:

```
[root@kvm-host ~]# getenforce Enforcing[root@kvm-host ~]# sestatus SELinux status:                 enabledSELinuxfs mount:                /sys/fs/selinuxSELinux root directory:         /etc/selinuxLoaded policy name:             targetedCurrent mode:                   enforcingMode from config file:          enforcingPolicy MLS status:              enabledPolicy deny_unknown status:     allowedMax kernel policy version:      28 [root@kvm-host ~]# getsebool -a | grep httpdhttpd_anon_write --> offhttpd_builtin_scripting --> onhttpd_can_check_spam --> offhttpd_can_connect_ftp --> offhttpd_can_connect_ldap --> offhttpd_can_connect_mythtv --> offhttpd_can_connect_zabbix --> offhttpd_can_network_connect --> offhttpd_can_network_connect_cobbler --> offhttpd_can_network_connect_db --> off <!-- 省略其他输出信息 -->[root@kvm-host ~]# setsebool httpd_enable_homedirs on [root@kvm-host ~]# getsebool -a | grep httpd_enable_homedirshttpd_enable_homedirs --> on [root@kvm-host ~]# mkdir html[root@kvm-host ~]# ll -Z | grep htmldrwxr-xr-x. root root unconfined_u:object_r:admin_home_t:s0 html [root@kvm-host ~]# chcon -R -t httpd_sys_content_t /root/html[root@kvm-host ~]# ll -Z | grep htmldrwxr-xr-x. root root unconfined_u:object_r:httpd_sys_content_t:s0 html
```

下面对这几个命令进行简单介绍.
·setenforce: 修改 SELinux 的运行模式, 其语法为 setenforce[Enforcing|Permissive|1|0], 设置为 1 或 Enforcing 就是让其处于生效状态, 设置为 0 或 Permissive 是让其处于宽松状态(不阻止违反 SELinux 策略的行为, 只是给一个警告提示).

·getenforce: 查询获得 SELinux 当前的状态, 可以是生效(Enforcing)、宽松(Per-missive)和关闭(Disabled)3 个状态.

·sestatus: 获取运行 SELinux 系统的状态, 通过此命令获取的 SELinux 信息更加详细.

·getsebool: 获取当前 SELinux 策略中各个配置项的布尔值, 每个布尔值有开启(on)和关闭(off)两个状态.

·setsebool: 设置 SELinux 策略中配置项的布尔值.

·chcon: 改变文件或目录的 SELinux 安全上下文, "-t[type]"参数是设置目标安全上下文的类型(TYPE), "-R"参数表示递归地更改目录中所有子目录和文件的安全上下文. 另外, "-u[user]"参数更改安全上下文中的用户(User), "-r[role]"参数更改安全上下文中的角色(Role).

(5)查看 SELinux 的安全上下文

在 SELinux 启动后, 所有文件和目录都有各自的安全上下文, 进程的安全上下文是域(domain). 关于安全上下文, 一般遵守如下几个规则.

1) 系统根据 Linux-PAM(可插拔认证模块, Pluggable Authentication Modules)子系统中的 pam_selinux.so 模块来设定登录者运行程序的安全上下文.

2) RPM 包安装时会根据 RPM 包内现有的记录来生成安全上下文.

3) 手动创建的一个文件或者目录, 会根据安全策略中的规则来设置其安全上下文.

4) 如果复制文件或目录(用 cp 命令), 会重新生成安全上下文.

5) 如果移动文件或目录(用 mv 命令), 其安全上下文保持不变.

安全上下文由用户(User)ID、角色(Role)、类型(Type)3 部分组成, 这里的用户和角色与普通系统概念中的用户名和用户分组是没有直接关系的.

·用户 ID(user). 与 Linux 系统中的 UID 类似, 提供身份识别的功能, 例如: user_u 表示普通用户; system_u 表示开机过程中系统进程的预设值; root 表示超级用户 root 登录后的预设; unconfined_u 为不限制的用户. 在默认的目标(targeted)工作模式下, 用户 ID 的设置并不重要.

·角色(role). 普通文件和目录的角色通常是 object_r; 用户可以具有多个角色, 但是在同一时间内只能使用一角色; 用户的角色类似于普通系统中的 GID(用户组 ID), 不同的角色具备不同的权限. 在默认的目标工作模式下, 用户角色的设置也不重要.

·类型(type). 将主体(程序)与客体(程序访问的文件)划分为不同的组, 一个组内的各个主体和系统中的客体都定义了一个类型. 类型是安全上下文中最重要的值, 一般来说, 一个主体程序能不能读取到这个文件资源, 与类型这一栏的值有关. 类型有时会有两个名称: 在文件资源(Object)上称为类型(Type); 而在主体程序(Subject)上称为域(Domain). 只有域与类型匹配, 一个程序才能正常访问一个文件资源.

·层级(level). 这是多层级安全(MLS)和多类别安全(MCS)的概念. 表示为类似 s0: c0-c1023 的形式. 一个层级可以对应 1024 个具体类别.
一般来说, 有 3 种安全上下文: 账号的安全上下文; 进程的安全上下文; 文件或目录的安全上下文. 可以分别用"id-Z""ps-Z""ls-Z"等命令进行查看. 示例如下:

```
[root@kvm-host ~]# id -Zunconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023[root@kvm-host ~]# ps -eZ | grep httpdsystem_u:system_r:httpd_t:s0      3389 ?        00:00:00 httpdsystem_u:system_r:httpd_t:s0      3794 ?        00:00:00 httpdsystem_u:system_r:httpd_t:s0      3795 ?        00:00:00 httpdsystem_u:system_r:httpd_t:s0      3796 ?        00:00:00 httpdsystem_u:system_r:httpd_t:s0      3797 ?        00:00:00 httpdsystem_u:system_r:httpd_t:s0      3798 ?        00:00:00 httpd[root@kvm-host ~]# ls -Z /var/www/drwxr-xr-x. root root system_u:object_r:httpd_sys_script_exec_t:s0 cgi-bindrwxr-xr-x. root root system_u:object_r:httpd_sys_content_t:s0 html
```

另外, chcon 命令行工具可以改变文件或目录的 SELinux 安全上下文内容, 对该工具的介绍和使用示例已经在本节前面部分提及了.
(6)sVirt 提供的与虚拟化相关的标签
sVirt 使用基于进程的机制和约束, 为虚拟化客户机提供了一个额外的安全保护层. 在一般情况下, 只要不违反约束的策略规则, 用户是不会感知到 sVirt 在后台运行的. sVirt 和 SELinux 将客户机用到的资源打上标签, 也对相应的 QEMU 进程打上标签, 然后保证只允许具有与被访问资源相对应的类型(type)和相同分类标签的 QEMU 进程访问某个磁盘镜像文件或其他存储资源. 如果有违反策略规则的进程试图非法访问资源, SELinux 会直接拒绝它的访问操作, 返回一个权限不足的错误提示(通常为"Permission denied").
sVirt 将 SELinux 与 QEMU/KVM 虚拟化进行了结合. sVirt 一般的工作方式是: 在 RHEL 系统中使用 libvirt 的守护进程(libvirtd)在后台管理客户机, 在启动客户机之前, libvirt 动态选择一个带有两个分类标志的随机的 MCS 标签(如 s0: c1, c2), 将该客户机使用到的所有存储资源(包括磁盘镜像、光盘等)都打上相应的标签(svirt_image_t: s0: c1, c2), 然后用该标签(svirt_t: c0: c1, c2)执行 qemu-kvm 进程, 从而启动了客户机.
在客户机启动之前, 查看一些关键的程序和磁盘镜像的安全上下文. 示例如下:

依次查看 libvirtd、virsh、qemu 的安全上下文

```
[root@kvm-host ~]# ls -Z /usr/sbin/libvirtd-rwxr-xr-x. root root system_u:object_r:virtd_exec_t:s0 /usr/sbin/libvirtd [root@kvm-host ~]# ls -Z /usr/bin/virsh-rwxr-xr-x. root root system_u:object_r:virsh_exec_t:s0 /usr/bin/virsh [root@kvm-host ~]# ls -Z /usr/libexec/qemu-kvm -rwxr-xr-x. root root system_u:object_r:qemu_exec_t:s0 /usr/libexec/qemu-kvm # 查看 libvirt 创建的客户机镜像的安全上下文. [root@kvm-host ~]# ll -Z *.img-rw-------. root root system_u:object_r:admin_home_t:s0 ia32e_rhel7u3_kvm-clone.img-rw-r--r--. root root system_u:object_r:admin_home_t:s0 ia32e_rhel7u3_kvm.img-rw-r--r--. root root system_u:object_r:admin_home_t:s0 raw_disk.img-rw-r--r--. root root system_u:object_r:admin_home_t:s0 rhel7.img
```

可以看到, 由 libvirt 创建管理的客户机镜像与其他客户机镜像的安全上下文是一样的(system_u: object_r: admin_home_t: s0).
然后分别使用上面 ia32e_rhel7u3_kvm.img 和 ia32e_rhel7u3_kvm-clone.img 两个磁盘镜像, 用 libvirt 的命令行或者 virt-manager 图形界面工具启动两个客户机. 在客户机启动后, 查看磁盘镜像和 qemu-kvm 进程的安全上下文. 命令行如下:

[root@kvm-host ~]# ll -Z *.img-rw-------. qemu qemu system_u:object_r:svirt_image_t:s0:c570,c648 ia32e_rhel7u3_kvm-clone.img-rw-r--r--. qemu qemu system_u:object_r:svirt_image_t:s0:c208,c224 ia32e_rhel7u3_kvm.img-rw-r--r--. root root system_u:object_r:admin_home_t:s0 raw_disk.img-rw-r--r--. root root system_u:object_r:admin_home_t:s0 rhel7.img[root@kvm-host ~]# ps -eZ | grep qemusystem_u:system_r:svirt_t:s0:c208,c224 87761 ?  00:00:18 qemu-kvmsystem_u:system_r:svirt_t:s0:c570,c648 87775 ?  00:00:17 qemu-kvm

由上面的输出信息可知, 两个磁盘镜像已经被动态地标记上了不同的标签, 两个相应的 qemu-kvm 进程也被标记上了对应的标签. 其中, PID 为 87761 的 qemu-kvm 进程对应的是 ia32e_rhel7u3_kvm.img 这个磁盘镜像, 由于 SELinux 和 sVirt 的配合使用, 即使 87761 qemu-kvm 进程对应的客户机被黑客入侵, 其 qemu-kvm 进程(PID 87761)也无法访问非它自己所有的 ia32e_rhel7u3_kvm-clone.img 这个镜像文件, 也就不会攻击到其他客户机(当然, 通过网络发起的攻击除外).
(7)sVirt 根据标签阻止不安全访问的示例
前面已经提及, 如果进程与资源的类型和标签不匹配, 那么进程对该资源的访问将会被拒绝. 真的这么有效吗?下面, 笔者通过示例来证明它的有效性: 模拟一些不安全的访问, 然后查看 sVirt 和 SELinux 是否将其阻止.
首先, 将 bash 这个 Shell 程序复制一份, 并将其重命名为 svirt-bash, 以"svirt_t"类型和"s0: c1, c2"标签执行 svirt-bash 程序, 进入这个 Shell 环境, 用"id-Z"命令查看当前用户的安全上下文. 然后创建/tmp/svirt-test.txt 这个文件, 并写入一些文字, 可以看到该文件的安全上下文中类型和标签分别是: svirt_tmp_t 和 s0: c1, c2. 该过程的命令行操作如下(包括部分命令行的注释):

```
# 复制一个 Shell, 重命名为/bin/svirt-bash[root@kvm-host ~]# cp /bin/bash /bin/svirt-bash # 默认策略规定, 使用 svirt_t 类型的入口程序必须标记为"qemu_exec_t"#类型, 与/usr/libexec/qemu-kvm 文件的安全上下文类似[root@kvm-host ~]# chcon -t qemu_exec_t /bin/svirt-bash # runcon 命令以某个特定的 SELinux 安全上下文来执行某个命令[root@kvm-host ~]# runcon -t svirt_t -l s0:c1,c2 /bin/svirt-bash svirt-bash: /root/.bashrc: Permission denied   #由于类型改变后无法读取.bashrc 配置文件, 对本次实验无影响, 忽略该错误提示# 已经进入使用 svirt_t:s0:c1,c2 这样的安全上下文的 Shell 环境中了 svirt-bash-4.2# id -Zunconfined_u:unconfined_r:svirt_t:s0:c1,c2 # 可能会遇到"svirt: child setpgid (89383 to 89383): Permission denied"这样的错误提示# Shell 试图为每个命令都设置进程的组 ID(setpgid), 对本次实验无影响# 在本次实验中忽略 setpgid 带来的错误提示 svirt-bash-4.2# echo "just for testing" >> /tmp/svirt-test.txtsvirt-bash-4.2# ls -Z /tmp/svirt-test.txt -rw-r--r--. root root unconfined_u:object_r:svirt_tmp_t:s0 /tmp/svirt-test.txt # 在另一个 shell 中, 将 svirt-test.txt 文件设置为层级 s0:c1,c2[root@kvm-host ~]# chcon -l s0:c1,c2 /tmp/svirt-test.txt# 在 svirt-bash 中查看更新过安全上下文的 svirt-test.txt 文件, 并尝试访问 svirt-bash-4.2# ls -l /tmp/svirt-test.txt -Z-rw-r--r--. root root unconfined_u:object_r:svirt_tmp_t:s0:c1,c2 /tmp/svirt-test.txtsvirt-bash-4.2# cat /tmp/svirt-test.txt just for testing
```

使用"svirt_t"类型和"s0: c1, c3"(与前面"s0: c1, c2"不同)标签启动 svirt-bash 这个 Shell 程序, 进到该 Shell 环境, 用"id-Z"命令查看当前用户的安全上下文, 用"ls-Z"查看上一步创建的临时测试文件的安全上下文, 试着用"cat"命令去读取该测试文件(svirt-test.txt). 该过程的命令行操作如下:

```
[root@kvm-host ~]# runcon -t svirt_t -l s0:c1,c3 /bin/svirt-bashsvirt-bash: /root/.bashrc: Permission deniedsvirt-bash-4.2#svirt-bash-4.2# id -Zunconfined_u:unconfined_r:svirt_t:s0:c1,c3 svirt-bash-4.2# ls -Z /tmp/svirt-test.txt -rw-r--r--. root root unconfined_u:object_r:svirt_tmp_t:s0:c1,c2 /tmp/svirt-test.txt# 在"s0:c1,c3"的标签下, 测试文件读取访问被拒绝 svirt-bash-4.2# cat /tmp/svirt-test.txt cat: /tmp/svirt-test.txt: Permission denied
```

可知, 在标签为"s0: c1, c3"的 Shell 环境中, 不能访问具有"s0: c1, c2"标签的一个临时测试文件, sVirt 拒绝本次非法访问, 故 sVirt 基于标签的安全访问控制策略是生效的.
在使用 Redhat 相关系统(如 RHEL、Fedora 等)时, 如果对 KVM 虚拟化安全性要求较高, 可以选择安装 SELinux 和 sVirt 相关的软件包, 使用 sVirt 来加强虚拟化中的安全性. 不过, 由于 SELinux 配置还是比较复杂的, 其默认策略对其他各种服务的资源访问限制还是较多的(为了安全性), 所以受到一些系统管理员的排斥, 一些系统管理员经常会关闭 SELinux. 总之, 一切从实际出发, 应根据实际项目中对安全性的需求对 SELinux 和 sVirt 进行相应的配置.

9.2.4　其他安全策略

1.镜像文件加密

随着网络与计算机技术的发展, 数据的一致性和数据的完整性在信息安全中变得越来越重要, 对数据进行加密处理对数据的一致性和完整性都有较好的保障. 有一种类型的攻击叫作"离线攻击", 如果攻击者在系统关机状态下可以物理接触到磁盘或其他存储介质, 就属于"离线攻击"的一种表现形式. 假设, 在一个公司内部, 不同职位的人有不同的职责和权限, 系统处于启动状态时的使用者是工作人员 A, 而系统关机后会存放在另外的位置(或不同部门), 而工作人员 B 可以获得该系统的物理硬件. 如果没有保护措施, 那么 B 就可以轻易地越权获得系统中的内容. 如果有了良好的加密保护, 就可以防止这样的攻击或内部数据泄露事件的发生.
在 KVM 虚拟化环境中, 存放客户机镜像的存储设备(如磁盘、U 盘等)可以对整个设备进行加密, 如果其分区是 LVM, 也可以对某个分区进行加密. 而对于客户机镜像文件本身, 也可以进行加密的处理, 如 5.4.3 节中已经介绍过 qcow2 镜像文件格式支持加密, 这里再简单介绍一下.
"qemu-img convert"命令在"-o encryption"参数的支持下, 可以将未加密或已经加密的镜像文件转化为加密的 qcow2 的文件格式. 先创建一个 8 GB 大小的 qcow2 格式镜像文件, 然后用"qemu-img convert"命令将其加密. 命令行操作如下:

```
[root@kvm-host ~]# qemu-img create -f qcow2 -o size=8G  guest.qcow2Formatting 'guest.qcow2', fmt=qcow2 size=8589934592 encryption=off cluster_size=65536 lazy_refcounts=off refcount_bits=16     [root@kvm-host ~]# qemu-img convert -o encryption -O qcow2 guest.qcow2 encrypted.qcow2Disk image 'encrypted.qcow2' is encrypted.password: ******  #此处输入你需要设置的密码, 然后按回车键确认
```

生成的 encrypted.qcow2 文件就是已经加密的文件. 查看其信息如下所示, 可以看到"encrypted: yes"的标志.

```
[root@kvm-host ~]# qemu-img info encrypted.qcow2image: encrypted.qcow2file format: qcow2virtual size: 8.0G (8589934592 bytes)disk size: 196Kencrypted: yescluster_size: 65536Format specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: false
```

在使用加密的 qcow2 格式的镜像文件启动客户机时, 客户机会先不启动而暂停, 需要在 QEMU monitor 中输入"cont"或"c"命令以便继续执行, 然后会要求输入已加密 qcow2 镜像文件的密码, 只有密码输入正确才可以正常启动客户机. 在 QEMU monitor 中的命令行示例如下:

```
(qemu) cide0-hd0 (encryped.qcow2) is encrypted.Password: ******       #此处输入先前设置过的密码(qemu)
```

当然, 在执行"qemu-img create"创建镜像文件时就可以将其创建为加密的 qcow2 文件格式, 但是不能交互式地指定密码. 命令行如下:

```
[root@kvm-host ~]# qemu-img create -f qcow2 -o backing_file=./rhel7.img,encryption encrypted.qcow2Formatting 'encrypted.qcow2', fmt=qcow2 size=42949672960 backing_file=./rhel7.img encryption=on cluster_size=65536 lazy_refcounts=off refcount_bits=16
```

这样创建的 qcow2 文件处于加密状态, 但是其密码为空, 在使用过程中提示输入密码时, 直接按回车键即可. 对于在创建时已设置为加密状态的 qcow2 文件, 仍然需要用上面介绍过的"qemu-img convert"命令来转换一次, 这样才能设置为自己所需的非空密码.
当使用 qcow2 镜像文件的加密功能时, 每次都要输入密码, 可能会给大规模地自动化部署 KVM 客户机带来一定的障碍. 其实, 输入密码验证的问题也是很容易解决的, 例如 Linux 上的 expect 工具就很容易解决需要输入密码这样的交互式会话问题. 在使用 expect 进行 ssh 远程登录过程中, 输入密码交互的一段 Bash 脚本如下:

```
SSH_Time_Out() {    local host=192.168.111.111local time_out=250local command="ifconfig"    local exp_cmd=$(cat << EOFspawn ssh root@$host $commandset timeout $time_outexpect {    "*assword:"  { send "123456\r"; exp_continue}    "*(yes/no)?" { send "yes\r"; exp_continue }    eof          { exit 0 }}EOF)    expect -c "$exp_cmd"    return $?}
```

上面的脚本示例实现了用 ssh 远程登录到一台主机上, 并执行了一个 ifconfig 命令. 其中远程主机的 root 用户的密码是通过 expect 的脚本来实现自动输入的, expect 的超时时间在这里被设置为 250 秒.
2.虚拟网络的安全
在 KVM 宿主机中, 为了网络安全的目的, 可以使用 Linux 防火墙——iptables 工具. 使用 iptables 工具(为 IPv4 协议)或 ip6tables(为 IPv6 协议), 可以创建、维护和检查 Linux 内核中 IP 数据报的过滤规则.
而对于客户机的网络, QEMU/KVM 提供了多种网络配置方式(见 5.5 节). 例如: 使用 NAT 方式让客户机获取网络, 就可以对外界隐藏客户机内部网络的细节, 对客户机网络的安全起到了保护作用. 不过, 在默认情况下, NAT 方式的网络让客户机可以访问外部网络, 而外部网络不能直接访问客户机. 如果客户机中的服务需要被外部网络直接访问, 就需要在宿主机中配置好 iptables 的端口映射规则, 通过宿主机的某个端口映射到客户机的一个对应端口, 其具体配置的示例已经在 5.5.3 节中详细介绍了.
如果物理网卡设备比较充足, 而且 CPU、芯片组、主板等都支持设备的直接分配技术(如 Intel VT-d 技术), 那么选择使用设备直接分配技术(VT-d)为每个客户机分配一个物理网卡也是一个非常不错的选择. 因为在使用设备直接分配使用网卡时, 网卡工作效率非常高, 而且各个客户机中的网卡在物理上是完全隔离的, 提高了客户机的隔离性和安全性, 即使一个客户机中网络流量很大(导致了阻塞)也不会影响其他客户机中网络的质量. 关于网卡设备的直接分配, 可以参考 6.2.4 节中的详细配置.
3.远程管理的安全
在 KVM 虚拟化环境中, 可以通过 VNC 的方式远程访问客户机. 为了虚拟化管理的安全性, 可以为 VNC 连接设置密码, 并且可以设置 VNC 连接的 TLS、X.509 等安全认证方式. 关于 VNC 的配置和密码设置等相关内容, 可以参考 5.6.2 节.
如果使用 libvirt 的应用程序接口来管理虚拟机, 包括使用 virsh、virt-manager、virt-viewer 等工具, 为了远程管理的安全性考虑, 最好只允许管理工具使用 SSH 连接或者带有 TLS 加密验证的 TCP 套接字来连接到宿主机的 libvirt. 关于 libvirt API 的简介及其配置、连接的方式, 可以阅读 4.1 节的内容.
4.普通 Linux 系统的安全准则
KVM 宿主机及运行在 KVM 上 Linux 客户机都是普通的 Linux 操作系统. 普通 Linux 系统的一些安全策略和准则都可以用于提高 KVM 环境的安全性.
美国国家安全局(National Security Agency, NSA)的一份公开文档中谈及了 Redhat Linux 系统的安全设置应遵循的几个通用原则, 对于任何 Linux 系统(甚至 Windows 系统)的安全配置都有一定的借鉴意义. 这里简要分享一下, 仅供读者参考.
(1)对传送的数据尽可能地进行加密处理
无论是在有线网络还是无线网络上, 传输的数据都很容易被监听, 所以只要对这些数据的加密方案存在, 都应该使用加密方案. 即使是预期仅在局域网中传输的数据, 也应该做加密处理. 对于一些身份认证相关的数据(如密码)等的加密尤其重要. RHEL 5/6/7 版本的 Linux 系统组成的网络中, 可以配置为它们之间传输的全部数据都经过加密处理.
(2)安装尽可能少的软件, 从而使软件漏洞数量尽可能的少
避免一个软件漏洞最简单的方法就是避免安装那个软件. 在 RHEL 系统中, 有 RPM 软件包管理工具可以比较方便地管理系统中已经安装的软件包. 随意安装的软件可能会以多种方式暴露出系统的漏洞:
1)包含 setuid 程序的软件包可能为本地的攻击者提供一种特权扩大的潜在途径.
2)包含网络服务的软件包可能为基于网络的攻击者提供攻击的机会.
3)包含被本地用户有预期地执行的程序(如图形界面登录后必须执行的程序)的软件可能会给特洛伊木马或者其他隐藏执行的攻击代码提供机会. 通常, 一个系统上安装的软件包数量可以被大量删减, 直到只包含环境或者操作所真正必需的软件.
(3)不同的网络服务尽量运行在不同的系统上
一个服务器(当然也可以是虚拟客户机)应该尽可能地专注于提供一个网络服务. 如果这样做, 即使一个攻击者成功地渗透了一个网络服务上的软件漏洞, 也不会危害到其他服务的正常运行.
(4)配置一些安全工具提高系统的鲁棒性
现存的几个工具可以有效地提高系统的抵抗能力和检测未知攻击的能力. 这些工具可以利用较低的配置成本去提高系统面对攻击时的鲁棒性. 有一些实际的工具就具有这样的能力, 如基于内核的防火墙——iptables, 基于内核的安全保护软件——SELinux, 以及其他一些日志记录和进程审计的软件基础设施.
(5)使用尽可能少的权限
只授予用户账号和运行的软件最少的必需的权限. 例如, 仅给真正需要用 sudo 管理员权限的用户开放 sudo 的功能(在 Ubuntu 中默认开放过多的 sudo 权限可能并不太安全); 限制系统的登录, 仅向需要管理系统的用户开放登录的权限. 另外, 还可以使用 SELinux 来限制一个软件对系统上其他资源的访问权限, 合适的 SELinux 策略可以让某个进程仅有权限做应该做的事情.
