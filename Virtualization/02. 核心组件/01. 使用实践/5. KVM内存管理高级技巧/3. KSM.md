
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [KSM 概述](#ksm-概述)
  - [KSM 的基本原理](#ksm-的基本原理)
  - [虚拟化场景下的 KSM](#虚拟化场景下的-ksm)
    - [提高内存速度和使用效率](#提高内存速度和使用效率)
  - [内核的支持以及配置](#内核的支持以及配置)
  - [1.4 QEMU 中的 KSM 支持](#14-qemu-中的-ksm-支持)
  - [1.5 KSM 可能带来的问题](#15-ksm-可能带来的问题)
    - [1.5.1 CPU 使用率提高](#151-cpu-使用率提高)
    - [1.5.2 客户机 OS 和应用程序大不相同时效率不高](#152-客户机-os-和应用程序大不相同时效率不高)
    - [1.5.3 足够的交换空间](#153-足够的交换空间)
- [2 KSM 操作实践](#2-ksm-操作实践)
  - [2.1 配置守护进程 ksmd](#21-配置守护进程-ksmd)
    - [2.1.1 ksmd 相关配置文件](#211-ksmd-相关配置文件)
    - [2.1.2 ksm 相关数据影响](#212-ksm-相关数据影响)
  - [2.2 ksmtuned: 动态调节 ksm](#22-ksmtuned-动态调节-ksm)
    - [2.2.1 ksm 和 ksmtuned 服务](#221-ksm-和-ksmtuned-服务)
    - [2.2.2 ksmtuned 配置文件](#222-ksmtuned-配置文件)
    - [2.2.3 KSM 实际操作效果对比](#223-ksm-实际操作效果对比)
- [3 QEMU 对 KSM 的控制](#3-qemu-对-ksm-的控制)

<!-- /code_chunk_output -->

在现代操作系统中, **共享内存**被很普遍地应用. 比如著名的"写时复制"(`copy-on-write`, COW)技术.

而本节介绍的 KSM 技术却与这种内存共享概念不同.

# KSM 概述

KSM 是"**Kernel SamePage Merging**"的缩写, 中文可称为"**内核同页合并**".

## KSM 的基本原理

KSM 允许内核在两个或多个进程(包括虚拟客户机)之间共享完全相同的内存页. KSM 让**内核扫描检查**正在**运行中的程序**并**比较它们的内存**, 如果发现它们有**完全相同的内存区域或内存页**, 就将多个相同的内存合并为一个单一的内存页, 并将其标识为"**写时复制**". 这样可以起到节省系统内存使用量的作用. 之后, 如果有进程试图去修改被标识为"写时复制"的合并内存页, 就为该进程复制出一个新的内存页供其使用.

## 虚拟化场景下的 KSM

在 QEMU/KVM 中, **一个虚拟客户机**就是**一个 QEMU 进程**, 所以使用 KSM 也可以实现**多个客户机之间**的**相同内存合并**. 而且, 如果在**同一宿主机**上的**多个客户机**运行的是**相同的操作系统**或**应用程序**, 则客户机之间的相同内存页的数量就可能比较大, 这种情况下**KSM 的作用就更加显著**.

在 KVM 环境下使用 KSM, 还允许 KVM 请求哪些相同的内存页是**可以被共享而合并的**, 所以 KSM**只会识别**并合并那些**不会干扰客户机运行**且**不会影响宿主机或客户机运行的安全内存页**.

### 提高内存速度和使用效率

可见, 在 KVM 虚拟化环境中, KSM 能够提高**内存的速度**和**使用效率**.

具体可以从以下两个方面来理解.

1)在**KSM**的帮助下, **相同的内存页被合并**了, 减少了客户机的内存使用量.

- 一方面, **内存中的内容**更容易被保存到**CPU 的缓存**中,
- 另一方面, **有更多的内存**可用于**缓存一些磁盘中的数据**.

因此, 不管是**内存的缓存命中率(CPU 缓存命中率**), 还是**磁盘数据的缓存命中率**(在**内存！！！中命中磁盘数据缓存**的命中率)都会提高, 从而提高了 KVM 客户机中操作系统或应用程序的**运行速度**.

2)正如在 5.3.3 节中提及的那样, **KSM**是**内存过载使用**的一种**较好的方式**.

KSM 通过**减少每个客户机实际占用的内存数量**, 可以让多个客户机分配的内存数量之和大于物理上的内存数量. 而对于使用相同内存量的客户机而言, 在物理内存量不变的情况下, 可以在一个宿主机中创建更多的客户机, 提高了虚拟化客户机部署的密度, 提高了**物理资源的利用效率**.

## 内核的支持以及配置

KSM 是在**Linux 内核 2.6.32**中被加入内核主干代码中去的. 目前多数流行的 Linux 发型版都已经将 KSM 的支持**编译到内核**中了, 其内核配置文件中有"**CONFIG_KSM=y**"项.

Linux 系统的**内核进程 ksmd**负责扫描后合并进程的**相同内存页**, 从而实现 KSM 功能. root 用户可以通过"/sys/kernel/mm/**ksm**/"**目录**下的文件来**配置和监控 ksmd 这个守护进程**.

KSM**只会**去扫描和试图合并那些**应用程序建议！！！为可合并的内存页**, **应用程序**(如 QEMU)通过如下的**madvice 系统调用**来告诉内核**哪些页可合并**.

## 1.4 QEMU 中的 KSM 支持

目前的 QEMU 都是支持 KSM 的, 也可以通过查看**其代码中对 madvise 函数的调用**情况来确定是否支持 KSM.

QEMU 中的关键函数简要分析如下:

```c
/* 将地址标志为 KSM 可合并的系统调用*/
/* int madvise(addr, length, MADV_MERGEABLE) */
/* madvise 系统调用的声明在 <sys/mman.h>中*/
/* int　madvise(　void　*start,　size_t　length,　int　advice　);　*/

/* qemu 代码的 exec.c 文件中, 开启内存可合并选项*/
static int memory_try_enable_merging(void *addr, size_t len)
{
/*这里可以看到: 通过 qemu 的-machine mem-merge=on|off 参数可以对每个客户机开启或关闭 KSM 支持.
  我们后面通过实例观察效果. */
    if (!machine_mem_merge(current_machine)) {
        /* disabled by the user */
        return 0;
    }

    return qemu_madvise(addr, len, QEMU_MADV_MERGEABLE);
}

/* qemu 代码的 osdep.c 文件中对 qemu_madvise()函数的定义*/
int qemu_madvise(void *addr, size_t len, int advice)
{
    if (advice == QEMU_MADV_INVALID) {
        errno = EINVAL;
        return -1;
    }
#if defined(CONFIG_MADVISE)
    return madvise(addr, len, advice);
#elif defined(CONFIG_POSIX_MADVISE)
    return posix_madvise(addr, len, advice);
#else
    errno = EINVAL;
    return -1;
#endif
}

/*在 osdep.h 中看到, 只有 QEMU configure 了 CONFIG-MADVISE(检查你的 config-host.mak)并且你
的宿主机系统支持 MADV_MERGEABLE 标准 POSIX 系统调用, QEMU 才可以支持 KSM; 否则, QEMU 就不会去调用 POSIX 接口来做 KSM*/
#if defined(CONFIG_MADVISE)

...
#ifdef MADV_MERGEABLE
#define QEMU_MADV_MERGEABLE MADV_MERGEABLE
#else
#define QEMU_MADV_MERGEABLE QEMU_MADV_INVALID
#endif
```

**KSM**最初就是**为 KVM 虚拟化中的使用而开发**的, 不过它对非虚拟化的系统依然非常有用.

## 1.5 KSM 可能带来的问题

由于 KSM 对 KVM 宿主机中的内存使用有较大的效率和性能的提高, 所以一般**建议打开 KSM 功能**.

### 1.5.1 CPU 使用率提高

不过, "金无足赤, 人无完人", KSM 必须有**一个或多个进程**去检测和找出哪些内存页是完全相同可以用于合并的, 而且需要**找到那些不会经常更新的内存页**, 这样的页才是最适合于合并的. 因此, KSM 让内存使用量降低了, 但是**CPU 使用率会有一定程度的升高**, 也可能会带来隐蔽的**性能问题**, 需要在实际使用环境中进行适当配置 KSM 的使用, 以便达到较好的平衡.

### 1.5.2 客户机 OS 和应用程序大不相同时效率不高

KSM 对内存合并而节省内存的数量与客户机操作系统类型及其上运行的应用程序有关, 如果宿主机上的客户机操作系统相同且其上运行的应用程序也类似, 节省内存的效果就会很显著, 甚至节省超过 50%的内存都有可能的. 反之, 如果**客户机操作系统不同**, 且运行的**应用程序也大不相同**, KSM**节省内存效率就不高**, 可能连 5%都不到.

### 1.5.3 足够的交换空间

另外, 在使用**KSM 实现内存过载使用**时, 最好保证系统的**交换空间(swap space**)足够大. 因为**KSM**将**不同客户机的相同内存页合并**而减少了内存使用量, 但是**客户机**可能由于需要**修改被 KSM 合并的内存页**, 从而使这些被修改的内存被**重新复制出来占用内存空间**, 因此可能会导致**系统内存的不足**, 这时就需要有足够的交换空间来保证系统的正常运行.

# 2 KSM 操作实践

## 2.1 配置守护进程 ksmd

内核的 KSM 守护进程是**ksmd**, 配置和监控 ksmd 的文件在"/sys/kernel/mm/ksm/"目录下.

### 2.1.1 ksmd 相关配置文件

通过如下命令行可以查看该目录下的几个文件:

```
[root@kvm-host ~]# ps -eLf | grep -i ksm
root      468    2   468  0  1  2016 ?     00:00:00 [ksmd]
root     1605    1  1605  0  1  2016 ?     00:00:13 /bin/bash /usr/sbin/ksmtuned
[root@kvm-host ~]# ls -l /sys/kernel/mm/ksm/
total 0
-r--r--r-- 1 root root 4096 Dec 31 11:35 full_scans
-rw-r--r-- 1 root root 4096 Dec 31 11:35 max_page_sharing
-rw-r--r-- 1 root root 4096 Dec 31 11:35 merge_across_nodes
-r--r--r-- 1 root root 4096 Dec 31 11:35 pages_shared
-r--r--r-- 1 root root 4096 Dec 31 11:35 pages_sharing
-rw-r--r-- 1 root root 4096 Dec 31 11:35 pages_to_scan
-r--r--r-- 1 root root 4096 Dec 31 11:35 pages_unshared
-r--r--r-- 1 root root 4096 Dec 31 11:35 pages_volatile
-rw-r--r-- 1 root root 4096 Jan  8 10:59 run
-rw-r--r-- 1 root root 4096 Dec 31 11:35 sleep_millisecs
-r--r--r-- 1 root root 4096 Dec 31 11:35 stable_node_chains
-rw-r--r-- 1 root root 4096 Dec 31 11:35 stable_node_chains_prune_millisecs
-r--r--r-- 1 root root 4096 Dec 31 11:35 stable_node_dups
```

这里面的几个文件对于了解 KSM 的实际工作状态来说是非常重要的. 下面简单介绍各个文件的作用.

- **full_scans**: 记录已经对**所有可合并的内存区域**扫描过的**次数**.
- **merge_across_nodes**: 在 NUMA(见 7.4 节)架构的平台上, **是否允许跨节点**(node)**合并内存页**.
- **pages_shared**: 记录**正在使用中**的**共享内存页的数量**.
- **pages_sharing**: 记录有**多少数量的内存页**正在使用**被合并的共享页**, 不包括合并的**内存页本身**. 这就是**实际节省的内存页数量**.
- pages_unshared: 记录了守护进程去检查并试图合并, 却发现了因**没有重复内容**而**不能被合并**的**内存页数量**.
- pages_volatile: 记录了因为其**内容很容易变化**而**不被合并的内存页**.
- pages_to_scan: 在**ksmd 进程休眠之前**扫描的**内存页数量**.
- sleep_millisecs: ksmd 进程**休眠的时间**(单位: 毫秒), ksmd 的**两次运行之间的间隔**.
- run: 控制 ksmd 进程**是否运行的参数**, 默认值为 0, 要**激活 KSM**必须要设置其值为**1**(除非内核**关闭了 sysfs 的功能**).
    - 设置为 0, 表示**停止运行 ksmd**但保持它已经合并的内存页;
    - 设置为 1, 表示马上运行 ksmd 进程;
    - 设置为 2 表示停止运行 ksmd, 并且分离已经合并的所有内存页, 但是保持已经注册为可合并的内存区域给下一次运行使用.

通过前面查看这些 sysfs 中的 ksm 相关的文件可以看出, 只有**pages_to_scan**、**sleep_millisecs**、**run**这 3 个文件对 root 用户是**可读可写**的, 其余 6 个文件都是**只读**的.

可以向 pages_to_scan、sleep_millisecs、run 这 3 个文件中写入自定义的值, 以便**控制 ksmd 的运行**.

例如,

- "echo 1200>/sys/kernel/mm/ksm/pages_to_scan", 用来调整**每次扫描**的**内存页数量**,

- "echo 10>/sys/kernel/mm/ksm/sleep_millisecs", 用来设置 ksmd 两次运行的**时间间隔**,

- "echo 1>/sys/kernel/mm/ksm/run", 用来**激活 ksmd 的运行**.

### 2.1.2 ksm 相关数据影响

**pages_sharing**的值越大, 说明 KSM 节省的内存越多, KSM 效果越好. 如下命令计算了节省的内存数量:

```
[root@kvm-host ~]# echo "KSM saved: $(( $(cat /sys/kernel/mm/ksm/pages_sharing) * $(getconf PAGESIZE) / 1024 / 1024 ))MB"
KSM saved: 7429MB
```

而 pages_sharing 除以 pages_shared 得到的值越大, 说明**相同内存页重复的次数越多**, **KSM 效率就越高**.

pages_unshared 除以 pages_sharing 得到的值越大, 说明 ksmd 扫描**不能合并的内存页越多**, KSM 的**效率越低**.

可能有多种因素影响 pages_volatile 的值, 不过**较高的 page_voliatile**值预示着很可能有**应用程序过多地使用了 madvise**(addr, length, MADV_MERGEABLE)系统调用, 将其内存标志为**KSM 可合并**.

## 2.2 ksmtuned: 动态调节 ksm

在通过"/sys/kernel/mm/ksm/run"等修改了 KSM 的设置之后, 系统**默认不会再修改**它的值, 这样可能并不能更好地使用后续的系统状况, 或者经常需要人工动态调节是比较麻烦的.

### 2.2.1 ksm 和 ksmtuned 服务

Redhat 系列系统(如 RHEL 6、RHEL 7)中提供了两个服务**ksm**和**ksmtuned**, 来**动态调节 KSM 的运行情况**.

RHEL 7.3 中**ksm**服务包含在**qemu\-kvm 安装包**中, **ksmtuned**服务包含在**qemu\-kvm\-common 安装包**中.

在 RHEL 7.3 上, 如果**不运行 ksm 服务程序**, 则 KSM**默认只会共享 2000 个内存页**, 这样一般很难起到较好的效果.

而在**启动 ksm 服务**后, KSM 能够共享最多达到系统**物理内存一半的内存页**.

**ksm**服务的类型是**一次性**的(**one shot**), 而**ksmtuned 服务**(**forking 类型**)一直保持**循环执行**(**每间隔若干时间**, 见下面配置), 以**调节 ksm**(/sys/kernel/mm/ksm/下各个参数)

```
# systemctl status ksm.service
# systemctl status ksmtuned.service
```

### 2.2.2 ksmtuned 配置文件

ksmtuned 服务配置文件在/etc/ksmtuned.conf 中.

配置文件的**默认内容**如下:

```
[root@kvm-host ~]# cat /etc/ksmtuned.conf
# Configuration file for ksmtuned.

# How long ksmtuned should sleep between tuning adjustments
# 多久运行一次 ksm tune
# KSM_MONITOR_INTERVAL=60

# Millisecond sleep between ksm scans for 16Gb server.
# Smaller servers sleep more, bigger sleep less.
# 会对应设置于/sys/kernel/mm/ksm/sleep_millisecs
# KSM_SLEEP_MSEC=10

# 下面几个参数条件 ksmtuned 内部维护的 npages 变量的变化, 这个动态调整的变量实际设置于/sys/kernel
  /mm/ksm/pages_to_scan

# 当空闲内存小于 threshhold 时, ksm 扫描页(npages)增加这个数值
# KSM_NPAGES_BOOST=300
# 当空闲内存大于 threshhold 时, ksm 扫描页(npages)减小这个数值
# KSM_NPAGES_DECAY=-50
# 无论动态算出来的 npages 值是多少, 都不可以超过下面 min、max 定义的范围. 在笔者的 128G 内存的系统
中, 因为富余内存比较多, 可以看到/sys/kernel/mm/ksm/pages_to_scan 一直被 ksmtuned 服务设置成 1250
# KSM_NPAGES_MIN=64
# KSM_NPAGES_MAX=1250

# 下面两个参数是调整 threshhold 值, 即当 free memory 低于多少时, 触发 ksm 去合并内存

# CONF 参数是一个百分比, 如下面默认的 20%, 表示当空闲内存<系统内存*20%时, 触发 ksm 行为
# KSM_THRES_COEF=20
# 但如果系统内存过小, 按上面算出来的内存小于下面这个 CONST 指定的值, 依然设置成这个 CONST 值
# KSM_THRES_CONST=2048

# uncomment the following if you want ksmtuned debug info
# 打开下面的设置可以 debug ksmtuned 行为
# LOGFILE=/var/log/ksmtuned
# DEBUG=1
```

可以看到, ksmtuned 其实是一个**实时动态调整 ksm**行为的**后台服务**, 笔者理解它的存在是因为前文我们所讲的 KSM 本身有利有弊, 而有了 ksmtuned, 方便用户合理有效地使用 KSM.

### 2.2.3 KSM 实际操作效果对比

下面演示一下 KSM 带来的节省内存的实际效果.

在**物理内存为 128GB**的系统上, 使用了**Linux 3.10**内核的**RHEL 7.3**系统作为**宿主机**, 开始时将**ksm 和 ksmtuned 服务暂停**, "/sys/kernel/mm/ksm/run"的默认值为 0, KSM 不生效.

然后启动**每个内存为 8GB**的 4 个 Windows 10 客户机(**没有安装 virtio\-balloon 驱动**, **没有以 ballooning 方式节省内存**), 启动 ksm 和 ksmtuned 服务, 10 分钟后检查系统内存的使用情况, 以确定 KSM 的效果. 实现该功能的一个示例脚本(ksm\-test.sh)如下:

```sh
#!/bin/bash
# file: ksm-test.sh

echo "----stoping services: ksm and ksmtuned ..."
systemctl stop ksm
systemctl stop ksmtuned

echo "----'free -m -h' command output before starting any guest."
free -m -h

# start 4 Win10 guest
for i in $(seq 1 4)
do
    qemu-system-x86_64 -enable-kvm -m 8G -smp 4 -drive file=./win10.img,format=raw,if=virtio -device virtio-net-pci,netdev=net0 -netdev bridge,br=virbr0,id=net0 -usb -usbdevice tablet -snapshot -name win10_$i -daemonize
    echo "starting the No. ${i} guest..."
    sleep 5
done

echo "----Wait 2 minutes for guests bootup ..."
sleep 120

echo "----'free -m -h' command output with several guests running ."
free -m -h

echo "----starting services: ksm and ksmtuned ..."
systemctl start ksm
systemctl start ksmtuned

echo "----Wait 10 minutes for KSM to take effect ..."
sleep 600

echo "----'free -m -h' command output with ksm and ksmtuned running."
free -m -h
```

执行该脚本, 其命令行输出如下:

```
[root@kvm-host ~]# ./ksm-test.sh
----stoping services: ksm and ksmtuned ...
----'free -m -h' command output before starting any guest.
              total        used        free      shared  buff/cache   available
Mem:           125G        2.9G         82G         84M         40G        121G
Swap:           31G          0B         31G
starting the No. 1 guest...
starting the No. 2 guest...
starting the No. 3 guest...
starting the No. 4 guest...
----Wait 2 minutes for guests bootup ...
----'free -m -h' command output with several guests running .
              total        used        free      shared  buff/cache   available
Mem:           125G         35G         48G         97M         41G         89G
Swap:           31G          0B         31G
----starting services: ksm and ksmtuned ...
----Wait 10 minutes for KSM to take effect ...
----'free -m -h' command output with ksm and ksmtuned running.
              total        used        free      shared  buff/cache   available
Mem:           125G         35G         43G         97M         47G         89G
Swap:           31G          0B         31G
```

在以上输出中, 从 ksm、ksmtuned 服务开始运行之前和之后的"free\-m"命令的"\-/\+buffers/cache: "这一行的输出数值看到:

1)启动**4 个 8GB**的**客户机**以后, 系统的空闲内存从**82GB 减少到 48GB**(相差**34GB**), 符合要求.

2)启动**ksm**和**ksmtuned 服务**以后, 等待 10 分钟, 系统空闲内存反而从**48GB 减少到了 43GB**, 这是为什么呢?KSM 不起作用吗?

不是的, 是因为 ksmtuned 里面**KSM_THRES_COEF**默认设置是**20%**, 系统**空闲内存**还有**48GB**(超过总内存的 35%), 没达到 ksmtuned 要启动 KSM 的地步.

我们从/var/log/ksmtuned 里面可以找到它的 debug 信息(事先要在/etc/ksmtuned.conf 里面**打开 debug**).

```
[root@kvm-host ~]# cat /var/log/ksmtuned
...
Sat Jan 21 14:47:44 CST 2017: committed 0 free 93409576
Sat Jan 21 14:47:44 CST 2017: 26361845 < 131809228 and free > 26361845, stop ksm
Sat Jan 21 14:48:44 CST 2017: committed 0 free 93407360
Sat Jan 21 14:48:44 CST 2017: 26361845 < 131809228 and free > 26361845, stop ksm
```

我们从/sys/kernel/mm/ksm/下面也可以看到, KSM 并**没有 page share**.

```
[root@kvm-host ~]# cat /sys/kernel/mm/ksm/pages_shared
0
```

我们把/etc/ksmtuned.conf 里面 KSM_THRES_COEF 设到 95, 再来做一次这个实验(记得实验前要把 ksm、ksmtuned 服务关掉, 否则下次启动设置不会生效. )

```
[root@kvm-host ~]# ./ksm-test.sh
----stoping services: ksm and ksmtuned ...
----'free -m -h' command output before starting any guest.
              total        used        free      shared  buff/cache   available
Mem:           125G        2.9G         82G         85M         40G        121G
Swap:           31G          0B         31G
starting the No. 1 guest...
starting the No. 2 guest...
starting the No. 3 guest...
starting the No. 4 guest...
----Wait 2 minutes for guests bootup ...
----'free -m -h' command output with several guests running .
              total        used        free      shared  buff/cache   available
Mem:           125G         35G         48G         97M         41G         89G
Swap:           31G          0B         31G
----starting services: ksm and ksmtuned ...
----Wait 10 minutes for KSM to take effect ...
----'free -m -h' command output with ksm and ksmtuned running.
              total        used        free      shared  buff/cache   available
Mem:           125G         18G         60G         97M         47G        106G
Swap:           31G          0B         31G
[root@kvm-host ~]# echo "KSM saved: $(( $(cat /sys/kernel/mm/ksm/pages_sharing) * $(getconf PAGESIZE) / 1024 / 1024 ))MB"
KSM saved: 22261MB
```

这次我们就看到**KSM 起作用**了, 它**大约节省了 22GB 内存**, 从 free \-m \-h 和/sys/kernel/mm/ksm/pages_sharing 都获得了印证.

此时查看"/sys/kernel/mm/ksm/"目录中 KSM 的状态, 如下:

```
[root@kvm-host ~]# cat /sys/kernel/mm/ksm/full_scans
41
[root@kvm-host ~]# cat /sys/kernel/mm/ksm/pages_shared
607746
[root@kvm-host ~]# cat /sys/kernel/mm/ksm/pages_sharing
5767606
[root@kvm-host ~]# cat /sys/kernel/mm/ksm/pages_to_scan
1250
[root@kvm-host ~]# cat /sys/kernel/mm/ksm/pages_unshared
1257693
[root@kvm-host ~]# cat /sys/kernel/mm/ksm/run
1
[root@kvm-host ~]# cat /sys/kernel/mm/ksm/sleep_millisecs
10
```

可见, KSM 已经为系统提供了不少的共享内存页, 而当前 KSM 的运行标志(run)设置为 1. 当然, 查看到的 run 值也可能为 0, 因为 ksm 和 ksmtuned 这两个服务会根据系统状况, 按照既定的规则来修改"/sys/kernel/mm/ksm/run"文件中的值, 从而调节 KSM 的运行.

# 3 QEMU 对 KSM 的控制

由 7.3.1 节的代码分析我们可以看到, QEMU 是通过**madvise 系统调用**告诉内核本进程的内存**可以被合并**.

在相关的代码中, 我们发现 QEMU 内部是有个**开关控制**的, 它开关与否, 就是通过\-**machine**(或者\-M 缩写)参数的**mem\-merge=on/off**来指定的. 默认是 on, 也就是允许内存合并.

我们还是通过上面的实验来说明它的效果. 我们把上面的 ksm\-test.sh 脚本稍微改一下, 将当中的启动客户机的命令改成如下:

```
qemu-system-x86_64 -enable-kvm -m 8G -smp 4 -drive file=./win10.img,format=raw, if=virtio -device virtio-net-pci,netdev=net0 -netdev bridge,br=virbr0,id=net0,mac=52:54:00:6a:8b:9$i -usb -usbdevice tablet -snapshot -name win10_$i -M q35,mem-merge=off -daemonize
```

重复那个实验, 我们看到的输出如下:

```
----stoping services: ksm and ksmtuned ...
----'free -m -h' command output before starting any guest.
              total        used        free      shared  buff/cache   available
Mem:           125G        2.9G         78G        109M         44G        121G
Swap:           31G          0B         31G
starting the No. 1 guest...
starting the No. 2 guest...
starting the No. 3 guest...
starting the No. 4 guest...
----Wait 5 minutes for guests bootup ...
----'free -m -h' command output with several guests running .
              total        used        free      shared  buff/cache   available
Mem:           125G         35G         45G        121M         45G         89G
Swap:           31G          0B         31G
----starting services: ksm and ksmtuned ...
----Wait 10 minutes for KSM to take effect ...
----'free -m -h' command output with ksm and ksmtuned running.
              total        used        free      shared  buff/cache   available
Mem:           125G         35G         44G        121M         45G         89G
Swap:           31G          0B         31G
[root@kvm-host ~]# echo "KSM saved: $(( $(cat /sys/kernel/mm/ksm/pages_sharing) * $(getconf PAGESIZE) / 1024 / 1024 ))MB"
KSM saved: 0MB
```

可以看到, **启动客户机**之后, 再**启动 ksm 和 ksmtuned**服务之后, 系统的 free 内存并**没有增加**. 通过/sys/kernel/mm/ksm/**pages_shared**、**page_sharing**等参数也发现, KSM**共享内存为 0**.

而通过下面检查/var/log/ksmtuned 的 debug 输出, 我们可以看到, KSM 其实是尝试合并的, 但**没有发现可以合并的内存页**.

```
[root@kvm-host ~]# cat /var/log/ksmtuned
...
Mon Jan 23 20:12:15 CST 2017: committed 0 free 93735500
Mon Jan 23 20:12:15 CST 2017: 125218766 > 131809228, start ksm
Mon Jan 23 20:12:15 CST 2017: 93735500 < 125218766, boost
Mon Jan 23 20:12:15 CST 2017: KSMCTL start 1250 10
Mon Jan 23 20:13:15 CST 2017: committed 0 free 93713948
Mon Jan 23 20:13:15 CST 2017: 125218766 > 131809228, start ksm
Mon Jan 23 20:13:15 CST 2017: 93713948 < 125218766, boost
Mon Jan 23 20:13:15 CST 2017: KSMCTL start 1250 10
```

如果我们将实验脚本 ksm\-test.sh 调整为两个客户机**mem\-merge=on**, 两个 mem\-merge=off, 实验结果则是**节省 13G 左右**.

```
----Wait 2 minutes for guests bootup ...
----'free -m -h' command output with several guests running .
              total        used        free      shared  buff/cache   available
Mem:           125G         35G         44G        129M         45G         89G
Swap:           31G          0B         31G
----starting services: ksm and ksmtuned ...
----Wait 3 minutes for KSM to take effect ...
----'free -m -h' command output with ksm and ksmtuned running.
              total        used        free      shared  buff/cache   available
Mem:           125G         22G         57G        129M         45G        102G
Swap:           31G          0B         31G
[root@kvm-host ~]# echo "KSM saved: $(( $(cat /sys/kernel/mm/ksm/pages_sharing) * $(getconf PAGESIZE) / 1024 / 1024 ))MB"
KSM saved: 14033MB
```

我们在 win10_3 的 QEMU 进程(23710)中可以看到类似这样的 VMAmapping, 显示是被 shared.

```
[root@kvm-host ~]# cat /proc/23710/smaps
......
7f757fe00000-7f777fe00000 rw-p 00000000 00:00 0
Size:            8388608 kB
Rss:             8388608 kB
Pss:             2275612 kB
Shared_Clean:          0 kB
Shared_Dirty:    6964916 kB
Private_Clean:         0 kB
Private_Dirty:   1423692 kB
Referenced:      8388608 kB
Anonymous:       8388608 kB
AnonHugePages:     18432 kB
Swap:                  0 kB
KernelPageSize:        4 kB
MMUPageSize:           4 kB
Locked:                0 kB
VmFlags: rd wr mr mw me dc ac sd hg mg
......
```

对比 win10_1 的 QEMU 进程(23666), 类似的这个 VMA, 是没有 shared.

```
[root@kvm-host ~]# cat /proc/23666/smaps
......
7fca7be00000-7fcc7be00000 rw-p 00000000 00:00 0
Size:            8388608 kB
Rss:             8388608 kB
Pss:             8388608 kB
Shared_Clean:          0 kB
Shared_Dirty:          0 kB
Private_Clean:         0 kB
Private_Dirty:   8388608 kB
Referenced:      8388608 kB
Anonymous:       8388608 kB
AnonHugePages:   8388608 kB
Swap:                  0 kB
KernelPageSize:        4 kB
MMUPageSize:           4 kB
Locked:                0 kB
VmFlags: rd wr mr mw me dc ac sd hg
......
```

QEMU 的\-machine q35(或者 pc)、mem\-merge 参数, 提供给用户精细化的控制: 可以只针对某个或某些客户机, 开启或关闭 KSM.
