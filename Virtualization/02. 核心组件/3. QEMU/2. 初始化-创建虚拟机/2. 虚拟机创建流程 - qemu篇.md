
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 从 libvirt 到 qemu](#1-从-libvirt-到-qemu)
- [2. qemu+kvm 虚拟化原理](#2-qemukvm-虚拟化原理)
  - [2.1. vcpu 运行](#21-vcpu-运行)
  - [2.2. 内存访问](#22-内存访问)
  - [2.3. 影子页表](#23-影子页表)
  - [2.4. EPT](#24-ept)
- [3. 代码流程分析](#3-代码流程分析)
  - [3.1. 初始化 QOM 设备](#31-初始化-qom-设备)
  - [3.2. 初始化命令行参数数据结构](#32-初始化命令行参数数据结构)
  - [3.3. 初始化虚拟机状态](#33-初始化虚拟机状态)
  - [3.4. 忽略早期 pipe 信号](#34-忽略早期-pipe-信号)
  - [3.5. 初始化芯片组信息](#35-初始化芯片组信息)
  - [3.6. 获取默认芯片组型号](#36-获取默认芯片组型号)
  - [3.7. 初始化 block](#37-初始化-block)
  - [3.8. 判断是否使用默认配置](#38-判断是否使用默认配置)
  - [3.9. 解析命令行参数](#39-解析命令行参数)
  - [3.10. 初始化 main_loop](#310-初始化-main_loop)
    - [3.10.1. 初始化时钟](#3101-初始化时钟)
    - [3.10.2. 注册信号处理函数](#3102-注册信号处理函数)
    - [3.10.3. 为 fd 申请内存](#3103-为-fd-申请内存)
  - [3.11. cpu_exec_init_all](#311-cpu_exec_init_all)
  - [3.12. 设置硬件版本](#312-设置硬件版本)
  - [3.13. 初始化支持的 cpu feature](#313-初始化支持的-cpu-feature)
  - [3.14. data_dir_idx](#314-data_dir_idx)
  - [3.15. smp 参数解析](#315-smp-参数解析)
  - [3.16. 是否配置默认串口, 井口等](#316-是否配置默认串口-井口等)
  - [3.17. 初始化所有 char dev](#317-初始化所有-char-dev)
  - [3.18. 打印 device help 日志](#318-打印-device-help-日志)
  - [3.19. 设置 current_machine](#319-设置-current_machine)
  - [3.20. 初始化虚拟化加速器](#320-初始化虚拟化加速器)
    - [3.20.1. kvm 加速初始化(创建虚拟机等 ioctl)](#3201-kvm-加速初始化创建虚拟机等-ioctl)
      - [3.20.1.1. 初始化 KVMState](#32011-初始化-kvmstate)
      - [3.20.1.2. 版本协商](#32012-版本协商)
      - [3.20.1.3. 最大内存插槽数目](#32013-最大内存插槽数目)
      - [3.20.1.4. 最大 vcpu 数(soft)](#32014-最大-vcpu-数soft)
      - [3.20.1.5. 最大 vcpu 数量(hard)](#32015-最大-vcpu-数量hard)
      - [3.20.1.6. 初始化虚拟机 kvm 结构体](#32016-初始化虚拟机-kvm-结构体)
      - [3.20.1.7. 检查 kvm 中的 qemu capability](#32017-检查-kvm-中的-qemu-capability)
      - [3.20.1.8. 初始化硬件架构相关特性](#32018-初始化硬件架构相关特性)
      - [3.20.1.9. 创建中断管理单元](#32019-创建中断管理单元)
      - [3.20.1.10. 注册 kvm_memory_listener](#320110-注册-kvm_memory_listener)
  - [3.21. 设置无硬盘启动相关参数](#321-设置无硬盘启动相关参数)
  - [3.22. 设置标准输出缓冲区](#322-设置标准输出缓冲区)
  - [3.23. 初始化 vcpu 相关的锁, 信号量](#323-初始化-vcpu-相关的锁-信号量)
  - [3.24. 初始化网络设备](#324-初始化网络设备)
  - [3.25. 磁盘设备初始化](#325-磁盘设备初始化)
  - [3.26. 创建 qemu monitor](#326-创建-qemu-monitor)
  - [3.27. 初始化主板](#327-初始化主板)
  - [3.28. 前端设备初始化](#328-前端设备初始化)
  - [3.29. 加载设备 rom](#329-加载设备-rom)
  - [3.30. main_loop](#330-main_loop)
- [4. 余下工作](#4-余下工作)
- [5. 参考](#5-参考)

<!-- /code_chunk_output -->

在 kvm+qemu 架构下, qemu 负责模拟虚拟机所有的硬件设备, 并与 kvm 交互.

qemu 是云计算中虚拟化的最终执行者, 通过 openstack, libvirt 等封装的各种设备配置都需要 qemu 模拟并运行.

本文会通过解析 qemu 在虚拟机创建过程中的流程来向大家介绍一下 qemu 的大致工作流程及其工作原理.

# 1. 从 libvirt 到 qemu

在上一篇中我们分析了 libvirt 中创建虚拟机的流程, 在最后阶段 libvirt 组装了 qemu command, 并通过 fork 调用拉起 qemu 进程, 在宿主机上可以看到这样一个进程:

```bash
/usr/bin/qemu-system-x86_64
-name guest=instance-000439eb,debug-threads=on
-S
-machine pc-i440fx-2.5,accel=kvm,usb=off
-cpu IvyBridge,+ds,+acpi,+ss,+ht,+tm,+pbe,+dtes64,+monitor,+ds_cpl,+vmx,+smx,+est,+tm2,+xtpr,+pdcm,+pcid,+dca,+osxsave,+pdpe1gb
-m size=1048576k,slots=64,maxmem=268435456k
-realtime mlock=off
-smp 1,maxcpus=64,sockets=64,cores=1,threads=1
-numa node,nodeid=0,cpus=0-63,mem=1024
-uuid 2178112f-1e08-4a0b-b495-3bcc8faf3d59
-smbios type=1,manufacturer=OpenStack Foundation,product=OpenStack Nova,version=2013.2-netease.910,serial=44454c4c-3900-1057-8032-b6c04f373232,uuid=2178112f-1e08-4a0b-b495-3bcc8faf3d59

-drive file=rbd:vms/2178112f-1e08-4a0b-b495-3bcc8faf3d59_disk:auth_supported=none:mon_host=10.180.0.47\:6789\;10.180.0.48\:6789\;10.180.0.49\:6789,format=raw,if=none,id=drive-virtio-disk0,cache=none
-device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1

-drive file=rbd:vms/2178112f-1e08-4a0b-b495-3bcc8faf3d59_disk.config:auth_supported=none:mon_host=10.180.0.47\:6789\;10.180.0.48\:6789\;10.180.0.49\:6789,format=raw,if=none,id=drive-virtio-disk25,readonly=on,cache=none
-device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk25,id=virtio-disk25

-netdev tap,fd=179,id=hostnet0,vhost=on,vhostfd=183
-device virtio-net-pci,netdev=hostnet0,id=net0,mac=fa:16:3e:38:e9:53,bus=pci.0,addr=0x3
-chardev file,id=charserial0,path=/data/nova/instances/2178112f-1e08-4a0b-b495-3bcc8faf3d59/console.log
-device isa-serial,chardev=charserial0,id=serial0
-chardev pty,id=charserial1 -device isa-serial,chardev=charserial1,id=serial1
-chardev socket,id=charchannel0,path=/var/lib/libvirt/qemu/org.qemu.guest_agent.0.instance-000439eb.sock,server,nowait
-device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charchannel0,id=channel0,name=org.qemu.guest_agent.0
-device usb-tablet,id=input0
-vnc 10.180.0.47:64,password -k en-us
-device cirrus-vga,id=video0,bus=pci.0,addr=0x2
-device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x7
-msg timestamp=on
-no-user-config
-nodefaults
-chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-408-instance-000439eb/monitor.sock,server,nowait
-mon chardev=charmonitor,id=monitor,mode=control
-rtc base=utc,driftfix=slew
-global kvm-pit.lost_tick_policy=discard
-no-shutdown
-boot strict=on
-device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2
-device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x4
```

可以看到, qemu 进程的可执行文件是**qemu-system-x86_64**, 该文件是 x86_64 架构的模拟器.

# 2. qemu+kvm 虚拟化原理

在 qemu kvm 架构下, qemu 负责各种设备的模拟, kvm 则负责保障虚拟机中的代码可以正常执行.

具体来说, kvm 暴露一个设备文件接口/dev/kvm 给用户态的 qemu 进程. 而 qemu 进程通过系统调用 ioctl 操作 kvm 接口, 完成一些需要真实硬件参与的虚拟机操作.

## 2.1. vcpu 运行

现在使用的 x86 架构的虚拟化技术利用了 intel 的 VT\-x 技术. vt\-x 的基本思想是区分 cpu 的工作模式, root 和非 root 模式. 每一种模式又分为 0-3 四个特权级. 在虚拟机中 cpu 运行在非 root 模式下, 当执行敏感指令时, cpu 会自动从非 root 模式切换到 root 模式, 称为 vm-exit. 对应的, VMM 也会发起从 root 模式到非 root 模式的切换, 称为 vm-entry. VT-x 还引入了 VMCS 的概念, 用户保存 cpu 在各种模式下的运行状态, 方便 cpu 在多种模式下的切换.

**VMCS**在系统中存储在一块最大**不超过 4kB 大小的内存**中, 内容包括

* **VMCS 版本号**,
* **VMCS 中止标识**
* 以及**VMCS 数据域**.

在数据域中包括如下各种信息:

* **客户机状态域**: 在虚拟机内运行时, 即非 root 模式下 CPU 的状态. vm-exit 发生时, cpu 当前的状态会存储到客户机状态域. vm-entry 发生时, 从客户机状态域恢复 cpu 状态.
* **宿主机状态域**: 在 VMM 运行时, 即 root 模式下 CPU 的状态. vm-exit 发生时, cpu 从这里恢复 cpu 运行状态.
* **vm\-entry 控制域**: 控制 vm\-entry 的过程
* **vm\-execution 控制域**: 控制非根模式下的行为
* **vm\-exit 控制域**: 控制 vm\-exit 的过程
* **vm\-exit 信息域**: 提供 vm\-exit 的原因和其他信息, 只读域.

## 2.2. 内存访问

在虚拟化场景下, 虚拟机内部如果需要访问一段内存, 需要经过两步映射才能找到真正的物理地址:  Guest 虚拟机地址(GVA)->Guest 物理地址(GPA)->宿主机虚拟地址(HVA)->宿主机物理地址(HPA)

## 2.3. 影子页表

在 hypervisor 中维护一张**内存影子页表**, 根据 GVA-GPA-HVA-HPA 的映射关系直接计算**GVA-HPA**的映射关系, 并将对应的映射关系写入影子页表. 这样可以解决虚拟机内存访问的问题, 但是依赖软件实现的影子页表也带来了很多问题. 像各种页表之间的同步问题, 页表本身的内存开销等.

## 2.4. EPT

EPT 页表利用硬件实现了从 GPA 到 HPA 的映射, **每个虚拟机**只需要**维护一个 EPT 页表**即可. 减少了开销, 提高了性能.

# 3. 代码流程分析

qemu command 命令运行之后, 首先进入 qemu 进程的入口--vl.c 文件的 main 函数中.

main 函数大致执行流程如下:

* 初始化各种设备的初始化入口函数
* 解析 qemu command 传入的各种参数
* 初始化加速器(与 kmod 交互)
* 初始化后端设备
* 初始化芯片组(vcpu, 内存, 南桥, 北桥, bios)
* 进入 main\_loop

下面对 main 函数做具体的分析:

## 3.1. 初始化 QOM 设备

```cpp
module_call_init(MODULE_INIT_QOM);
```

打开这个函数的代码可以看到如下的内容, 看起来非常简单:

```cpp
void module_call_init(module_init_type type)
{
    ModuleTypeList *l;
    ModuleEntry *e;

    module_load(type);
    l = find_type(type);

    QTAILQ_FOREACH(e, l, node) {
        e->init();
    }
}
```

这个函数实现的功能就是执行 ModuleTypeList 类型的链表中每一个节点的 init 函数. 可是我们现在是在一个二进制文件的入口 main 函数中, 并没有初始化这样一个链表么~那么这个链表中的内容是怎么来的呢. 通过搜索代码我们可以看到很多设备文件中最后都会调用一个**type\_init 函数**:

```cpp
// include/qemu/module.h
type_init(virtio_register_types)

// include/qemu/module.h
#define type_init(function) module_init(function, MODULE_INIT_QOM)

// include/qemu/module.h
/* This should not be used directly.  Use block_init etc. instead.  */
#define module_init(function, type)                                         \
static void __attribute__((constructor)) do_qemu_init_ ## function(void)    \
{                                                                           \
    register_module_init(function, type);                                   \
}

// util/module.c
void register_module_init(void (*fn)(void), module_init_type type)
{
    ModuleEntry *e;
    ModuleTypeList *l;

    e = g_malloc0(sizeof(*e));
    e->init = fn;
    e->type = type;

    l = find_type(type);

    QTAILQ_INSERT_TAIL(l, e, node);
}

// util/module.c
static ModuleTypeList *find_type(module_init_type type)
{
    ModuleTypeList *l;

    init_lists();

    l = &init_type_list[type];

    return l;
}

// util/module.c
static ModuleTypeList init_type_list[MODULE_INIT_MAX];

// include/qemu/module.h
typedef enum {
    MODULE_INIT_BLOCK,
    MODULE_INIT_MACHINE,
    MODULE_INIT_QAPI,
    MODULE_INIT_QOM,
    MODULE_INIT_MAX
} module_init_type;
```

这些代码我们需要按照如下的顺序来看:

* qemu 定义**各种设备类型**
* 初始化一个**ModuleTypeList 类型**的**链表数组**
* 通过 find\_type 函数可以获取**指定设备类型**的**列表**.
* **register\_module\_init**中把参数**指定的设备**加入到其所属**设备类型的链表**中
* 把上面的函数**封装到一个函数**中, 并且这个函数添加了 gcc 的 attribute: \_\_**attribute**\_\_((constructor)),其含义是在**整个程序**的**main 函数执行之前**该函数就会被执行.

至此我们就可以看到, 在**main 函数**开始**执行之前**, **init\_type\_list 链表**就已经**初始化完成**. 因此上面的 module\_call\_init(MODULE\_INIT\_QOM);就可以遍历**所有的 QOM 设备**并执行他们的 init 函数, 以 virtio\-blk 函数为例, init 的执行内容如下. 就是注册一下当前的设备类型, 设备总线, 以及其它一些相关的初始化函数.

```cpp
// hw/virtio/virtio.c
static const TypeInfo virtio_device_info = {
    .name = TYPE_VIRTIO_BLK,
    .parent = TYPE_VIRTIO_DEVICE,
    .instance_size = sizeof(VirtIOBlock),
    .instance_init = virtio_blk_instance_init,
    .class_init = virtio_blk_class_init,ß
};

static void virtio_register_types(void)
{
    type_register_static(&virtio_device_info);
}

// 会直接调用, 直到 register_module_init
type_init(virtio_register_types)
```

QOM 到底指的是什么设备呢?QOM 即**qemu object model**, qemu 设备模型, 是一种**qemu 设备模拟的规范**. 目前基本上**所有 qemu 支持的设备**都**使用这种规范**. 我们可以看到在初始化的链表数组中还有**其它类型的设备**, 后面会涉及到.

同理`MODULE_INIT_BLOCK`类型设备链表是通过`block_init()`调用的

## 3.2. 初始化命令行参数数据结构

初始化 qemu 记录**命令行参数的数据结构**, 等待下面解析 qemu command. 这里为所有可能的 qemu command 参数准备了存储结构.

```cpp
    qemu_add_opts(&qemu_drive_opts);
    qemu_add_drive_opts(&qemu_legacy_drive_opts);
    qemu_add_drive_opts(&qemu_common_drive_opts);
    qemu_add_drive_opts(&qemu_drive_opts);
    qemu_add_opts(&qemu_chardev_opts);
    qemu_add_opts(&qemu_device_opts);
    qemu_add_opts(&qemu_netdev_opts);
    qemu_add_opts(&qemu_net_opts);
    qemu_add_opts(&qemu_rtc_opts);
    qemu_add_opts(&qemu_global_opts);
    qemu_add_opts(&qemu_mon_opts);
    qemu_add_opts(&qemu_trace_opts);
    qemu_add_opts(&qemu_option_rom_opts);
    qemu_add_opts(&qemu_machine_opts);
    qemu_add_opts(&qemu_mem_opts);
    qemu_add_opts(&qemu_smp_opts);
    qemu_add_opts(&qemu_boot_opts);
    qemu_add_opts(&qemu_sandbox_opts);
    qemu_add_opts(&qemu_add_fd_opts);
    qemu_add_opts(&qemu_object_opts);
    qemu_add_opts(&qemu_tpmdev_opts);
    qemu_add_opts(&qemu_realtime_opts);
    qemu_add_opts(&qemu_msg_opts);
    qemu_add_opts(&qemu_name_opts);
    qemu_add_opts(&qemu_numa_opts);
```

## 3.3. 初始化虚拟机状态

初始化 qemu 管理的**虚拟机运行状态**

```cpp
#qemu 中通过一个二维数组记录虚拟机的状态变化, 这个二维数组中记录了所有可能的状态变化, 第一维表示初始状态, 第二维表示目标状态.
runstate_init();
```

## 3.4. 忽略早期 pipe 信号

```cpp
os_setup_early_signal_handling();
```

## 3.5. 初始化芯片组信息

初始化**芯片组入口信息**, 使用的仍然是第一步中已经分析过的 module\_init 方式. 但是这里指定的初始化类型是`MODULE_INIT_MACHINE`

```cpp
module_call_init(MODULE_INIT_MACHINE);
```

我们现在使用的**默认主板类型**是`pc-i440fx-2.5`, 通过代码搜索我们可以直接找到`pc_piix.c`文件, 这个文件就是用于**模拟 Intel piix 系列芯片组**的. 在这个文件的最后通过`module_init`在**main 函数执行之前**注册链表中的初始化函数. 在 main 函数执行到初始化 machine 的时候, 会**注册**qemu 支持的**所有 Intel piix 芯片组**的**初始化入口**. 这里使用的代码版本比较低, 还没有支持我们使用的 i440fx-2.5 版本的芯片组. 我们主要是分析逻辑, 具体的版本差异就先不考虑了.

```cpp
static void pc_machine_init(void)
{
    qemu_register_pc_machine(&pc_i440fx_machine_v2_1);
    qemu_register_pc_machine(&pc_i440fx_machine_v2_0);
    qemu_register_pc_machine(&pc_i440fx_machine_v1_7);
    qemu_register_pc_machine(&pc_i440fx_machine_v1_6);
    qemu_register_pc_machine(&pc_i440fx_machine_v1_5);
    qemu_register_pc_machine(&pc_i440fx_machine_v1_4);
    qemu_register_pc_machine(&pc_machine_v1_3);
    qemu_register_pc_machine(&pc_machine_v1_2);
    qemu_register_pc_machine(&pc_machine_v1_1);
    qemu_register_pc_machine(&pc_machine_v1_0);
    qemu_register_pc_machine(&pc_machine_v0_15);
    qemu_register_pc_machine(&pc_machine_v0_14);
    qemu_register_pc_machine(&pc_machine_v0_13);
    qemu_register_pc_machine(&pc_machine_v0_12);
    qemu_register_pc_machine(&pc_machine_v0_11);
    qemu_register_pc_machine(&pc_machine_v0_10);
    qemu_register_pc_machine(&isapc_machine);
#ifdef CONFIG_XEN
    qemu_register_pc_machine(&xenfv_machine);
#endif
}

// 调用
machine_init(pc_machine_init);
```

## 3.6. 获取默认芯片组型号

获取**当前 arch**下的**默认芯片组型号**. qemu 本身支持多种 arch, 在初始化时根据执行的二进制文件**只会初始化某一个 arch**. 而**每一个 arch**中都会有一个具体的型号作为**默认的芯片组型号**, 一般都是当前支持的最新版本.

```cpp
machine_class = find_default_machine();
```

## 3.7. 初始化 block

初始化**block driver 入口**. 使用的仍然是`module_init`方式. 这里的 block driver 即我们在使用**file disk**的时候指定的**各种 driver 类型**, 如 qcow2, raw 等.

```cpp
bdrv_init_with_whitelist();
```

## 3.8. 判断是否使用默认配置

以上 qemu 已经执行了 6 个关键步骤, 但都是一些**基本的初始化操作**, 在**物理节点**上**每一个虚拟机**启动都会执行**完全一样的操作**. 而区分**不同虚拟机**的**qemu command 参数**到这里为止还没有解析. 接下来会先遍历一遍 qemu command 中的参数, 根据参数确定是否使用预先配置在/**etc/qemu/target-{arch}.conf**文件中的**配置参数**.

```cpp
    if (defconfig) {
        int ret;
        ret = qemu_read_default_config_files(userconfig);
        if (ret < 0) {
            exit(1);
        }
    }
```

## 3.9. 解析命令行参数

接下来真正解析 qemu command 中配置的各种参数 通过一个 for 循环, **解析结果**放入`vm_config_groups`, 这个很关键

```cpp
// util/qemu-config.c
static QemuOptsList *vm_config_groups[48];
```

## 3.10. 初始化 main_loop

初始化 main loop, 还没真正进入 main loop

```cpp
if(qemu_init_main_loop(&main_loop_err)){
    error_report_err(main_loop_err);
    exit(1);
}
```

在`qemu_init_main_loop`中

### 3.10.1. 初始化时钟

初始化 main\_loop 中使用的时钟.

```cpp
init_clocks();
```

在当前的架构下, qemu 中需要维护三种时间:

* `QEMU_CLOCK_REALTIME` RTC
* `QEMU_CLOCK_VIRTUAL` 虚拟机运行时间
* `QEMU_CLOCK_HOST` 宿主机时间

### 3.10.2. 注册信号处理函数

注册 qemu 进程**信号量处理函数**, **qemu**收到的**进程信号**会触发注册的`sigfd_handler`回调函数

```cpp
qemu_signal_init();
```

### 3.10.3. 为 fd 申请内存

为 main\_loop 监听的**fd**申请**管理内存**

```cpp
gpollfds = g_array_new(FALSE, FALSE, sizeof(GPollFD));
```

## 3.11. cpu_exec_init_all

遗留

* `memory_map_init` qemu 进程设备模拟占用的内存申请及初始化
* `io\_mem\_init` io rom 内存空间申请及初始化

## 3.12. 设置硬件版本

```cpp
    if (machine_class->hw_version) {
        qemu_set_version(machine_class->hw_version);
    }
```

## 3.13. 初始化支持的 cpu feature

**cpudef\_init**初始化**支持的 cpu feature**, 可以通过如下命令查询当前 qemu 支持的 cpu feature

```cpp
# /usr/bin/qemu-system-x86_64 -cpu help
    cpudef_init();
    if (cpu_model && is_help_option(cpu_model)) {
        list_cpus(stdout, &fprintf, cpu_model);
        exit(0);
    }
```

## 3.14. data_dir_idx

遗留 怀疑是 bios 文件路径

## 3.15. smp 参数解析

解析\-smp 参数记录到全局变量中

```cpp
smp_parse(qemu_opts_find(qemu_find_opts("smp-opts"), NULL));

machine_class->max_cpus = machine_class->max_cpus ?: 1; /* Default to UP */#未配置 cpu 情况下 默认配置一个

#校验参数合法性
if (smp_cpus > machine_class->max_cpus) {
    fprintf(stderr, "Number of SMP cpus requested (%d), exceeds max cpus "
            "supported by machine `%s' (%d)\n", smp_cpus,
            machine_class->name, machine_class->max_cpus);
    exit(1);
}
```

## 3.16. 是否配置默认串口, 井口等

根据 machine 类型判断是否配置默认的串口, 并口等设备. 如果需要则创建默认的设备配置

```cpp
if (machine_class->default_machine_opts){
    qemu_opts_set_defaults(qemu_find_opts("machine"),
                            machine_class->default_machine_opts, 0);
}
```

## 3.17. 初始化所有 char dev

初始化所有的 char dev(pty\-\-serial/socket\-\-qga)

```cpp
    if (qemu_opts_foreach(qemu_find_opts("chardev"), chardev_init_func, NULL, 1) != 0)
        exit(1);
```

## 3.18. 打印 device help 日志

```cpp
    if (qemu_opts_foreach(qemu_find_opts("device"), device_help_func, NULL, 0)
        != 0) {
        exit(0);
    }
```

## 3.19. 设置 current_machine

从 qemu command 中获取设置的 machine 相关参数, 并赋值给 current\_machine

```cpp
    machine_opts = qemu_get_machine_opts();
    if (qemu_opt_foreach(machine_opts, object_set_property, current_machine,
                         1) < 0) {
        object_unref(OBJECT(current_machine));
        exit(1);
    }
```

## 3.20. 初始化虚拟化加速器

```
-machine pc-i440fx-rhel7.3.0,accel=kvm,usb=off,dump-guest-core=off
```
初始化虚拟化加速器 configure_accelerator.

```cpp
configure_accelerator(current_machine);
```

这里的作用其实就是配置一些**qemu 与 hypervisor 层**的**交互接口**.

**qemu**通过一些**句柄**以**ioctl 的方式**与**kmod**交互, 完成虚拟化相关的操作. 在 qemu 中维护的句柄包括:

* vmfd: 虚拟机相关操作句柄, 通过该句柄读写的都是与**虚拟机相关**的**信息**.
* devkvmfd: qemu 与 kmod 交互的句柄, 负责读取**kmod 中的公共信息**.
* vcpufd: 虚拟 CPU 句柄, **每个 vcpu**会分配一个句柄用于与 kmod 交互.

在 qemu 中维护了一个**结构体数组**, 用于记录**各种虚拟化方案**下的**加速器初始化入口**:

```cpp
accel_list[] = {
    { "tcg", "tcg", tcg_available, tcg_init, &tcg_allowed },
    { "xen", "Xen", xen_available, xen_init, &xen_allowed },
    { "kvm", "KVM", kvm_available, kvm_init, &kvm_allowed },
    { "qtest", "QTest", qtest_available, qtest_init, &qtest_allowed },
};
```

### 3.20.1. kvm 加速初始化(创建虚拟机等 ioctl)

该初始化是在`kvm-all.c`文件中

```cpp
type_init(kvm_type_init);
```

```cpp
// accel/kvm/kvm-all.c
static int kvm_init(MachineState *ms)
```

从上面的映射关系中可以看到, 我们当前的配置下使用 kvm\_init 作为初始化入口.

#### 3.20.1.1. 初始化 KVMState

* 为虚拟机初始化一个 KVMState *s, 用于保存 vmfd, devkvmfd, 中断路由等与 kvm 交互的信息

```cpp
KVMState *s = KVM_STATE(ms->acelerator)
```

#### 3.20.1.2. 版本协商

* 版本协商: devkvmfd

```cpp
ret = kvm_ioctl(s, KVM_GET_API_VERSION, 0);
```

#### 3.20.1.3. 最大内存插槽数目

* 支持的最大内存插槽数量: devkvmfd

```cpp
s->nr_slots = kvm_check_extension(s, KVM_CAP_NR_MEMSLOTS);
```

调用了

```cpp
ret = kvm_ioctl(s, KVM_CHECK_EXTENSION, extension);
```

#### 3.20.1.4. 最大 vcpu 数(soft)

* 获取 kvm 建议的每个虚拟机支持的最大 vcpu 数量(soft): devkvmfd

```cpp
int ret = kvm_check_extension(s, KVM_CAP_NR_VCPUS);
```

#### 3.20.1.5. 最大 vcpu 数量(hard)

* 获取每个虚拟机支持的最大 vcpu 数量(hard): devkvmfd

```cpp
int ret = kvm_check_extension(s, KVM_CAP_MAX_VCPUS);
```

#### 3.20.1.6. 初始化虚拟机 kvm 结构体

* 校验传入的 vcpu 参数 smp/maxvcpus
* 没用到**kvm\_type**, 貌似在 powerpc 里面会用到这个参数
* 在**kmod**中初始化一个与虚拟机一一对应的**kvm 结构体**, 用于保存 qemu 与 kvm 交互的状态. 返回**qemu 的 vmfd**

```cpp
ret = kvm_ioctl(s, KVM_CREATE_VM, type);

linux kernel
static long kvm_dev_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)

static int kvm_dev_ioctl_create_vm(unsigned long type)
kvm = kvm_create_vm(type);
```

#### 3.20.1.7. 检查 kvm 中的 qemu capability

* 检查**qemu capability**在 kvm 中是否支持

```cpp
missing_cap = kvm_check_extension_list(s, kvm_required_capabilites);
```

#### 3.20.1.8. 初始化硬件架构相关特性

* 在 `kvm_arch_init` 中初始化硬件架构相关的一些特性, 比如 e820 表, 中断路由表等

`target/i386/kvm.c`

检查**kvm 是否支持 MSR**(model specific register, 用于标识 cpu 的工作环境和工作状态), 通常为**一组寄存器**, 分别表示不同的标志位. 其中**一些**要求**必须存在**, 否则无法正常启动.

```cpp
ret = kvm_ioctl(s, KVM_GET_MSR_INDEX_LIST, &msr_list);
```

初始化 e820 entry, 以及 e820 table. (e820 表用于维护机器的物理内存布局)

#### 3.20.1.9. 创建中断管理单元

创建中断管理单元

```cpp
kvm_vm_ioctl(s, KVM_CREATE_IRQCHIP);
```

通过 vmfd 在 kmod 中创建一个虚拟 pic

初始化**中断路由表**(irqrouting 中断路由, 与中断亲和性等相关. **kvm**通过该表可以知道将**某一个中断**路由到**哪一个具体的 vcpu**上处理)

#### 3.20.1.10. 注册 kvm_memory_listener



## 3.21. 设置无硬盘启动相关参数

```cpp
    machine_opts = qemu_get_machine_opts();
    kernel_filename = qemu_opt_get(machine_opts, "kernel");
    initrd_filename = qemu_opt_get(machine_opts, "initrd");
    kernel_cmdline = qemu_opt_get(machine_opts, "append");
    bios_name = qemu_opt_get(machine_opts, "firmware");
```

## 3.22. 设置标准输出缓冲区

```cpp
os_set_line_buffering();
```

## 3.23. 初始化 vcpu 相关的锁, 信号量

```cpp
qemu_init_cpu_loop();
```

## 3.24. 初始化网络设备

完成**后端 tap 设备**的初始化, 与宿主机 kernel 交互, 拉起**vhost 内核线程**, 完成**vhost 的初始化**.

**正常**情况下, **虚拟机内部网卡收发数据**会通过**vring**和**内存共享**. 首先走一遍**虚拟机内核**的**网络栈**将数据包放入**共享内存**, 通过**vring**通知**后端网络设备**拷贝共享内存, 因为**后端网络设备**是在**用户态**的, 因此又要重新走一次**宿主机的内核网络协议栈**. 这种流程会导致**网络 IO 性能较差**, 因此引入了 vhost 的概念.

vhost 是一个**内核中的线程**, 会映射**qemu**中的**共享内存页**. 当虚拟机内部发出网络包的时候, 从**后端共享内存**直接映射到了**host 内核**, 跳过 qemu 处理环节, 节省了处理时间, 提升了性能.

```cpp
int net_init_clients(void);
net_init_netdev
net_client_init1
net_init_tap
net_init_tap_one
    后端 tap 设备初始化
    (vhost 即虚拟机网卡 IO 数据通过一个内核线程在内核中直接处理而不需要经过 qemu)
    vhost 设备初始化--与内核交互, 拉起 vhost 内核线程.
```

On 32-bit hosts, QEMU is limited by virtual address space

## 3.25. 磁盘设备初始化

磁盘设备初始化, 与磁盘热插流程类似. 把设备 fd 加入 main_loop,注册 read 和 write 的回调函数

```cpp
    if (qemu_opts_foreach(qemu_find_opts("drive"), drive_init_func,
                          &machine_class->block_default_type, 1) != 0) {
        exit(1);
    }
```

```bash
-drive file=rbd:switch01_sas_vms/493c6d20-3329-480f-ad6e-391d9e997f52_disk.config:auth_supported=none:mon_host=10.180.0.32\:6789\;10.180.0.49\:6789\;10.180.0.161\:6789,format=raw,if=none,id=drive-virtio-disk25,readonly=on,cache=none
-device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x7,drive=drive-virtio-disk25,id=virtio-disk25
```

## 3.26. 创建 qemu monitor

创建 qemu monitor, 用于 qemu 与外部的通信, 使用**qmp 协议**.

```cpp
    if (qemu_opts_foreach(qemu_find_opts("mon"), mon_init_func, NULL, 1) != 0) {
        exit(1);
    }
```

## 3.27. 初始化主板

初始化主板(VCPU bios 北桥 内存 南桥, 外围设备, 中断等初始化)

```cpp
    current_machine->ram_size = ram_size;
    current_machine->maxram_size = maxram_size;
    current_machine->ram_slots = ram_slots;
    current_machine->boot_order = boot_order;
    current_machine->cpu_model = cpu_model;

    machine_class->init(current_machine);
    pc_init1
        pc_cpus_init 创建 vcpu 线程
        pc_memory_init 创建虚拟机内存
        kvm_pc_setup_irq_routing 创建中断路由表
        i440fx_init 初始化北桥
        kvm_i8259_init 初始化中断控制器
        pc_vga_init 初始化显卡设备
        pc_basic_device_init 初始化基础设备(一些默认的设备, 如 IDE 总线, ISA 总线, USB 总线等)
        pc_cmos_init 初始化 bios
```

## 3.28. 前端设备初始化

前端设备初始化(qemu command 中的-device 参数)

```cpp
qemu_opts_foreach(qemu_find_opts("device"), device_init_func, NULL, 1)
```

## 3.29. 加载设备 rom

加载设备 rom(/**usr/share/qemu**)

```cpp
int rom_load_all(void)
```

## 3.30. main_loop

主线程开启循环, 监听事件

经过上面步骤的准备之后, 一台虚拟机的所有虚拟硬件都已经准备完毕, 这时候 qemu 会进入关键的流程 main_loop 中.

main_loop 是一个**死循环**, 通过**内核**的**epoll 机制**监听上面创建的**所有 fd**, 包括设备, vcpu 等. **虚拟机内部**所有对**设备的读写**/对**vcpu 的操作**都会触发**句柄改变状态**并被 **main_loop 循环监听到**, 分发给注册好的回调函数处理这些事件.

处理完成之后继续进入 wait 状态等待下一次触发.

# 4. 余下工作

以上就是 qemu 进程启动的主要流程, 接下来启动虚拟机操作系统的流程为:

* libvirt 通过 qmp 协议下发启动命令
* main_loop 中捕获 qemu monitor 句柄事件
* qemu monitor 回调函数中调用上电流程.
* 系统上电, 执行 bios.
* bios 根据设置的 boot order 依次引导每一个启动设备, 直到遇到第一个有活动分区的设备. (一般第一块硬盘的第一个分区是活动分区, 分区表中标志位为 80. )
* 如果是启动设备是硬盘设备, 会把这块硬盘上前 446 字节的 bootloader 读入到内存中.
* bootloader 开始引导硬盘上的操作系统.

# 5. 参考

上: https://sq.163yun.com/blog/article/175668619278782464

下: https://sq.163yun.com/blog/article/175669179507773440

https://www.cnblogs.com/Bozh/p/5753379.html (还没整理)