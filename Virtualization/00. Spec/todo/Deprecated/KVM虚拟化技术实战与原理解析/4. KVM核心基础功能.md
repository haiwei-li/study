```
第4章　KVM核心基础功能
4.1　硬件平台和软件版本说明
4.2　CPU配置
4.2.1　vCPU的概念, KVM的三种模式相互切换, 切换图
4.2.2　SMP的支持, 参考cpu-info.sh,/proc/cpuinfo文件查看系统cpu, 多核心, 超线程等情况
     qemu-system-x86_64 -smp(或不加) 8  其他参数   /*.img
4.2.3　CPU过载使用
         利用客户机使用物理CPU时间差异, 保证物理CPU一直处在最大状态, 如果8核物理机, 开3个客户机都是8核的, 但3个客户机运行的时间不一样, 一个早上, 一个下午, 一个晚上. 
4.2.4　CPU模型
      qemu64,kvm64,等
      好处: 便于迁移, 兼容等问题
4.2.5　进程的处理器亲和性和vCPU的绑定(**)
      一个进程在前一个时间片是在CPUM上, 但是在后一个时间片是在CPUN上, 
      亲和性是指: 将进程绑定到指定的一个或多个CPU上, 而不允许将进程调度到其他进程上. 
      每个vCPU是宿主机上的一个普通的Qemu进程, 使用taskset工具, 设置处理器的亲和性, 将某个vCPU绑定到某个或几个固定的cpu上去调度. 
      测试步骤: 
        1)启动宿主机时, 在linux内核上加上"isolcpus="参数, 实现cpu的隔离, 从宿主机中隔离出几个cpu供客户机使用; 
        2)启动2个客户机, 并实现vcpu和物理cpu的绑定. 

4.3　内存配置
4.3.1　内存设置基本参数
    1)qemu-system-x86_64 rhe16u3.img
    2)客户机中, free -m  ,查看内核使用的情况
                         dmesg

4.3.2　EPT和VPID简介
    客户机虚拟地址GVA——>客户机物理地址GPA转换, 通过客户机操作系统实现
    客户机物理地址GPA——>宿主机物理地址HPA转换, 通过Hypervisor实现
   
    影子页表, 软件实现GVA——>HPA的转换过程, 后来引入EPT, 硬件实现GVA——GPA——HPA的转换. 
    VPID, 虚拟处理器标识, 在硬件上为每个TLB项增加一个标识, 用于区分不同的虚拟处理器的地址空间. 
4.3.3　大页(Huge Page)
    4KB的内存页——>2MB的内存页, 减少了内存页的数量, 提高了缓存命中率, 这是一种提高性能的技术手段. 
4.3.4　内存过载使用
    内存不足, 解决方案
    1)内存交换, 和交换分区来交换, openstack目测使用的就是这种方式. 
            性能较差, 要求(物理内存空间+交换空间大小总和)>所有客户机的内存总和. 
            举例: 64个内存1G的客户机, 32G内存的物理机, 如何分区, 其中宿主机维持自身进程占用资源4G. 
                   客户机要求的交换分区总和   64x1G+4G-32G=36G.
                   安装redhat建议, 32G的物理内存, 建议使用8G的交换分区. 
                   故而, 在宿主机中使用 44GB(36GB+8GB)的交换分区来满足安全使用内存过载问题. 
    2)气球(ballooning技术), 通过virio_balloon驱动来实现宿主机和客户机之间的协作. 网易的openstack实践中好像就是用这种. 
    3)页共享(page sharing), 通过KSM合并多个客户机进程使用相同的内存页. 
4.4　存储配置
4.4.1　存储配置和启动顺序
         1)qumu-kvm参数      -hda file    /  -hdb file  / ... /  -fdb  file  ;  -driver参数
         2)客户机的启动顺序: 即类似Bios中系统引导顺序
         3)举例
4.4.2　qemu-img命令
        1)qemu-img check rhe16.img 检查镜像文件; 
        2)qemu-img create -f qcow2 -o ? temp.cow
        3)  qemu-img convertmy -o qcow2 rhe16.img rhe16-1.gcow2
        4)  qemu-img info rhe16.img
        5)  snapshot  /rebase /resize  
4.4.3　QEMU支持的镜像文件格式
        1)raw——原始格式, 一次性占用磁盘空间. 
        2)qcow2——支持稀疏文件和加密、压缩. qcow——老版本, 支持后端镜像和加密
        3)sheepdog——为KVM虚拟化提供块存储, 单点无故障, 淘宝贡献较多. 
        4)clinder——openstack镜像块存储. 
4.4.4　客户机存储方式
        1)物理磁盘或磁盘分区; 
        2)LVM
        3)分布式文件系统, NFS, iSCSI, GFS2

4.5　网络配置
        1)QEMU支持的网络模式——virtio类型
        A.使用网桥模式, 通过linux-bridge来实现. 此部分可以实际参考. 
        B.使用NAT模式——此部分可以参考, 讲的不错. 
                dnsmasq, 宿主机中运行的DHCP工具, 给宿主机分配NAT内网的IP地址. 基本架构图. 
                bridge-util 管理linux-brige的工具
                iptables 对内核中IPv4包过滤工具和NAT管理工具. 
        C.QEMU内部的用户模式网络
                Qeum自身实现的网络管理, 性能差, 不常用. 
        D.其他网络选项

4.6　图形显示
4.6.1　SDL的使用
4.6.2　VNC的使用
4.6.3　VNC显示中的鼠标偏移
4.6.4　非图形模式
4.6.5　显示相关的其他选项
```

KVM采用的完全虚拟化(Full Virtualizaiton)技术,客户机(Guest)操作系统是未经过修改的普通操作系统. 在硬件虚拟化技术的支持下, 内核的KVM模块与QEMU的设备模拟协同工作, 就构成了一整套与物理计算机系统完全一致的虚拟化的计算机软硬件系统. 


一个完整的计算机系统, 必不可少的子系统包括: 处理器(CPU)、内存(Memory)、存储(Storage)、网络(Network)、显示(Display)等. 本章将会介绍KVM环境中这些基本子系统的基本概念、原理、配置和实践. 

1. 硬件平台和软件版本说明

本章以及第5章中, 默认硬件平台(CPU)是Intel(R)Xeon(R)CPU E5-4600(Romley-EP 4S), 软件系统中宿主机和客户机都是RHEL6.3系统, 而宿主机内核是KVM内核3.5版本, 用户态的QEMU是qemu-kvm 1.1.0版本, 并且各个实验中在使用qemu-kvm时都开启了KVM加速的功能. 

(1)硬件平台

一般使用支持硬件辅助虚拟化(如Intel的VT-x)的硬件平台即可. 

(2)KVM内核

选取一个较新的又较稳定的正式发布版本. 可以通过如下链接下载Linux 3.5版本: 

http://www.kernel.org/pub/linux/kernel/v3.x/linux-3.5.tar.gz.

如果是在linux.git的源代码仓库中, 可以查询到v3.5这个发布标签. 如下: 

![git tag](images/27.png)

如果使用的是kvm.git, 由于没有Linux的v3.5版本标签, 不能直接执行"git checkout v3.5", 因此需要在"git log"中查询到Linux发布3.5版本的信息, 然后切换到对应的commit, 如下: 

![安装qemu-kvm](images/28.png)

切换到合适的源码版本就可以进行配置、编译、安装等操作. 参考第三章内容. 

(3)qemu-kvm

qemu-kvm使用2012年7月初发布的qemu-kvm-1.1.0版本, 下载: 

http://sourceforge.net/projects/kvm/files/qemu-kvm/1.1.0/qemu-kvm-1.1.0.tar.gz.

在qemu-kvm.git的GIT代码仓库中, 可以先通过"git tag"命令查看有哪些标签, 然后找到"qemu-kvm-1.1.0"标签, 用"git checkout qemu-kvm-1.1.0"(或"git reset--hard qemu-kvm-1.1.0")命令切换到1.1.0的qemu-kvm版本. 

(4)QEMU命令行开启KVM加速功能

如果使用的是qemu-kvm命令行, 默认开启了对KVM的支持的, 可以通过在QEMU monitor中的"info kvm"命令来看是否显示"kvm support:enabled". 如果使用的不是qemu-kvm而是普通QEMU, 可能KVM没有被打开, 需要在QEMU启动命令行加上"-enable-kvm"参数. 

2. CPU配置

在QEMU/KVM中, QEMU提供对CPU的模拟; 在KVM打开的情况下, 客户机中CPU指令的执行由硬件处理器的虚拟化功能(如Intel VT-x和AMD AMD-V)来辅助执行, 效率很高. 

2.1 vCPU的概念

QEMU/KVM为客户机提供一套完整的硬件系统环境, 在客户机看来所拥有的CPU即vCPU(virtual cpu). 在KVM环境中, 每个客户机都是一个标准的Linux进程(QEMU进程), 而每个vCPU在宿主机中是QEMU进程派生的一个普通线程. 

普通Linux中, 进程一般有两种执行模式: 内核模式和用户模式. 而在KVM环境中, 增加了第三种模式: 客户模式. vCPU在三种执行模式下的分工如下: 

(1) 用户模式(User Mode)

主要处理I/O的模拟和管理, 由QEMU的代码实现. 

(2) 内核模式(Kernel Mode)

主要处理特别需要高性能和安全相关的指令, 如处理客户模式到内核模式的转换, 处理客户模式下的I/O指令或其他特权指令引起的退出(VM-Exit), 处理影子内存管理(shadow MMU). 

(3) 客户模式(Guest Mode)

主要执行Guest中的大部分指令, I/O和一些特权指令除外(它们会引起VM-Exit, 被hypervisor截获并模拟). 

vCPU在KVM中的三种执行模式下的转换如下图. 

![vCPU在KVM中的三种执行模式](images/29.png)

在KVM环境中, 整个系统基本架构如下. 

![KVM系统的分层架构](images/30.png)

KVM的内核部分是作为可动态加载内核模块运行在宿主机中的, 其中一个模块是和硬件平台无关的实现虚拟化核心基础架构的kvm模块, 另一个是硬件平台相关的kvm_intel(或kvm_amd)模块. 而KVM中的一个客户机是作为一个用户空间进程(qemu-kvm)运行的, 它和其他普通的用户空间进程(如gnome、kde、firefox、chrome等)一样由内核来调度使其运行在物理CPU上, 不过它由KVM模块的控制, 可以在前面介绍的三种执行模式下运行. 多个客户机就是宿主机中的多个QEMU进程, 而一个客户机的多个vCPU就是一个QEMU进程中的多个线程. 

2.2 SMP的支持

SMP(Symmetric Multi-Processor,对称多处理器). 在SMP系统中, 多个程序(进程)可以做到真正并行执行, 而单个进程的多个线程也能并行执行. 

在硬件方面, 早期更多的是在一个主板上拥有多个物理的CPU插槽来实现SMP系统, 后来随着多核技术、超线程(Hyper-Threading)技术的出现, SMP系统就会使用多处理器、多核、超线程等技术中的一个或多个. 

在操作系统软件方面, 多数的现代操作系统都提供了对SMP系统的支持. 

在Linux中, 下面的脚本(cpu-info.sh)可以根据/proc/cpuinfo文件来检查当前系统中CPU数量、多核及超线程的使用情况. 

![cpu-info.sh](images/31.png)

QEMU在给客户机模拟CPU时, 也可以提供对SMP(Symmetric Multi-processing, 对称多处理)架构的模拟, 让客户机运行在SMP系统中, 充分利用物理硬件的SMP并行处理优势. 每个vCPU在宿主机中都是一个线程, 并且宿主机Linux系统是支持多任务处理的, 因此可以通过两种操作来实现客户机的SMP, 一是将不同的vCPU的进程交换执行(分时调度, 即使物理硬件非SMP, 也可以为客户机模拟出SMP系统环境), 二是将在物理SMP硬件系统上同时执行多个vCPU的进程. 

在qemu-kvm命令行中, "-smp"参数即是为了配置客户机的SMP系统, 其具体参数如下: 

```
-smp n[,maxcpus=XX][,cores=XX][,threads=XX][,sockets=XX]
```

其中: 

- n用于设置客户机中使用的逻辑CPU数量(默认值是1). 
- maxcpus用于设置客户机中最大可能被使用的CPU数量, 包括启动时处于下线(offline)状态的CPU数量(可用于热插拔hot-plug加入CPU, 但不能超过maxcpus这个上限). 
- cores用于设置每个CPU socket上的core数量(默认值是1). 
- threads用于设置每个CPU core上的线程数(默认值是1). 
- sockets用于设置客户机中看到的总的CPU socket数量. 

示例: 

```
qemu-system-x86_64 -smp 8,sockets=2,cores=2,threads=2 /root/kvm_demo/rhel6u3.img
```

客户机中的CPU信息如下: 

![客户机的CPU信息](images/32.png)

客户机中共有8个逻辑CPU(cpu0~cpu7), 2个CPU socket, 每个socket有2个核, 每个核有2个线程(超线程处于打开状态). 

2.3 CPU过载使用

KVM允许客户机过载使用(over-commit)物理资源. 

CPU的过载使用, 是让一个或多个客户机使用vCPU的总数量超过实际拥有的物理CPU数量. QEMU会启动更多的线程来为客户机提供服务, 这些线程也是被Linux内核调度运行在物理CPU硬件上. 

关于CPU的过载使用, 推荐的做法是对多个单CPU的客户机使用over-commit, 比如, 在拥有4个逻辑CPU的宿主机中, 同时运行多于4个(如8个、16个)客户机, 其中每个客户机都分配一个vCPU. 这时, 如果每个宿主机的负载不是很大, 宿主机Linux对每个客户机的调度是非常有效的, 这样的过载使用并不会带来客户机的性能损失. 

最不推荐的做法是让某一个客户机的vCPU数量超过物理系统上存在的CPU数量. 比如, 在拥有4个逻辑CPU的宿主机中, 同时运行一个或多个客户机, 其中每个客户机的vCPU数量多于4个(如16个). 这样的使用方法会带来比较明显的性能下降, 其性能反而不如为客户机分配2个(或4个)vCPU的情况, 而且如果客户机中负载过重, 可能会让整个系统运行不稳定. 不过, 在并非100%满负载的情况下, 一个(或多个)有4个vCPU的客户机运行在拥有4个逻辑CPU的宿主机中并不会带来明显的性能损失. 

2.4 CPU模型

每一种虚拟机管理程序(Virtual Machine Monitor, 简称VMM或Hypervisor)都会定义自己的策略, 让客户机看起来有一个默认的CPU类型. 有的Hypervisor会简单地将宿主机中CPU的类型和特性直接传递给客户机使用, 而QEMU/KVM在默认情况下会向客户机提供一个名为qemu64或qemu32的基本CPU模型. QEMU/KVM的这种策略会带来一些好处, 如可以对CPU特性提供一些高级的过滤功能, 还可以将物理平台根据提供的基本CPU模型进行分组(如几台IvyBridge和Sandybridge硬件平台分为一组, 都提供相互兼容的SandyBridge或qemu64的CPU模型), 从而让客户机在同一组硬件平台上的动态迁移更加平滑和安全. 

如下命令行可以查看当前的QEMU支持的所有CPU模型

```
qemu-system-x86_64 -cpu ?
```

输出如下: 

![QEMU支持的CPU模型](images/33.png)

其中, 加了方括号的"qemu64"、"kvm64"、"kvm32"等CPU模型死QEMU命令中原生自带(build-in)的, 未加方括号的"SandyBridge"、"Westmere"、"Nehalem"等CPU模型是在配置文件中配置的. 原生自带的CPU模型是在源代码qemu-kvm.git/target-i386/cpu.c中的结构体数组builtin_x86_defs[]中定义的, 而用于自定义配置CPU模型的文件在源代码仓库中为qemu-kvm.git/sysconfigs/target/cpus-x86_64.conf(安装后的路径一般为/usr/local/share/qemu/cpus-x86_64.conf). 不过, 在qemu-kvm-1.3.0版本中, QEMU开发者删除了cpus-x86_64.conf这个文件, 而将所有CPU模型的定义都放在了target-i386/cpu.c文件中. 

在x86-64平台上编译和运行的QEMU, 在不加"-cpu"参数启动时, 采用"qemu64"作为默认的CPU模型. 可以用"-cpu cpu_model"来指定在客户机中的CPU模型. CPU的vendor_id、cpu family、flags、cpuid level等都是在cpus-x86_64.conf文件中配置好的. 

2.5 进程的处理器亲和性和vCPU的绑定

通常在SMP系统中, Linux内核的进程调度器根据自有的调度策略将系统中的一个进程调度到某个CPU上执行. 一个进程在前一个执行时间是在cpuM(M为系统中的某CPU的ID)上运行, 而在后一个执行时间是在cpuN(N为系统中另一CPU的ID)上运行. 因为Linux对进程执行的调度采用时间片法则(即用完自己的时间片立即暂停执行), 而在默认情况下, 一个普通进程或线程的处理器亲和性体现在所有可用的CPU上, 进程或线程有可能在这些CPU之中的任何一个(包括超线程)上执行. 

进程的处理器亲和性(Processor Affinity), 即CPU的绑定设置, 是指将进程绑定到特定的一个或多个CPU上去执行, 而不允许将进程调度到其他的CPU上. Linux内核对进程的调度算法也是遵守进程的处理器亲和性设置的. 设置进程的处理器亲和性带来的好处是可以减少进程在多个CPU之间交换运行带来的缓存命中失效(cache missing), 从该进程运行的角度来看, 可能带来一定程度上的性能提升. 换个角度来看, 对进程亲和性的设置也可能带来一定的问题, 如破坏了原有SMP系统中各个CPU的负载均衡(load balance), 这可能会导致整个系统的进程调度变得低效. 特别是在多处理器、多核、多线程技术使用的情况下, 在NUMA(Non-Uniform Memory Access)结构的系统中, 如果不能基于对系统的CPU、内存等有深入的了解, 对进程的处理器亲和性进行设置可能导致系统的整体性能的下降而非提升. 

每个vCPU都是宿主机中的一个普通的QEMU线程, 可以使用taskset工具对其设置处理器亲和性, 使其绑定到某一个或几个固定的CPU上去调度. 尽管Linux内核的进程调度算法已经非常高效了, 在多数情况下不需要对进程的调度进行干预, 不过, 在虚拟化环境中有时有必要将客户机的QEMU进程或线程绑定到固定的逻辑CPU上. 

提供一个有两个逻辑CPU计算能力的一个客户机. 要求CPU资源独立被占用, 不受宿主机中其他客户机的负载水平的影响. 为了满足这个需求, 可以分如下两个步骤来实现. 

第一步, 启动宿主机时隔离出两个逻辑CPU专门供一个客户机使用. 在Linux内核启动的命令行加上"isolcpus="参数, 可以实现CPU的隔离. 例如, 隔离了cpu2和cpu3的grub的配置文件如下: 

![grub配置](images/34.png)

在系统启动后, 在宿主机中检查是否隔离成功, 命令行如下: 

![CPU信息](images/35.png)

由上面的命令行输出信息可知, cpu0和cpu1上分别有106和107个线程在运行, 而cpu2和cpu3上分别只有4个线程在运行. 而且, 根据输出信息中cpu2和cpu3上运行的线程信息(也包括进程在内), 分别有migration进程(用于进程在不同CPU间迁移)、两个kworker进程(用于处理workqueues)、ksoftirqd进程(用于调度CPU软中断的进程), 这些进程都是内核对各个CPU的一些守护进程. 没有其他的普通进程在cup2和cpu3上运行, 说明对它们的隔离是生效的. 

ps命令显示当前系统的进程信息的状态, 它的"-e"参数用于显示所有的进程, "-L"参数用于将线程(LWP, light-weight process)也显示出来, "-o"参数表示以用户自定义的格式输出(其中"psr"这列表示当前分配给进程运行的处理器编号, "lwp"列表示线程的ID, "ruser"表示运行进程的用户, "pid"表示进程的ID, "ppid"表示父进程的ID, "args"表示运行的命令及其参数). 结合ps和awk工具的使用, 是为了分别将在处理器cpu2和cpu3上运行的进程打印出来. 

第二步, 启动一个拥有两个vCPU的客户机并将其vCPU绑定到宿主机中两个CPU上. 此操作过程的命令行如下: 

![vcpu](images/36.png)

![vcpu](images/37.png)

![vcpu](images/38.png)

对于taskset命令, 此处用法是: taskset -p [mask] pid. 其中, mask是一个代表了处理器亲和性的掩码数字, 转化为二进制表示后, 其值从最低位到最高位分别代表了第一个逻辑CPU到最后一个逻辑CPU, 进程调度器可能将该进程调度到所有标志为"1"的位代表的逻辑CPU上去运行. 根据上面的输出, 在运行taskset命令之前, QEMU线程的处理器亲和性mask值是0x3(其二进制值为0011), 可知其可能会被调度到cpu0和cpu1上运行; 而在运行"taskset -p 0x4 3967"命令后, 提示新的mask值被设为0x4(其二进制值为0100), 所以该进程就只能被调度到cpu2上去运行, 即通过taskset工具实现了将vCPU进程绑定到特定的CPU上. 

在上面命令行中, 根据ps命令可以看到QEMU的线程和进程的关系, 但如何查看vCPU与QEMU线程之间的关系呢?可以切换(使用"Ctrl+Alt+2"快捷键)到QEMU monitor中进行查看, 运行"info cpus"命令即可(还记得3.6节中运行过的"info kvm"命令吧), 其输出结果如下: 

![info cpus](images/39.png)

客户机中的cpu0对应的线程ID为3967, cpu1对应的线程ID为3968. 另外, "CPU#0"前面有一个星号(*), 是标识cpu0是BSP(Boot Strap Processor, 系统最初启动时在SMP生效前使用的CPU). 

3. 内存配置

作用是暂时存放CPU中将要执行的指令和数据, 所有程序的运行都必须先载入到内存中才能够执行. 本节主要介绍KVM中内存的配置. 

3.1 内存设置基本参数

通过QEMU命令行启动客户机时设置内存大小的参数如下: 

-m megs　#设置客户机的内存为megs MB大小

默认的单位为MB, 也支持加上"M"或"G"作为后缀来显式指定使用MB或GB作为内存分配的单位. 如果不设置-m参数, QEMU对客户机分配的内存大小默认值为128MB. 

free命令用于查看内存的使用情况, "-m"参数是内存大小以MB为单位来显示, 以上信息中显示总的内存为112MB, 这个值与128MB有一定差距, 其原因是free命令显示的总内存是除去了内核执行文件占用内存和一些系统保留的内存之后能使用的内存. 而通过dmesg命令显示的内核打印的信息可以看出, 内核检测到总的内存为131064 KB, 几乎是完完整整的128MB内存了(128*1024=131072, 与131064非常接近). 

通过/proc/meminfo看到的"MemTotal"的大小为1048568kB, 比1024MB稍小, 其原因与前面free命令输出的总内存是一样的. 

3.2 EPT和VPID简介

EPT(Extended Page Tables, 扩展页表), 属于Intel的第二代硬件虚拟化技术, 它是针对内存管理单元(MMU)的虚拟化扩展. EPT降低了内存虚拟化的难度(与影子页表相比), 也提升了内存虚拟化的性能. 是CPU硬件的一个特性. 

和运行在真实物理硬件上的操作系统一样, 在客户机操作系统看来, 客户机可用的内存空间也是一个从零地址开始的连续的物理内存空间. 为了达到这个目的, Hypervisor(即KVM)引入了一层新的地址空间, 即客户机物理地址空间, 这个地址空间不是真正的硬件上的地址空间, 它们之间还有一层映射. 所以, 在虚拟化环境下, 内存使用就需要两层的地址转换, 即客户机应用程序可见的客户机虚拟地址(Guest Virtual Address,GVA)到客户机物理地址(Guest Physical Address, GPA)的转换, 再从客户机物理地址(GPA)到宿主机物理地址(Host Physical Address, HPA)的转换. 其中, 前一个转换由客户机操作系统来完成, 而后一个转换由Hypervisor来负责. 

在硬件EPT特性加入之前, 影子页表(Shadow Page Tables)是从软件上维护了从客户机虚拟地址(GVA)到宿主机物理地址(HPA)之间的映射, 每一份客户机操作系统的页表也对应一份影子页表. 有了影子页表, 在普通的内存访问时都可实现从GVA到HPA的直接转换, 从而避免了上面前面提到的两次地址转换. Hypervisor将影子页表载入到物理上的内存管理单元(Memory Management Unit, MMU)中进行地址翻译. 下图展示了GVA、GPA、HPA之间的转换以及影子页表的作用. 

![影子页表的作用](images/40.png)

尽管影子页表提供了在物理MMU硬件中能使用的页表, 但缺点明显. 首先影子页表实现非常复杂, 导致其开发、调试和维护都比较困难. 其次, 影子页表的内存开销也比较大, 因为需要为每个客户机进程对应的页表的都维护一个影子页表. 

Intel的CPU提供了EPT技术(AMD提供的类似技术叫做NPT, 即Nested Page Tables), 直接在硬件上支持GVA--＞GPA--＞HPA的两次地址转换, 从而降低内存虚拟化实现的复杂度, 也进一步提升了内存虚拟化的性能. 图4-4展示了Intel EPT技术的基本原理. 

![EPT基本原理](images/41.png)

CR3(控制寄存器3)将客户机程序所见的客户机虚拟地址(GVA)转化为客户机物理地址(GPA), 然后在通过EPT将客户机物理地址(GPA)转化为宿主机物理地址(HPA). 这两次转换地址转换都是由CPU硬件来自动完成的, 其转换效率非常高. 在使用EPT的情况下, 客户机内部的Page Fault、INVLPG(使TLB项目失效)指令、CR3寄存器的访问等都不会引起VM-Exit, 因此大大减少了VM-Exit的数量, 从而提高了性能. 另外, EPT只需要维护一张EPT页表, 而不需要像"影子页表"那样为每个客户机进程的页表维护一张影子页表, 从而也减少了内存的开销. 

VPID(VirtualProcessor Identifiers, 虚拟处理器标识), 是在硬件上对TLB资源管理的优化, 通过在硬件上为每个TLB项增加一个标识, 用于不同的虚拟处理器的地址空间, 从而能够区分开Hypervisor和不同处理器的TLB. 硬件区分了不同的TLB项分别属于不同虚拟处理器, 因此可以避免每次进行VM-Entry和VM-Exit时都让TLB全部失效, 提高了VM切换的效率. 由于有了这些在VM切换后仍然继续存在的TLB项, 硬件减少了一些不必要的页表访问, 减少了内存访问次数, 从而提高了Hypervisor和客户机的运行速度. VPID也会对客户机的实时迁移(Live Migration)有很好的效率提升, 会节省实时迁移的开销, 会提升实时迁移的速度, 降低迁移的延迟(Latency). 

VPID与EPT是一起加入到CPU中的特性, 也是Intel公司在2009年推出Nehalem系列处理器上新增的与虚拟化相关的重要功能. 

在Linux操作系统中, 可以通过如下命令查看/proc/cpuinfo中的CPU标志来确定当前系统是否支持EPT和VPID功能. 

grep ept /proc/cpuinfo

grep pvpid /proc/cpuinfo

在宿主机中, 可以根据sysfs文件系统中kvm_intel模块的当前参数值来确定KVM是否打开EPT和VPID特性. 在默认情况下, 如果硬件支持了EPT、VPID, 则kvm_intel模块加载时默认开启EPT和VPID特性, 这样KVM会默认使用它们. 

![查看KVM是否打开](images/42.png)

在加载kvm_intel模块时, 可以通过设置ept和vpid参数的值来打开或关闭EPT和VPID. 当然, 如果kvm_intel模块已经处于加载状态, 则需要先卸载这个模块, 在重新加载之时加入所需的参数设置. 当然, 一般不要手动关闭EPT和VPID功能, 否则会导致客户机中内存访问的性能下降. 

![kvm_intel](images/43.png)

3.3 大页(Huge Page)

x86(包括x86-32和x86-64)架构的CPU默认使用4KB大小的内存页面, 但是它们也支持较大的内存页, 如x86-64系统就支持2MB大小的大页(huge page). 

Linux2.6及以上的内核都支持huge page. 如果在系统中使用了huge page, 则内存页的数量会减少, 从而需要更少的页表(page table), 节约了页表所占用的内存数量, 并且所需的地址转换也减少了, TLB缓存失效的次数就减少了, 从而提高了内存访问的性能. 另外, 由于地址转换所需的信息一般保存在CPU的缓存中, huge page的使用让地址转换信息减少, 从而减少了CPU缓存的使用, 减轻了CPU缓存的压力, 让CPU缓存能更多地用于应用程序的数据缓存, 也能够在整体上提升系统的性能. 

在KVM中, 也可以将huge page的特性应用到客户机中, qemu-kvm就提供了"-mem-path FILE"参数选项用于使用huge page. 另外, 还有一个参数"-mem-prealloc"可以让宿主机在启动客户机时就全部分配好客户机的内存, 而不是在客户机实际用到更多内存时才按需分配. -mem-prealloc必须在有"-mem-path"参数时才能使用. 提前分配好内存的好处是客户机的内存访问速度更快, 缺点是客户机启动时就得到了所有的内存, 从而让宿主机的内存很快减少了(而不是根据客户机的需求而动态调整内存分配). 

可以通过在宿主机中的如下几个操作让客户机使用huge page. 

(1) 检查宿主机目前状态, 检查默认内存大小和内存使用情况

![检查](images/44.png)

(2) 挂载hugetlbfs文件系统, 命令为 "mount -t hugetlbfs hugetlbfs /dev/hugepages"

![挂载文件系统](images/45.png)

(3) 设置hugepage的数量, 命令为"sysctl vm.nr_hugepages=num"

![设置hugepage数量](images/46.png)

(4) 启动客户机让其使用hugepage的内存, 使用"-mem-path"参数

![配置客户机](images/47.png)

(5) 查看宿主机中huge page的使用情况, 可以看到"HugePages_Free"数量减少, 因为客户机使用了一定数量的hugepage. 在如下的输出中, "HugePages_Free"数量的减少没有512(512*2MB=1024MB)那么多, 这是因为启动客户机时并没有实际分配1024MB内存, qemu-kvm命令行中加上前面提到的"-mem-prealloc"参数就会让meminfo文件中"HugePages_Free"的数量减少和分配给客户机的一致. 

![宿主机huge page](images/48.png)

对于内存访问密集型的应用, 在KVM客户机中使用huge page是可以较明显地提高客户机性能的, 不过, 它也有一个缺点, 使用huge page的内存不能被换出(swap out), 也不能使用ballooning方式自动增长. 

3.4 内存过载使用

一般来说, 有如下三种方式来实现内存的过载使用. 

1)内存交换(swapping): 用交换空间(swap space)来弥补内存的不足. 

2)气球(ballooning): 通过virio_balloon驱动来实现宿主机Hypervisor和客户机之间的协作. 

3)页共享(page sharing): 通过KSM(Kernel Samepage Merging)合并多个客户机进程使用的相同内存页. 

其中, 第一种内存交换的方式是最成熟的(Linux中很早就开始应用), 也是目前广泛使用的, 不过, 相比KSM和ballooning的方式效率较低一些. ballooning和KSM将在第5章中详细介绍, 本章主要介绍利用swapping这种方式实现内存过载使用. 

KVM中客户机是一个QEMU进程, 宿主机系统没有特殊对待它而分配特定的内存给QEMU, 只是把它当做一个普通Linux进程. Linux内核在进程请求更多内存时才分配给它们更多的内存, 所以也是在客户机操作系统请求更多内存时, KVM才向其分配更多的内存. 用swapping方式来让内存过载使用, 要求有足够的交换空间(swap space)来满足所有的客户机进程和宿主机中其他进程所需内存. 可用的物理内存空间和交换空间的大小之和应该等于或大于配置给所有客户机的内存总和, 否则, 在各个客户机内存使用同时达到较高比率时可能会有客户机(因内存不足)被强制关闭. 

下面通过一个实际的例子来说明如何计算应该分配的交换空间大小以满足内存的过载使用. 

某个服务器有32GB的物理内存, 想在其上运行64个内存配置为1GB的客户机. 在宿主机中, 大约需要4GB大小的内存来满足系统进程、驱动、磁盘缓存及其他应用程序所需内存(不包括客户机进程所需内存). 计算过程如下: 
客户机所需交换分区为: 64 x 1GB+4GB-32GB=36GB. 
根据Redhat的建议, 对于32GB物理内存的RHEL系统, 推荐使用8GB的交换分区. 

所以, 在宿主机中总共需要建立44GB(36GB+8GB)的交换分区来满足安全实现客户机内存的过载使用. 

下面是在一台Ivy Bridge桌面级的硬件平台上进行的简单实验, 可以看出客户机并非一开始就在宿主机中占用其启动时配置的内存. 
在宿主机中, 在启动客户机之前和之后查看到的系统内存情况如下: 

![宿主机系统内存变化](images/49.png)

在客户机中, 查看内存使用情况如下: 

![客户机系统内存](images/50.png)

一般不建议过多地过载使用内存. 一方面, 交换空间通常是由磁盘分区来实现的, 其读写速度比物理内存读写速度慢得多, 性能并不好; 另一方面, 过多的内存过载使用也可能导致系统稳定性降低. 

4. 存储配置

本节主要以磁盘、光盘等为例介绍KVM中的存储配置. 

4.1　存储配置和启动顺序

QEMU提供了对多种块存储设备的模拟, 包括IDE设备、SCSI设备、软盘、U盘、virtio磁盘等, 而且对设备的启动顺序提供了灵活的配置. 

1.存储的基本配置选项

在qemu-kvm命令行工具中, 主要有如下的参数来配置客户机的存储. 

(1)-hda file

将file镜像文件作为客户机中的第一个IDE设备(序号0), 在客户机中表现为/dev/hda设备(若客户机中使用PIIX_IDE驱动)或/dev/sda设备(若客户机中使用ata_piix驱动). 如果不指定-hda或-hdb等参数, 那么在前面一些例子中提到的"qemu-system-x86_64/root/kvm_demo/rhel6u3.img"就与加上-hda参数来指定镜像文件的效果一样的. 另外, 也可以将宿主机中的一个硬盘(如/dev/sdb)作为-hda的file参数来使用, 从而让整个硬盘模拟为客户机的第一个IDE设备. 如果file文件的文件名中包含有英文逗号(","), 则在书写file时应该使用两个逗号(因为逗号是qemu-kvm命令行中的特殊间隔符, 例如用于"-cpu qemu64,+vmx"这样的选项), 如使用"-hda my,,file"将my,file这个文件作为客户机的第一个IDE设备. 

(2)-hdb file

将file作为客户机中的第二个IDE设备(序号1), 在客户机中表现为/dev/hdb或/dev/sdb设备. 

(3)-hdc file

将file作为客户机中的第三个IDE设备(序号2), 在客户机中表现为/dev/hdc或/dev/sdc设备. 

(4)-hdd file

将file作为客户机中的第四个IDE设备(序号3), 在客户机中表现为/dev/hdd或/dev/sdd设备. 

(5)-fda file

将file作为客户机中的第一个软盘设备(序号0), 在客户机中表现为/dev/fd0设备. 也可以将宿主机中的软驱(/dev/fd0)作为-fda的file来使用. 

(6)-fdb file

将file作为客户机中的第二个软盘设备(序号1), 在客户机中表现为/dev/fd1设备. 

(7)-cdrom file

将file作为客户机中的光盘CD-ROM, 在客户机中通常表现为/dev/cdrom设备. 也可以将宿主机中的光驱(/dev/cdrom)作为-cdrom的file来使用. 注意, -cdrom参数不能和-hdc参数同时使用, 因为"-cdrom"就是客户机中的第三个IDE设备. 在通过物理光驱中的光盘或磁盘中ISO镜像文件安装客户机操作系统时(参见3.5节安装客户机), 一般会使用-cdrom参数. 

(8)-mtdblock file

使用file文件作为客户机自带的一个Flash存储器(通常说的闪存). 

(9)-sd file

使用file文件作为客户机中的SD卡(Secure Digital Card). 

(10)-pflash file

使用file文件作为客户机的并行Flash存储器(Parallel Flash Memory). 

2. 详细配置存储驱动器的-driver参数

qemu-kvm还提供了"-driver"参数来详细定义一个存储驱动器, 该参数的具体形式如下: 

-driver option[, option[,option[, ...]]]

3. 配置客户机启动顺序的参数

-boot [order=drivers][,once=drivers][,menu=on|off][,splash=splashfile][,splash-time=sp-time]

在QEMU模拟的X86 PC平台上, 用"a"、"b"分别表示第一和第二个软驱, 用"c"表示第一个硬盘, 用"d"表示CD-ROM光驱, 用"n"表示从网络启动. 其中, 默认从硬盘启动, 要从光盘启动可以设置"-boot order=d". "once"表示设置第一次启动的启动顺序, 在系统重启(reboot)后该设置即无效, 如"-boot once=d"设置表示本次从光盘启动, 但系统重启后从默认的硬盘启动. "memu=on|off"用于设置交互式的启动菜单选项(前提是使用的客户机BIOS支持), 它的默认值是"menu=off", 表示不开启交互式的启动菜单选择. "splash=splashfile"和"splash-time=sp-time"选项都是在"menu=on"时才有效, 将名为splashfile的图片作为logo传递给BIOS来显示, 而sp-time是BIOS显示splash图片的时间, 其单位是毫秒(ms). 下图展示了在使用"-boot order=dc,menu=on"设置后, 在客户机启动窗口中按F12进入的启动菜单. 

![客户机中的启动菜单选项](images/51.png)

4. 存储配置示例

通过如下的3个等价命令之一启动一个客户机. 

1)qemu-system-x86_64 -m 1024 -smp 2 rhel6u3.img

2)qemu-system-x86_64 -m 1024 -smp 2 -hda rhel6u3.img

3)qemu-system-x86_64 -m 1024 -smp 2 -drive file=rhel6u3.img,if=ide,cache=writethrough

在客户机中查看磁盘情况, 如下: 

![客户机磁盘](images/52.png)  
![客户机磁盘](images/53.png)  

由于这个客户机使用的是piix_ide驱动, 因此看到的是/dev/hda这样的磁盘, 而对于同样一个磁盘, 如果客户机操作系统使用的是ata_piix驱动(此时无piix_ide驱动), 则看到的是/dev/sda这样的磁盘, 效果如下: 

![客户机磁盘](images/54.png)  

4.2 qemu-img命令

qemu-img是QEMU的磁盘管理工具, 在完成qemu-kvm源码编译后就会默认编译好qemu-img这个二进制文件. qemu-img也是QEMU/KVM使用过程中一个比较重要的工具, 本节对其用法进行介绍. 

基本用法如下: 

qemu-img command [command options]

它支持的命令分为如下几种. 

(1)check [-f fmt]filename

对磁盘镜像文件进行一致性检查, 查找镜像文件中的错误, 目前仅支持对"qcow2"、"qed"、"vdi"格式文件的检查. 

其中, qcow2是QEMU 0.8.3版本引入的镜像文件格式, 也是目前使用最广泛的格式. qed(QEMU enhanced disk)是从QEMU 0.14版开始加入的增强磁盘文件格式, 避免了qcow2格式的一些缺点, 也提高了性能, 不过目前还不够成熟. 而vdi(Virtual Disk Image)是Oracle的VirtualBox虚拟机中的存储格式. 参数-f fmt是指定文件的格式, 如果不指定格式, qemu-img会自动检测. filename是磁盘镜像文件的名称(包括路径). 

(2)create [-f fmt] [-o options] filename [size]

创建一个格式为fmt, 大小为size, 文件名为filename的镜像文件. 根据文件格式fmt的不同, 还可以添加一个或多个选项(options)来附加对该文件的各种功能设置, 可以使用"-o ?"来查询某种格式文件支持哪些选项, 在"-o"选项中各个选项用逗号来分隔. 

如果在"-o"选项中使用了backing_file这个选项来指定其后端镜像文件, 那么这个创建的镜像文件仅记录与后端镜像文件的差异部分. 后端镜像文件不会被修改, 除非在QEMU monitor中使用"commit"命令或使用"qemu-img commit"命令去手动提交这些改动. 在这种情况下, size参数不是必须需的, 其值默认为后端镜像文件的大小. 另外, 直接使用"-b backfile"参数也与"-o backing_file=backfile"效果相同. 

size选项用于指定镜像文件的大小, 其默认单位是字节(bytes), 也可以支持k(即K)、M、G、T来分别表示KB、MB、GB、TB大小. 另外, 镜像文件的大小(size)也并非必须写在命令的最后, 也可以写在"-o"选项中作为其中一个选项. 

对create命令的演示如下所示, 其中包括查询qcow2格式支持的选项、创建有backing_file的qcow2格式的镜像文件、创建没有backing_file的10GB大小的qcow2格式的镜像文件. 

![qemu-img](images/55.png)  

(3)commit [-f fmt] filename

提交filename文件中的更改到后端支持镜像文件(创建时通过backing_file指定的)中. 

(4)convert [-c] [-f fmt] [-O output_fmt] [-o options] flename [flename2[...]] output_flename

将fmt格式的filename镜像文件根据options选项转换为格式为output_fmt的名为output_filename的镜像文件. 这个命令支持不同格式的镜像文件之间的转换, 比如可以用VMware使用的vmdk格式文件转换为qcow2文件, 这对从其他虚拟化方案转移到KVM上的用户非常有用. 一般来说, 输入文件格式fmt由qemu-img工具自动检测到, 而输出文件格式output_fmt根据自己需要来指定, 默认会被转换为raw文件格式(且默认使用稀疏文件的方式存储以节省存储空间). 

其中, "-c"参数表示对输出的镜像文件进行压缩, 不过只有qcow2和qcow格式的镜像文件才支持压缩, 并且这种压缩是只读的, 如果压缩的扇区被重写, 则会被重写为未压缩的数据. 同样可以使用"-o options"来指定各种选项, 如后端镜像、文件大小、是否加密, 等等. 使用backing_file选项来指定后端镜像, 使生成的文件成为copy-on-write的增量文件, 这时必须让在转换命令中指定的后端镜像与输入文件的后端镜像的内容相同, 尽管它们各自后端镜像的目录和格式可能不同. 

如果使用qcow2、qcow、cow等作为输出文件格式来转换raw格式的镜像文件(非稀疏文件格式), 镜像转换还可以起到将镜像文件转化为更小的镜像, 因为它可以将空的扇区删除使之在生成的输出文件中不存在. 

(5)info [-f fmt] filename

展示filename镜像文件的信息. 如果文件使用的是稀疏文件的存储方式, 也会显示出它本来分配的大小及实际已占用的磁盘空间大小. 如果文件中存放有客户机快照, 快照的信息也会被显示出来. 

(6)snapshot [-l| -a snapshot|-c snapshot|-d snapshot]filename

"-l"选项表示查询并列出镜像文件中的所有快照, "-a snapshot"表示让镜像文件使用某个快照, "-c snapshot"表示创建一个快照, "-d"表示删除一个快照. 

(7)rebase [-f fmt] [-t cache] [-p] [-u] -b backing_file [-F backing_fmt] filename

改变镜像文件的后端镜像文件, 只有qcow2和qed格式支持rebase命令. 使用"-b backing_file"中指定的文件作为后端镜像, 后端镜像也被转化为"-F backing_fmt"中指定的后端镜像格式. 

这个命令可以工作于两种模式之下, 一种是安全模式(Safe Mode), 这是默认的模式, qemu-img会根据比较原来的后端镜像与现在的后端镜像的不同进行合理的处理; 另一种是非安全模式(Unsafe Mode), 是通过"-u"参数来指定的, 这种模式主要用于将后端镜像重命名或移动位置后对前端镜像文件的修复处理, 由用户去保证后端镜像的一致性. 

(8)resize filename[+|-]size

改变镜像文件的大小, 使其不同于创建之时的大小. "+"和"-"分别表示增加和减少镜像文件的大小, size也支持K、M、G、T等单位的使用. 缩小镜像的大小之前, 需要在客户机中保证其中的文件系统有空余空间, 否则会数据丢失, 另外, qcow2格式文件不支持缩小镜像的操作. 在增加了镜像文件大小后, 也需启动客户机在其中应用"fdisk"、"parted"等分区工具进行相应的操作才能真正让客户机使用到增加后的镜像空间. 不过使用resize命令时需要小心(做好备份), 如果失败, 可能会导致镜像文件无法正常使用而造成数据丢失. 

4.3 QEMU支持的镜像文件格式

qemu-img支持非常多种的文件格式, 可以通过"qemu-img -h"查看其命令帮助得到, 它支持二十多种格式: blkdebug、blkverify、bochs、cloop、cow、tftp、ftps、ftp、https、http、dmg、nbd、parallels、qcow、qcow2、qed、host_cdrom、host_floppy、host_device、file、raw、sheepdog、vdi、vmdk、vpc、vvfat. 

下面对其中的几种文件格式做简单的介绍. 

(1)raw

原始的磁盘镜像格式, 也是qemu-img命令默认的文件格式. 优势在于它非常简单且非常容易移植到其他模拟器(emulator, QEMU也是一个emulator)上去使用. 如果客户机文件系统(如Linux上的ext2/ext3/ext4、Windows的NTFS)支持"空洞"(hole), 那么镜像文件只有在被写有数据的扇区才会真正占用磁盘空间, 从而起到节省磁盘空间的作用, 就如前面用"qemu-img info"命令查看镜像文件信息中看到的那样. qemu-img默认的raw格式的文件其实是稀疏文件(sparse file), 而3.5节"安装客户机"中使用"dd"命令创建的镜像也是raw格式, 不过那是一开始就让镜像实际占用了分配的空间, 而没有使用稀疏文件的方式对待空洞来节省磁盘空间. 尽管一开始就实际占用磁盘空间的方式没有节省磁盘的效果, 不过这种方式在写入新的数据时不需要宿主机从现有磁盘空间中分配, 因此在第一次写入数据时, 这种方式的性能会比稀疏文件的方式更好一点. 

(2)host_device

在需要将镜像转化到不支持空洞的磁盘设备时需要用这种格式来代替raw格式. 

(3)qcow2

qcow2是QEMU目前推荐的镜像格式, 它是功能最多的格式. 它支持稀疏文件(即支持空洞)以节省存储空间, 它支持可选的AES加密以提高镜像文件安全性, 支持基于zlib的压缩, 支持在一个镜像文件中有多个虚拟机快照. 

在qemu-img命令中qcow2支持如下几个选项: 

- backing_file, 用于指定后端镜像文件. 

- backing_fmt, 设置后端镜像的镜像格式. 

- cluster_size, 设置镜像中簇的大小, 取值在512B到2MB之间, 默认值为64KB. 较小的簇可以节省镜像文件的空间, 而较大的簇可以带来更好的性能, 需要根据实际情况来平衡, 一般采用默认值即可. 

- preallocation, 设置镜像文件空间的预分配模式, 其值可为"off"、"metadata"之一. "off"模式是默认值, 设置了不为镜像文件预分配磁盘空间. 而"metadata"模式用于设置为镜像文件预分配metadata的磁盘空间, 所以这种方式生成的镜像文件稍大一点, 不过在其真正分配空间写入数据时效率更高. 另外, 一些版本的qemu-img(如RHEL6.3自带的)还支持"full"模式的预分配, 它表示在物理上预分配全部的磁盘空间, 它将整个镜像的空间都填充零以占用空间, 当然它所花费的时间较长, 不过使用时性能更好. 

- encryption用于设置加密, 当它等于"on"时, 镜像被加密. 它使用128位密钥的ASE加密算法, 故其密码长度可达16个字符(每个字符8位), 可以保证加密的安全性较高. 在将"qemu-img convert"命令转化为qcow2格式时, 加上"-o encryption"即可对镜像文件设置密码, 而在使用镜像启动客户机时需要在QEMU monitor中输入"cont"或"c"(是continue的意思)命令来唤醒客户机输入密码后继续执行(否则客户机将不会真正启动), 命令行演示如下: 

 ![image](images/56.png)  

(4)qcow

较旧的QEMU镜像格式, 现在已经很少使用了, 一般用于兼容比较老版本的QEMU. 它支持backing_file(后端镜像)和encryption(加密)两个选项. 

(5)cow

用户模式Linux(User-Mode Linux)的Copy-On-Write的镜像文件格式. 

(6)vdi

兼容Oracle(Sun)VirtualBox1.1的镜像文件格式(Virtual Disk Image). 

(7)vmdk

兼容VMware 4版本以上的镜像文件格式(Virtual Machine Disk Format). 

(8)vpc

兼容Microsoft的Virtual PC的镜像文件格式(Virtual Hard Disk format). 

(9)sheepdog

Sheepdog项目是由日本NTT实验室发起的, 为QEMU/KVM做的一个开源的分布式存储系统, 为KVM虚拟化提供块存储. 它无单点故障(无类似于元数据服务器的中央节点), 方便扩展(已经支持上千的节点数量), 其配置简单、运维成本较低, 总的来说, 具有高可用性、易扩展性、易管理性等优势. 

sheepdog项目的官方网站为: http://www.osrg.net/sheepdog/. 

目前, 国内的淘宝公司对该项目也有较大的贡献, 详情可见淘宝sheepdog项目的网站: http://sheepdog.taobao.org/

4.4 客户机存储方式

在QEMU/KVM中, 客户机镜像文件可以由很多种方式来构建, 其中几种如下: 

1)本地存储的客户机镜像文件. 

2)物理磁盘或磁盘分区. 

3)LVM(Logical Volume Management), 逻辑分区. 

4)NFS(Network File System), 网络文件系统. 

5)iSCSI(Internet Small Computer System Interface), 基于Internet的小型计算机系统接口. 

6)本地或光纤通道连接的LUN(Logical Unit Number). 

7)GFS2(Global File System 2).

不仅一个文件可以分配给客户机作为镜像文件系统, 而且一个完整的磁盘或LVM分区也可以作为镜像分配给客户机使用. 不过, 磁盘分区、LVM分区由于没有磁盘的MBR引导记录, 不能作为客户机的启动镜像, 只能作为客户机附属的非启动块设备. 一般来说, 磁盘或LVM分区会有较好的性能, 读写的延迟较低、吞吐量较高. 

而NFS作为使用非常广泛的分布式文件系统, 可以使客户端挂载远程NFS服务器中的共享目录, 然后像使用本地文件系统一样使用NFS远程文件系统. 如果NFS服务器端向客户端开放了读写的权限, 那么可以直接挂载NFS, 然后使用其中的镜像文件作为客户启动磁盘. 如果没有向客户端开放写权限, 也可以在NFS客户端系统将远程NFS系统上的镜像文件作为后端镜像(backing file), 以建立qcow2格式Copy-On-Write的本地镜像文件供客户机使用, 这样做还有一个好处是保持NFS服务器上的镜像一致性、完整性, 从而可以供给多个客户端同时使用. 

在宿主机中, 挂载NFS文件系统、建立qcow2镜像, 然后启动客户机, 如下所示: 

![image](images/57.png)  

在客户机中, 查看磁盘文件系统, 如下: 

![磁盘文件系统](images/58.png)  

iSCSI是一套基于IP协议的网络存储标准, 真正的物理存储放在初始端(initiator), 而使用iSCSI磁盘的是目标端(target), 它们之间实现了SCSI标准的命令, 让目标端使用起来就和使用本地的SCSI硬盘一样, 只是数据是在网络上进行读写操作的. 光纤通道(Fibre Channel)也可以实现与iSCSI类似的存储区域网络(storage area network, SAN), 不过它需要光纤作为特殊的网络媒介. 而GFS2是由Redhat公司主导开发的主要给Linux计算机集群使用的共享磁盘文件系统, 一般在Redhat的RHEL系列系统中有较多使用, 它也可被用做QEMU/KVM的磁盘存储系统. 

另外, 如果需要获得更高性能的磁盘IO, 可以使用半虚拟化的virtio作为磁盘驱动程序, 第5章中将会详细介绍virtio的相关内容. 

5. 网络配置

5.1　QEMU支持的网络模式

qemu-kvm主要向客户机提供了如下4种不同模式的网络. 

1)基于网桥(bridge)的虚拟网卡. 

2)基于NAT(Network Addresss Translation)的虚拟网络.  

3)QEMU内置的用户模式网络(user mode networking). 

4)直接分配网络设备的网络(包括VT-d和SR-IOV). 

这里主要讲述前三种模式, 第4种网络设备的直接分配将在第5章中详细讲述. 

除了特别的需要iptables配置端口映射、数据包转发规则的情况, 一般默认将防火墙所有规则都关闭, 以避免妨碍客户机中的网络畅通, 在实际生产环境中, 可根据实际系统的特点进行配置. 

在QEMU命令行中, 对客户机网络的配置(除了网络设备直接分配之外)都是用"-net"参数进行配置的, 如果没有设置任何的"-net"参数, 则默认使用"-net nic-net user"参数, 进而使用完全基于QEMU内部实现的用户模式下的网络协议栈. 

qemu-kvm提供了对一系列主流和兼容性良好的网卡的模拟, 通过"-net nic,model=?"参数可以查询到当前的qemu-kvm工具实现了哪些网卡的模拟. 如下命令行显示了qemu-kvm-1.1.0中能模拟的网卡种类. 

qemu-system-x86_64 -net nic,model=?

![qemu-kvm模拟的网卡种类](images/59.png)  

"rtl8139"这个网卡模式是qemu-kvm默认的模拟网卡类型. RTL8139是Realtek半导体公司的一个10/100M网卡系列, 是曾经非常流行(当然现在看来有点古老)且兼容性好的网卡. 几乎所有的现代操作系统都对RTL8139网卡驱动提供支持. "e1000"是提供Intel e1000系列的网卡模拟, 纯的QEMU(非qemu-kvm)默认就是提供Intel e1000系列的虚拟网卡. 而virtio类型是qemu-kvm对半虚拟化IO(virtio)驱动的支持(将会在第5章中详细介绍virtio的基本原理、配置和使用). 

qemu-kvm命令行在不加任何网络相关的参数启动客户机后, 在客户机中可以看到它有一个默认的RTL8139系列的网卡(如下). 

![默认网卡](images/60.png)  

如下的命令行会模拟一个Intel e1000系列的网卡供客户机使用. 

qmu-system-x86_64 rhel6u3.img -net nic,model=e1000

在客户机中看到的e1000系列网卡如下, 默认是Intel 82540EM系列的网卡. 

![82540系列网卡](images/61.png)  

qemu-kvm命令行中基本的"-net"参数细节如下: 

-net nic[,vlan=n][,macaddr=mac][,model=type][,name=name][,addr=addr][,vectors=v]

执行这个命令会让QEMU建立一个新的网卡并将其连接到n号VLAN上. 

其中: 

"-net nic"是必需的参数, 表明这是一个网卡的配置. 

vlan=n, 表示将网卡放入到编号为n的VLAN, 默认为0. 

macaddr=mac, 设置网卡的MAC地址, 默认会根据宿主机中网卡的地址来分配. 若局域网中客户机太多, 建议自己设置MAC地址, 以防止MAC地址冲突. 

model=type, 设置模拟的网卡的类型, 在qemu-kvm中默认为rtl8139. 

name=name, 为网卡设置一个易读的名称, 该名称仅在QEMU monitor中可能用到. 

addr=addr, 设置网卡在客户机中的PCI设备地址为addr. 

vectors=v, 设置该网卡设备的MSI-X向量的数量为n, 该选项仅对使用virtio驱动的网卡有效. 设置为"vectors=0"是关闭virtio网卡的MSI-X中断方式. 

如果需要向一个客户机提供多个网卡, 可以多次使用"-net"参数. 

在宿主机中用如下的命令行启动一个客户机, 并使用上面的一些网络参数. 

![启动虚拟机](images/62.png)  

在客户机中用一些工具查看网卡相关的信息如下(这里使用了用户模式的网络栈, 其详细介绍可参考5.4节), 由此可知上面的网络设置都已生效. 

![网卡信息](images/63.png)  

在QEMU Monitor中查看网卡信息, 如下: 

![网络信息](images/64.png)  

接下来介绍各个网络工作模式的原理和配置方法. 

5.2 网桥模式

在QEMU/KVM的网络使用中, 网桥(bridge)模式可以让客户机和宿主机共享一个物理网络设备连接网络, 客户机有自己的独立IP地址, 可以直接连接与宿主机一模一样的网络, 客户机可以访问外部网络, 外部网络也可以直接访问客户机(就像访问普通物理主机一样). 即使宿主机只有一个网卡设备, 使用bridge模式也可知让多个客户机与宿主机共享网络设备, bridge模式使用非常方便, 应用也非常广泛. 

在qemu-kvm的命令行中, 关于bridge模式的网络参数如下: 

-nettap[,vlan=n][,name=str][,fd=h][,ifname=name][,script=file][,downscript=dfile][,helper=helper][,sndbuf=nbytes][,vnet_hdr=on|off][,vhost=on|off][,vhostfd=h][,vhostforce=on|off]

该配置表示连接宿主机的TAP网络接口到n号VLAN中, 并且使用file和dfile两个脚本在启动客户机时配置网络和在关闭客户机时取消网络配置. 

tap参数, 表明使用TAP设备. TAP是虚拟网络设备, 它仿真了一个数据链路层设备(ISO七层网络结构的第二层), 它像以太网的数据帧一样处理第二层数据报. 而TUN与TAP类似, 也是一种虚拟网络设备, 它是对网络层设备的仿真. TAP用于创建一个网络桥, 而TUN与路由相关. 

vlan=n, 设置该设备VLAN编号, 默认值为0. 

name=name, 设置名称, 在QEMU monior中可能用到, 一般由系统自动分配即可. 

fd=h, 连接到现在已经打开着的TAP接口的文件描述符, 一般不要设置该选项, 而是让QEMU自动创建一个TAP接口. 在使用了fd=h的选项后, ifname、script、downscript、helper、vnet_hdr等选项都不可使用了(不能与fd选项同时出现在命令行中). 

ifname=name, 设置在宿主机中添加的TAP虚拟设备的名称(如tap1、tap5等), 当不设置这个参数时, QEMU会根据系统中目前的情况, 产生一个TAP接口的名称. 

script=file, 设置宿主机在启动客户机时自动执行的网络配置脚本. 如果不指定, 其默认值为"/etc/qemu-ifup"这个脚本, 可指定自己的脚本路径以取代默认值; 如果不需要执行脚本, 则设置为"script=no". 

downscript=dfile, 设置宿主机在客户机关闭时自动执行的网络配置脚本. 如果不设置, 其默认值为"/etc/qemu-ifdown"; 若客户机关闭时宿主机不需要执行脚本, 则设置为"downscript=no". 

helper=helper, 设置启动客户机时在宿主机中运行的辅助程序, 包括建立一个TAP虚拟设备, 默认值为/usr/local/libexec/qemu-bridge-helper. 一般不用自定义, 采用默认值即可. 

sndbuf=nbytes, 限制TAP设备的发送缓冲区大小为n字节, 当需要流量进行流量控制时可以设置该选项. 其默认值为"sndbuf=0", 即不限制发送缓冲区的大小. 

其余几个选项都是与virtio相关的. 

上面介绍了使用TAP设备的一些选项, 接下来通过在宿主机中执行如下步骤来实现网桥方式的网络配置. 

1)要采用bridge模式的网络配置, 首先需要安装两个RPM包, 即bridge-utils和tunctl, 它们提供所需的brctl和tunctl命令行工具. 可以用yum工具安装这两个RPM包

yum install bridge-utils tunctl

2)查看tun模块是否加载

lsmod | grep tun

如果tun模块没有加载, 则运行"modprobe tun"命令来加载. 当然, 如果已经将tun编译到内核(可查看内核config文件中是否有"CONFIG_TUN=y"选项), 则不需要加载了. 如果内核完全没有配置TUN模块, 则需要重新编译内核才行. 

3)检查/dev/net/tun的权限, 需要让当前用户拥有可读写的权限. 

ll /dev/net/tun

4)建立一个bridge, 并将其绑定到一个可以正常工作的网络接口上, 同时让bridge成为连接本机与外部网络的接口. 主要的配置命令如下: 

![配置bridge](images/65.png)  

建立bridge后的状态是让网络接口eth0进入混杂模式(promiscuous mode, 接收网络中所有数据包), 网桥br0进入转发状态(forwarding state), 并且与eth0有相同的MAC地址, 一般也会得到和eth0相同的IP. "brctl stp br0 on"是打开br0的STP协议, STP是生成树协议(Spanning Tree Protocol), 在这里使用STP主要是为了避免在建有bridge的以太网LAN中出现环路. 如果不打开STP, 则可能出现数据链路层的环路, 从而导致建有bridge的主机网络不畅通. 

这里默认通过DHCP方式动态获得IP. 在绑定了bridge之后, 也可以使用"ifconfig"和"route"等命令设置br0的IP、网关、默认路由等, 需要将bridge设置为与其绑定的物理网络接口一样的IP和MAC地址, 并且让默认路由使用bridge(而不是ethX)来连通. 这个步骤可能导致宿主机的网络断掉, 之后重新通过bridge建立网络连接, 因此建立bridge这一步骤最好不要通过SSH连接远程配置. 另外, 在RHEL系列系统中最好将NetworkManager这个程序结束掉, 因为它并不能管理bridge的网络配置, 相反, 它在后台运行则可能对网络设置有些干扰. 

5)准备qemu-ifup和qemu-ifdown脚本. 

在客户机启动网络前会执行的脚本是由"script"选项配置的(默认为/etc/qemuif-up). 一般在该脚本中创建一个TAP设备并将其与bridge绑定起来. 如下是qemu-ifup脚本的示例, 其中"$1"是qemu-kvm命令工具传递给脚本的参数, 它是客户机使用的TAP设备名称(如tap0、tap1等, 或者是前面提及的ifname选项的值). 另外, 其中的"tunctl"命令这一行是不需要的, 因为qemu-bridge-helper程序已经会创建好TAP设备, 这里列出来只是因为在一些版本较旧的qemu-kvm中可能没有自动创建TAP设备. 

![script](images/66.png)  

由于qemu-kvm工具在客户机关闭时会解除TAP设备的bridge绑定, 也会自动删除已不再使用的TAP设备, 所以qemu-ifdown这个脚本不是必需的, 最好设置为"downscript=no". 如下列出一个qemu-ifdown脚本的示例, 是为了说明清理bridge模式网络环境的步骤, 在qemu-kvm没有自动处理时可以使用. 

![qemu-ifdown](images/67.png)  

6)用qemu-kvm命令启动bridge模式的网络. 

在宿主机中, 用命令行启动客户机并检查bridge的状态, 如下: 

![启动并检查bridge](images/68.png)  

由上面信息可知, 在创建客户机后, 添加了一个名为tap1的TAP虚拟网络设备, 将其绑定在br0这个bridge上. 查看到的3个虚拟网络设备依次为: 网络回路设备lo(就是一般IP为127.0.0.1的设备)、前面建立好的bridge设备br0、为客户机提供网络的TAP设备tap1. 

在客户机中, 如下的几个命令用于检查网络是否配置好. 

![客户机检查网络](images/69.png)  

然而, 在将客户机关机后, 在宿主机中再次查看bridge状态和虚拟网络设备的状态, 如下: 

![宿主机检查网络](images/70.png)  

上面信息可以看到, qemu-kvm工具已经将tap1设备删掉了. 

5.3 使用NAT模式

NAT(Network Addresss Translation , 网络地址转换), 属于广域网接入技术的一种, 它将内网地址转化为外网的合法IP地址, 它被广泛应用于各种类型的Internet接入方式和各种类型的网络之中. NAT将来自内网IP数据包的包头中的源IP地址转换为一个外网的IP地址. 众所周知, IPv4的地址资源已几近枯竭, 而NAT使内网的多个主机可以共用一个IP地址接入网络, 这样有助于节约IP地址资源, 这也是NAT最主要的作用. 另外, 通过NAT访问外部网络的内部主机, 其内部IP对外是不可见的, 这就隐藏了NAT内部网络拓扑结构和IP信息, 也就能够避免内部主机受到外部网络的攻击. 客观事物总是有正反两面性的, 没有任何技术是十全十美的. NAT技术隐藏了内部主机细节从而提高了安全性, 但是如果NAT内的主机作为Web或数据库服务器需接受来自外部网络的主动连接, 这时NAT就表现出了局限性, 不过, 可以在拥有外网IP的主机上使用iptables等工具实现端口映射, 从而让外网对这个外网IP的一个端口的访问被重新映射到NAT内网的某个主机的相应端口上去. 

在QEMU/KVM中, 默认使用IP伪装的方式去实现NAT, 而不是使用SNAT(Source-NAT)或DNAT(Destination-NAT)的方式. 图4-6展示了KVM中的NAT模式网络的结构图, 宿主机在外网的IP是10.10.10.190, 其上运行的各个客户机的IP属于内网的网络段192.168.122.0/24. 

![KVM的NAT模式网络](images/71.png)

在KVM中配置客户机的NAT网络方式, 需要在宿主机中运行一个DHCP服务器给宿主机分配NAT内网的IP地址, 可以使用dnsmasq工具来实现. 在KVM中, DHCP服务器为客户机提供服务的基本架构如图所示. 

![宿主机中的dnsmasq为客户机提供DHCP服务](images/72.png)

通过下面几步可以使客户机启动并以NAT方式配置好它的网络. 

1) 检查配置宿主机内核编译的配置, 将网络配置选项中与NAT相关的选项配置好, 否则在启动客户机使用NAT网络配置时可能会遇到如下错误提示, 因为无法按需加载"iptable_nat"和"nf_nat"等模块. 

iptables v1.4.7: can't initialize iptables table `nat': Table does not exist (do you need to insmod?)

遇到这样的情况, 只能重新配置和编译内核了. 下面截取的一小段内核配置, 是一般情况下NAT的部分相关配置. 

```
#
# IP: Netfilter Configuration
#
CONFIG_NF_DEFRAG_IPV4=m
CONFIG_NF_CONNTRACK_IPV4=m
CONFIG_NF_CONNTRACK_PROC_COMPAT=y
CONFIG_IP_NF_QUEUE=m
CONFIG_IP_NF_IPTABLES=m
CONFIG_IP_NF_MATCH_AH=m
CONFIG_IP_NF_MATCH_ECN=m
CONFIG_IP_NF_MATCH_RPFILTER=m
CONFIG_IP_NF_MATCH_TTL=m
CONFIG_IP_NF_FILTER=m
CONFIG_IP_NF_TARGET_REJECT=m
CONFIG_IP_NF_TARGET_ULOG=m
CONFIG_NF_NAT=m
CONFIG_NF_NAT_NEEDED=y
CONFIG_IP_NF_TARGET_MASQUERADE=m
CONFIG_IP_NF_TARGET_NETMAP=m
CONFIG_IP_NF_TARGET_REDIRECT=m
```

2)	安装必要的软件包: bridge-utils、iptables和dnsmasq等. 其中bridge-utils包含管理bridge的工具brctl(之前已使用过), iptables是对内核网络协议栈中IPv4包的过滤工具和NAT管理工具, dnsmasq是一个轻量级的DHCP和DNS服务器软件. 当然, 如有其他满足类似功能的软件包, 也可以选用. 在宿主机中, 查看所需软件包情况, 如下: 

```
[root@jay-linux kvm_demo]# rpm -qa | grep bridge
bridge-utils-1.2-9.el6.x86_64
[root@jay-linux kvm_demo]# rpm -qa | grep iptables
iptables-ipv6-1.4.7-4.el6.x86_64
iptables-1.4.7-4.el6.x86_64
[root@jay-linux kvm_demo]# rpm -qa | grep dnsmasq
dnsmasq-2.48-5.el6.x86_64
```

3)	准备一个为客户机建立NAT用的qemu-ifup脚本及关闭网络用的qemu-ifdown脚本. 这两个脚本中的$1(传递给它们的第一个参数)就是在客户机中使用的网络接口在宿主机中的虚拟网络名称(如tap0、tap1等). 

其中, 在启动客户机时建立网络的脚本示例(/etc/qemu-ifup-NAT)如下, 主要功能是: 建立bridge, 设置bridge的内网IP(此处为192.168.122.1), 并且将客户机的网络接口与其绑定, 然后打开系统中网络IP包转发的功能, 设置iptables的NAT规则, 最后启动dnsmasq作为一个简单的DHCP服务器. 

```
#!/bin/bash
# qemu-ifup script for QEMU/KVM with NAT netowrk mode

# set your bridge name
BRIDGE=virbr0

# Network information
NETWORK=192.168.122.0
NETMASK=255.255.255.0
# GATEWAY for internal guests is the bridge in host
GATEWAY=192.168.122.1
DHCPRANGE=192.168.122.2,192.168.122.254

# Optionally parameters to enable PXE support
TFTPROOT=
BOOTP=

function check_bridge()
{
        if brctl show | grep "^$BRIDGE" &> /dev/null; then
                return 1
        else
                return 0
        fi
}

function create_bridge()
{
            brctl addbr "$BRIDGE"
            brctl stp "$BRIDGE" on
            brctl setfd "$BRIDGE" 0
            ifconfig "$BRIDGE" "$GATEWAY" netmask "$NETMASK" up
}

function enable_ip_forward()
{
        echo 1 > /proc/sys/net/ipv4/ip_forward
}

function add_filter_rules()
{
        iptables -t nat -A POSTROUTING -s "$NETWORK"/"$NETMASK" \
                ! -d "$NETWORK"/"$NETMASK" -j MASQUERADE
}

function start_dnsmasq()
{
        # don't run dnsmasq repeatedly
        ps -ef | grep "dnsmasq" | grep -v "grep" &> /dev/null
        if [ $? -eq 0 ]; then
                echo "Warning:dnsmasq is already running."
                return 1
        fi

        dnsmasq \
                --strict-order \
                --except-interface=lo \
                --interface=$BRIDGE \
                --listen-address=$GATEWAY \
                --bind-interfaces \
                --dhcp-range=$DHCPRANGE \
                --conf-file="" \
                --pid-file=/var/run/qemu-dhcp-$BRIDGE.pid \
                --dhcp-leasefile=/var/run/qemu-dhcp-$BRIDGE.leases \
                --dhcp-no-override \
                ${TFTPROOT:+"--enable-tftp"} \
                ${TFTPROOT:+"--tftp-root=$TFTPROOT"} \
                ${BOOTP:+"--dhcp-boot=$BOOTP"}
}

function setup_bridge_nat()
{
        check_bridge "$BRIDGE"
        if [ $? -eq 0 ]; then
                create_bridge
        fi
        enable_ip_forward
        add_filter_rules "$BRIDGE"
        start_dnsmasq "$BRIDGE"
}

# need to check $1 arg before setup
if [ -n "$1" ]; then
        setup_bridge_nat
        ifconfig "$1" 0.0.0.0 up
        brctl addif "$BRIDGE" "$1"
        exit 0
else
        echo "Error: no interface specified."
        exit 1
fi
```

关闭客户机时调用的网络脚本示例(/etc/qemu-ifdown-NAT)如下, 它主要完成解除bridge绑定、删除bridge和清空iptalbes的NAT规则. 

```
#!/bin/bash
# qemu-ifdown script for QEMU/KVM with NAT network mode

# set your bridge name
BRIDGE="virbr0"

if [ -n "$1" ]; then
        echo "Tearing down network bridge for $1" 
        ip link set $1 down
        brctl delif "$BRIDGE" $1
        ip link set "$BRIDGE" down
        brctl delbr "$BRIDGE"
        iptables -t nat -F
        exit 0
else
        echo "Error: no interface specified" 
        exit 1
fi
```

4)	当启动客户机时, 使用上面提到的启动脚本. 创建客户机的qemu-kvm命令行如下: 

qemu-system-x86_64 rhel6u3.img -m 1024 -smp 2 -net nic -net tap,script=/etc/qemu-ifup-NAT,downscript=/etc/qemu-ifdown-NAT

在启动客户机后, 检查脚本中描述的宿主机中的各种配置生效的情况, 如下: 

```
[root@jay-linux kvm_demo]# brctl show
bridge name     bridge id               STP enabled     interfaces
br0             8000.60eb692129b7       yes              eth0
virbr0          8000.4a826f3a76d5       yes             tap0
#注意区别这两个bridge的不同: 
#br0是前面提到的网桥模式使用的, 它与一个物理上的网络接口eth0绑定, 
#virbr0是这里介绍的NAT方式的bridge, 它没有绑定任何物理网络接口, 只是绑定了tap0这个客户机使用的虚拟网络接口
```

```
[root@jay-linux kvm_demo]# iptables -t nat -L
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  192.168.122.0/24    !192.168.122.0/24

[root@jay-linux ~]# ifconfig  virbr0
virbr0    Link encap:Ethernet  HWaddr 4A:82:6F:3A:76:D5
          inet addr:192.168.122.1  Bcast:192.168.122.255  Mask:255.255.255.0
          inet6 addr: fe80::4882:6fff:fe3a:76d5/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:59 errors:0 dropped:0 overruns:0 frame:0
          TX packets:49 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:7537 (7.3 KiB)  TX bytes:6471 (6.3 KiB)
[root@jay-linux ~]# ps -ef | grep dnsmasq | grep -v grep
nobody   22507     1  0 Jul27 ?        00:00:08 dnsmasq --strict-order --except-interface=lo --interface=virbr0 --listen-address=192.168.122.1 --bind-interfaces --dhcp-range=192.168.122.2,192.168.122.254 --conf-file= --pid-file=/var/run/qemu-dhcp-virbr0.pid --dhcp-leasefile=/var/run/qemu-dhcp-virbr0.leases --dhcp-no-override
```

5)	在客户机中, 通过DHCP动态获得IP, 并且检查网络是否畅通, 如下: 

```
[root@kvm-guest ~]# dhclient eth0
eth0: link up, 100Mbps, full-duplex, lpa 0x05E1
[root@kvm-guest ~]# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 52:54:00:12:34:56
          inet addr:192.168.122.140  Bcast:192.168.122.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fe12:3456/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:112 errors:0 dropped:0 overruns:0 frame:0
          TX packets:108 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:12073 (11.7 KiB)  TX bytes:18654 (18.2 KiB)
          Interrupt:11 Base address:0xa000
[root@kvm-guest ~]# route
Kernel IP routing table
Destination     Gateway     Genmask       Flags Metric Ref    Use Iface
192.168.122.0   *           255.255.255.0   U     0      0       0 eth0
default     192.168.122.1   0.0.0.0         UG    0      0       0 eth0
[root@kvm-guest ~]# ping 192.168.122.1 -c 1
PING 192.168.122.1 (192.168.122.1) 56(84) bytes of data.
64 bytes from 192.168.122.1: icmp_seq=1 ttl=64 time=0.373 ms

--- 192.168.122.1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.373/0.373/0.373/0.000 ms
[root@kvm-guest ~]# ping 192.168.199.103 -c 1
PING 192.168.199.103 (192.168.199.103) 56(84) bytes of data.
64 bytes from 192.168.199.103: icmp_seq=1 ttl=63 time=0.947 ms

--- 192.168.199.103 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 1ms
rtt min/avg/max/mdev = 0.947/0.947/0.947/0.000 ms
```

从上面的命令行输出可知, 客户机可以通过DHCP获得网络IP(192.168.122.0/24子网中), 其默认网关是宿主机的bridge的IP(192.168.122.1), 并且可以ping通网关(192.168.122.1)和子网外的另外一个主机(192.168.199.103), 说明其与外部网络的连接正常. 

另外, 客户机中的DNS服务器默认配置为宿主机(192.168.122.1), 如果宿主机没有启动DNS服务, 则可能导致在客户机中无法解析域名. 这时需要将客户机中/etc/resolv.conf修改为与宿主机中一致可用的DNS配置, 然后就可以正常解析外部的域名(主机名)了, 如下: 

```
[root@kvm-guest ~]# vi /etc/resolv.conf
[root@kvm-guest ~]# cat /etc/resolv.conf
; generated by /sbin/dhclient-script
search tsp.org
nameserver 192.168.199.3
[root@kvm-guest ~]# nslookup vt-snb9
Server:         192.168.199.3
Address:        192.168.199.3#53

Name:   vt-snb9.tsp.org
Address: 192.168.199.99
[root@kvm-guest ~]# ping vt-snb9 -c 1
PING vt-snb9.tsp.org (192.168.199.99) 56(84) bytes of data.
64 bytes from 192.168.199.99: icmp_seq=1 ttl=63 time=0.741 ms

--- vt-snb9.tsp.org ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 2ms
rtt min/avg/max/mdev = 0.741/0.741/0.741/0.000 ms
```

6)	添加iptables规则进行端口映射, 让外网主机也能访问客户机. 

到步骤5)为止, 客户机已可以正常连通外部网络, 但是外部网络(除宿主机外)无法直接连接到客户机. 其中一个解决方案是, 在宿主机中设置iptables的规则进行端口映射, 使外部主机对宿主机IP的一个端口的请求转发到客户机中的某一个端口. 

在宿主机中, 查看网络配置情况, 然后iptables设置端口映射将如下, 将宿主机的80端口(常用于HTTP服务)映射到客户机的80端口. 

```
[root@jay-linux kvm_demo]# route
Kernel IP routing table
Destination     Gateway     Genmask     Flags Metric Ref    Use Iface
default     sqa-gate.tsp.or 0.0.0.0      UG    0      0        0 br0
192.168.0.0     *          255.255.0.0     U     0      0        0 br0
192.168.122.0   *         255.255.255.0   U     0      0        0 virbr0
[root@jay-linux kvm_demo]# ifconfig br0
br0       Link encap:Ethernet  HWaddr 60:EB:69:21:29:B7
          inet addr:192.168.82.0  Bcast:192.168.255.255  Mask:255.255.0.0
          inet6 addr: fe80::ccc7:84ff:fe41:280c/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:608315 errors:0 dropped:0 overruns:0 frame:0
          TX packets:49539 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:72662298 (69.2 MiB)  TX bytes:79644268 (75.9 MiB)
[root@jay-linux kvm_demo]# iptables -t nat -A PREROUTING -p tcp –d \ 192.168.82.0 --dport 80 -j DNAT --to 192.168.122.140:80
[root@jay-linux kvm_demo]# iptables -t nat -L
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
DNAT       tcp  --  anywhere             192.168.82.0        tcp dpt:http to:192.168.122.140:80

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  192.168.122.0/24    !192.168.122.0/24
```

在客户机中, 编辑一个在HTTP服务中被访问的示例文件(/var/www/html/index.html, Apache默认根目录为/var/www/html), 然后启动Apache服务. 

```
[root@kvm-guest ~]# cat /var/www/html/index.html
This an index page demo running on apache in kvm-guest.
[root@kvm-guest ~]# service httpd start
Starting httpd: httpd: apr_sockaddr_info_get() failed for kvm-guest
httpd: Could not reliably determine the server's fully qualified domain name, using 127.0.0.1 for ServerName
[ OK ]
```

在外部网络某主机上测试连接宿主机(192.168.82.0)的80端口, 就会被映射到客户机(192.168.122.140)中的80端口, 如图所示, 外部网络已经可以正常访问在NAT内网中的那台客户机80端口上的HTTP服务了. 

![外部网络访问](images/73.png)

上图外部网络主机访问宿主机而被映射到客户机中相应的端口
在上面的示例中, NAT的配置涉及的一些iptables配置规则仅用于实验演示, 在实际生产环境中需要根据实际情况进行更细粒度的配置, 如果将访问规则和数据包转发规则设置得过于宽松可能会带来网络安全方面的隐患. 

5.4 QEMU内部的用户模式网络

在没有任何"-net"参数时, qemu-kvm默认使用的是"-net nic -net user"的参数, 提供了一种用户模式(user-mode)的网络模拟. 使用用户模式的网络的客户机可以连通宿主机及外部的网络. 用户模式网络是完全由QEMU自身实现的, 不依赖于其他的工具(如前面提到的bridge-utils、dnsmasq、iptables等), 而且不需要root用户权限(前面介绍过的bridge模式和NAT模式在配置宿主机网络和设置iptables规则时一般都需要root用户权限). QEMU使用Slirp实现了一整套TCP/IP协议栈, 并且使用这个协议栈实现了一套虚拟的NAT网络. 

用户模式网路也有如下3个缺点: 

1)	由于其在QEMU内部实现所有网络协议栈, 因此其性能较差. 

2)	不支持部分网络功能(如ICMP), 所以不能在客户机中使用ping命令测试外网连通性. 

3)	不能从宿主机或外部网络直接访问客户机. 

使用用户模式的网络, 其qemu-kvm命令行参数为: 

-net user[,option][,option][,...]

其中常见的选项(option)及其意义如下: 

- vlan=n, 将用户模式网络栈连接到编号为n的VLAN中(默认值为0). 
- name=name, 分配一个在QEMU monitor中会用到的名字(如在monitor的"info network"命令中 可看到这个网卡的name). 
- net=addr[/mask], 设置客户机可以看到的IP地址(客户机所在子网), 其默认值是10.0.2.0/24. 其中, 子网掩码(mask)有两种形式可选, 一种是类似于255.255.255.0这样地址, 另一种是32位IP地址中前面被置位为1的位数(如10.0.2.0/24). 
- host=addr, 指定客户机可见宿主机的地址, 默认值为客户机所在网络的第2个IP地址(如10.0.2.2). 
- restrict=y|yes|n|no, 如果将此选项打开(为y或yes), 则客户机将会被隔离, 客户机不能与宿主机通信, 其IP数据包也不能通过宿主机而路由到外部网络中. 这个选项不会影响"hostfwd"显示地指定的转发规则, "hostfwd"选项始终会生效. 默认值为n或no, 不会隔离客户机. 
- hostname=name, 设置在宿主机DHCP服务器中保存的客户机主机名. 
- dhcpstart=addr, 设置能够分配给客户机的第一个IP, 在QEMU内嵌的DHCP服务器有16个IP地址可供分配. 在客户机中IP地址范围的默认值是子网中的第15到第30个IP地址(如10.0.2.15 ~ 10.0.2.30). 
- dns=addr, 指定虚拟DNS的地址, 这个地址必须与宿主机地址(在"host=addr"中指定的)不相同, 其默认值是网络中的第3个IP地址(如10.0.2.3). 
- tftp=dir, 激活QEMU内嵌的TFTP服务器, 目录dir是TFTP服务的根目录. 不过, 在客户机使用TFTP客户端连接TFTP服务后需要使用binary模式来操作. 
- hostfwd=[tcp|udp]:[hostaddr]:hostport-[guestaddr]:guestport, 将访问宿主机的hostpot端口的TCP/UDP连接重定向到客户机(IP为guestaddr)的guestport端口上. 如果没有设置guestaddr, 那么默认使用x.x.x.15(DHCP服务器可分配的第一个IP地址). 如果指定了hostaddr的值, 则可以根据宿主机上的一个特定网络接口的IP端口来重定向. 如果没有设置连接类型为TCP或UDP, 则默认使用TCP连接. "hostfwd=..."这个选项在一个命令行中可以多次重复使用. 
- guestfwd=[tcp]:server:port-dev, 将客户机中访问IP地址为server的port端口的连接转发到宿主机的dev这个字符设备上. "guestfwd=..."这个选项也可以在一个命令行中多次重复使用. 
- bootfile=file, 让file文件成为客户机可以使用的BOOTP[11]启动镜像文件. 它与"tfpt"选项联合使用, 可以实现从网络(使用本地目录中的文件)启动客户机的功能. 
- smb=dir[,smbserver=addr], 	激活Samba[12]服务器, 以便让Windows客户机很方便地透明地访问宿主机中的dir目录. addr设置了Samba服务器的IP地址, addr的默认值为用户模式网络中的第4个IP地址(如10.0.2.4). 值得注意的是, 该选项要求宿主机中安装有Samba服务器软件, 并且启动文件为"/usr/sbin/smbd"(当然这个smbd的路径可以在qemu-kvm编译时加上特定的配置使之变为用户特定的文件路径). 

下面用一个示例来介绍qemu-kvm中用户模式网络的使用. 

首先, 通过如下的命令行启动了一个客户机, 为它配置用户模式网络, 并且开启TFTP服务, 还将宿主机的5022端口转发到客户机的22端口(SSH服务默认端口), 将宿主机的5080端口转发到客户机的80端口(HTTP服务默认端口). 

```
qemu-system-x86_64 rhel6u3.img -m 1024 -smp 2 -net nic –net user,tftp=/root/tftp,hostfwd=tcp::5022-:22,hostfwd=tcp::5080-:80
```

然后, 在客户机中通过DHCP获得网络IP, 检查其路由状态和默认网关, 用ping命令来测试ICMP包的对外传输(如前面所说, ICMP在用户模式和网络中是不可用的), 再用ssh工具测试客户机与宿主机(网关)的连通性, 使用wget访问Google网站测试其外网网络连通性, 并且访问宿主机中的TFTP服务(在其中测试了下载文件), 还启动了客户机中的HTTP服务器. 

```
[root@kvm-guest ~]# dhclient eth0
eth0: link up, 100Mbps, full-duplex, lpa 0x05E1
[root@kvm-guest ~]# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 52:54:00:12:34:56
          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fe12:3456/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:218 errors:0 dropped:0 overruns:0 frame:0
          TX packets:203 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:20704 (20.2 KiB)  TX bytes:27661 (27.0 KiB)
          Interrupt:11
[root@kvm-guest ~]# route
Kernel IP routing table
Destination     Gateway    Genmask      Flags Metric Ref    Use Iface
10.0.2.0        *          255.255.255.0   U     0      0        0 eth0
default         10.0.2.2   0.0.0.0         UG    0      0        0 eth0
[root@kvm-guest ~]# ping vt-snb7 -c 1
PING vt-snb7.tsp.org (192.168.199.108) 56(84) bytes of data.

--- vt-snb7.tsp.org ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 10001ms
[root@kvm-guest ~]# ssh 10.0.2.2
The authenticity of host '10.0.2.2 (10.0.2.2)' can't be established.
RSA key fingerprint is 6b:69:03:c4:64:9d:c2:63:72:8b:34:eb:b5:c3:ad:f7.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '10.0.2.2' (RSA) to the list of known hosts.
root@10.0.2.2's password:
Last login: Mon Jul 30 23:50:19 2012 from 192.168.162.55
[root@jay-linux ~]# hostname
jay-linux
[root@jay-linux ~]# exit
logout
Connection to 10.0.2.2 closed.
[root@kvm-guest ~]# wget www.google.com
--2012-07-31 00:45:13--  http://www.google.com/
#......
awaiting response... 200 OK
Length: unspecified [text/html]
Saving to: "index.html"
   [ <=>           ] 14,405      --.-K/s   in 0.04s
2012-07-31 00:45:14 (387 KB/s) - "index.html" saved [14405]
[root@kvm-guest ~]# tftp 10.0.2.2
tftp> binary
tftp> verbose
Verbose mode on.
tftp> get tftp-demo.txt
getting from 10.0.2.2:tftp-demo.txt to tftp-demo.txt [octet]
Received 14 bytes in 0.0 seconds [230955 bit/s]
tftp> quit
[root@kvm-guest ~]#
[root@kvm-guest ~]# service httpd start
Starting httpd: httpd: apr_sockaddr_info_get() failed for kvm-guest
httpd: Could not reliably determine the server's fully qualified domain name, using 127.0.0.1 for ServerName
                                                           [  OK  ]
```   

最后, 在外网中另外一台主机上测试前面配置的宿主机对客户机的端口转发. 如下命令行是在某台主机上, 通过ssh连接到宿主机的5022端口(使用-p参数指定ssh连接的端口), 连接请求被自动转发到了客户机的22端口(ssh服务), 然后可以登录到客户机. 

```
[root@vt-snb9 ~]# ssh -p 5022 192.168.82.0
The authenticity of host '[192.168.82.0]:5022 ([192.168.82.0]:5022)' can't be established.
RSA key fingerprint is a8:9d:c7:c8:bc:7a:a3:f4:13:5f:d3:8d:08:1e:56:14.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[192.168.82.0]:5022' (RSA) to the list of known hosts.
root@192.168.82.0's password:
Last login: Tue Jul 31 00:15:36 2012
[root@kvm-guest ~]# hostname
kvm-guest
```

同样, 在外部网络的一个主机上通过浏览器对宿主机的5080端口的访问, 即被转发到了客户机的80端口, 浏览器中显示了在客户机的HTTP服务中测试网页内容, 如图. 

![在宿主机转发功能作用下, 外部网络得以访问用户模式网络中的客户机](images/74.png)

5.5 其他网络选项

(1) 使用TCP socket连接客户机的VLAN

-net socket[,vlan=n][,name=name][,fd=h][,listen=[host]:port][,connect=host:port]

使用TCP socket连接将n号VLAN连接到一个远程的qemu虚拟机的VLAN. 如果有"listen=···"参数, 那么QEMU会等待对port端口的连接, 而host参数是可选的(默认为127.0.0.1). 如果有"connect=···"参数, 则表示去连接远端的已经使用"listen"参数的QEMU实例. 可使用"fd=h"(文件描述符h)指定一个已经存在的TCP socket. 

(2) 使用UDP的多播socket建立客户机间的连接

(3) 使用VDE switch的网络连接

(4) 转存(dump)出VLAN中的网络数据

(5) 不分配任何网络设备

(6) 新的网络配置参数"-netdev"

6. 图形显示

本节主要主要介绍KVM中图形界面显示相关配置. 

6.1 SDL的使用

SDL(Simple DirectMedia Layer)是一个用C语言编写的、跨平台的、免费和开源的**多媒体程序库**, 它提供了一个简单的接口用于操作硬件平台的图形显示、声音、输入设备等. SDL库被广泛应用于各种操作系统(如Linux、FreeBSD、Windows、Mac OS、iOS、Android等)上的**游戏开发、多媒体播放器、模拟器**(如QEMU)等众多应用程序之中. 尽管SDL是用C语言编写的, 但是其他很多流行的编程语言都提供了SDL库的绑定. 

在QEMU模拟器中的图形显示默认就是使用SDL的. 当然, 需要在编译qemu-kvm时需要配置SDL的支持(./configure --enable-sdl), 之后才能编译SDL功能到QEMU的命令行工具中, 最后才能启动客户机时使用SDL的功能. 在编译qemu-kvm的系统中, 需要有SDL的开发包的支持. 如果有SDL-devel软件包, 在3.4.2节中配置QEMU时默认就会配置为提供SDL的支持, 通过运行configure程序, 在其输出信息中可以看到"SDL support   yes"即表明SDL支持将会被编译进去. 当然, 如果不想将SDL的支持编译进去, 在配置qemu-kvm时加上"–disable-sdl"的参数即可, configure输出信息中会显示提示"SDL support no". 

在运行QEMU命令行的系统中必须安装SDL软件包. 

SDL的功能很好用, 也比较强大, 不过它也有一个局限性, 就是在创建客户机并以SDL方式显示时, 它会直接弹出一个窗口, 所以SDL方式只能在图形界面中使用. 如果在非图形界面中(如ssh连接到宿主机中), 使用SDL时会出现如下的错误信息. 

[root@jay-linux kvm_demo]# qemu-system-x86_64 rhel6u3.img

Could not initialize SDL(No available video device) – exiting

qemu-kvm命令启动客户机时, 采用SDL方式, 其效果如图

![启动客户机使用SDL, 自动弹出客户机的显示窗口](images/75.jpg)

在使用SDL时, 如果将鼠标放入到客户机中进行操作会被完全抢占, 此时在宿主机中不能使用鼠标进行任何操作. QEMU默认使用"Ctrl+Alt"组合键来实现鼠标在客户机与宿主机中切换. 下图显示了客户机抢占了鼠标的使用场景, 在QEMU monitor上部边框中提示了按哪个组合键可以释放出鼠标. 

![SDL显示中客户机完全占用鼠标SDL显示中客户机完全占用鼠标](images/76.jpg)

使用SDL方式启动客户机时, 弹出的QEMU窗口是一个普通的窗口, 其右上角有最小化、最大化(或还原)和关闭窗口等功能. 其中, 如果点击了"关闭"按钮, 会将QEMU窗口关闭, 而同时客户机也被直接关闭了, QEMU进程会直接退出. 为了避免因误操作而关闭窗口从而让客户机直接推出的情况发生, QEMU命令行提供了"-no-quit"参数来去掉SDL窗口的直接关闭功能. 加了"-no-quit"参数后, SDL窗口中的"关闭"按钮的功能将会失效, 而最小化、最大化(或还原)等功能正常. 下图演示了启动客户机时加上了"-no-quit"参数时情况. 

![添加"-no-quit"参数后, SDL窗口中的关闭窗口功能失效](images/77.jpg)

6.2 VNC的使用

VNC(Virtual Network Computing)是图形化的桌面分享系统, 使用RFB(Remote FrameBuffer)协议来远程控制另外一台计算机系统. 它通过网络将控制端的键盘、鼠标的操作传递到远程受控计算机中, 而将远程计算机中的图形显示屏幕反向传输回控制端的VNC窗口中. VNC不依赖于操作系统. 

它克服了SDL的"只能在图形界面中使用"的局限性. VNC中操作在VNC窗口关闭或网络断开仍在服务端继续执行. 另外, 使用了VNC, 可以在服务端分别启动多个VNC Server进程. 

下面讲述在宿主机中直接使用VNC和在通过qemu-kvm命令行创建客户机时采用VNC方式的图形显示. 

1. 宿主机中的VNC使用
 
第一步, 在宿主机中安装VNC的服务器软件包(如tigervnc-server). 可用命令"rpm -qa | grep vnc"查询**vnc server**的安装情况. 

第二步, 设置宿主机中的安全策略, 使其允许VNC方式的访问, 主要需要设置防火墙和SELinux的安全策略. 这里直接关闭防火墙和SELinux. 

使用"setup"命令来设置或关闭防火墙. (对于CentO S, 可以使用"systemctl stop firewalld; systemctl disable firewalld")

对于SELinux的关闭, 可以采用三种方式: 

- 在运行时执行"setenforce 0"命令设置

![setenforce 0](images/78.png)

- 修改配置文件"/etc/selinux/config", 如下: 

![配置文件](images/79.png)

- 设置系统启动时GRUB配置的kernel命令参数, 加上"selinux=0"即可. grub.cfg配置文件中KVM启动条目示例如下: 

![grub.cfg](images/80.png)

第三步, 在宿主机中启动VNC服务端, 运行命令"vncserver :1", 即启动端口为5901(5900+1)的VNC远程着米娜的服务器, 如下. 

![vncserver](images/81.png)

第四步, 在客户端安装VNC的客户端软件. 在linux可以查询tigervnc的rpm包; 在windows可以安装RealVNC的VNC Viewer软件. 

第五步, 连接到远程宿主机服务器. 

2. QEMU使用VNC图形显示方式启动客户机
 
在qemu-kvm命令行中, 添加"-vnc display,option" 参数就能让VGA显示输出到VNC会话中而不是SDL中. 如果在进行qemu-kvm编译时没有SDL支持, 却有VNC的支持, 则qemu-kvm命令行在启动客户机时不需要"-vnc"参数也会自动使用VNC而不是SDL, 3.6命令启动客户机时提示"VNC server running on ::1:5900"就是这个原因. 

![远程连接到宿主机的1号VNC桌面](images/82.png)

在QEMU命令行的VNC参数中, display参数必不可少, 有三种值. 

(1) host:N

表示仅允许从host主机的N号显示窗口来建立TCP连接到客户机. 在通常情况下, QEMU会根据数字N建立对应的TCP端口, 其端口号为5900+N. 如果host为空, 则表示QEMU建立的Server端接受来自任何主机的连接. 

(2) unix:path

表示允许通过Unix domain socket来连接到客户机, path是一个处于监听状态的socket的位置路径. 此法用的不多. 

(3) none

表示VNC已经被初始化, 但是并不在开始时启动. 而在需要真正使用VNC之时, 可以在QEMU monitor中的change命令可以用来启动VNC连接. 

作为可选参数的option则有下面可选值, 每个option标志通过逗号隔开. 

(1) reverse

表示"反向"连接到一个处于监听中的VNC客户端, 这个客户端是由前面的display参数(host:N)来指定的. 需要注意的是, 在反向连接这种情况下, display中的端口号N是处于监听中的TCP端口, 而不是现实窗口编号, 即如果客户端(IP地址为IP_Demo)已经监听的命令为"vncviewer -listen :2", 则这里的VNC反向连接的参数为"-vnc IP_Demo:5902,reverse", 而不是用2这个编号. 

(2) password

表示在客户端连接时需要采取基于密码的认证机制, 但这里只是声明它使用密码验证, 具体密码值必须到QEMU monitor中用change命令设置. 

(3) "tls"、"x509=/path/to/certificate/dir"、"x509verify=/path/to/certificate/dir"、"sasl"和"acl"

都是和VNC验证安全相关. 

下面示例实践. 

准备两个系统, 一个是KVM的宿主机系统A(IP:192.168..185.229, 主机名: jay-linux), 另外一安装RHEL 6.3的Linux系统B(IP: 192.168.199.99), 这两个网络连通. 

示例1.

![示例一](images/83.png)

示例2.

![示例二](images/84.png)

![示例二](images/85.png)

示例3.

![示例三](images/86.png)

示例4.

![示例四](images/87.png)

示例5.

![示例五](images/88.png)

![示例五](images/89.png)

6.4 非图形模式

在qemu-kvm命令行中, 添加"-nographic"参数可以完全关闭QEMU的图形界面输出. 从而让QEMU在该模式下完全成为简单的命令行工具. 而QEMU中模拟产生的串口被重定向到了当前的控制台(console)中, 所以如果在客户机中对其内核进行配置从而让内核的控制台输出重定向到串口后, 就依然可以在非图形模式下管理客户机系统或调试客户机的内核. 

需要修改客户机的grub使其在kernel行加上将console输出重定向到串口ttyS0, 如下为一个客户机的修改后的grub配置文件. 

default=0

timeout=5

splashimage=(hd0,0)/boot/grub/splash.xpm.gz

hiddenmenu

title Red Hat Enterprise Linux (2.6.32-279.el6.x86_64)

root (hd0,0)

kernel /boot/vmlinuz-2.6.32-279.el6.x86_64 ro root=UUID=9a971721-db8f-4002c-a3f4-f4ae8b037ba3 3 console=ttyS0

initrd /boot/initramfs-2.6.32-279.el6.x86_64.img

用"-nographic"参数关闭图形输出, 其启动命令行及客户机启动(并登录进入客户机)的过程如下所示, 可见内核启动的信息就通过重定向到串口从而输出在当前的终端之中, 而且可以通过串口这里登录到客户机系统(有的客户机Linux系统需要进行额外的设置才能允从串口登录). 

[root@jay-linux kvm_demo]# **qemu-system-x86_64 rhel6u3.img -m 1024 -nographic**

Initializing cgroup subsys cpuset

Initializing cgroup subsys cpu

Linux version 2.6.32-278.el6.x86_64 (mockbuild@x86-010.build.bos.redhat.com) (gcc version 4.4.6 20120305 (Red Hat 4.4.6-4) (GCC) ) #1 SMP Fri Jun 1 14:17:12 EDT 2012

Command line: ro root=UUID=9a971721-db8f-402c-a3f4-f4ae8b037ba3 3 console=ttyS0

KERNEL supported cpus:

Intel GenuineIntel

AMD AuthenticAMD

Centaur CentaurHauls

BIOS-provided physical RAM map:

BIOS-e820: 0000000000000000 – 000000000009f400 (usable)

BIOS-e820: 000000000009f400 – 00000000000a0000 (reserved)

BIOS-e820: 00000000000f0000 – 0000000000100000 (reserved)

<!–此处省略数百行 启动时的串口输出信息–>

Starting postfix: [  OK  ]

Starting abrt daemon: [  OK  ]

Starting ksm: [  OK  ]

Starting ksmtuned: [  OK  ]

Starting crond: [  OK  ]

Starting atd: [  OK  ]

Starting rhsmcertd 240 1440[  OK  ]

Starting certmonger: [  OK  ]

Red Hat Enterprise Linux Server release 6.3 (Santiago)

Kernel 2.6.32-278.el6.x86_64 on an x86_64

 

kvm-guest login: root      #这里就是客户机的登录界面了

Password:

Last login: Tue Jul 17 15:56:21 on ttyS0

[root@kvm-guest ~]# pwd

/root

[root@kvm-guest ~]# date

Tue Jul 17 16:01:03 CST 2012

[root@kvm-guest ~]#

6.5 显示相关的其他选项

QEMU还有关于图形显示其他选项, 介绍几个比较有用的. 

1. -curses

让QEMU将VGA显示输出到使用curses/ncurses接口支持的文本模式界面, 而不是使用SDL来显示客户机. 与"-nographic"模式相比, 它的好处在于, 由于它是接受客户机VGA的正常输出而不是串口的输出信息, 因此不需要额外更改客户机配置将控制台重定向到串口. 当然, 为使用"-curses"选项, 在宿主机中必须有"curses或ncurses"这样的软件包. 

在宿主机添加"-curses"参数的qemu-kvm命令行来启动客户机, 启动到登录界面如图. 

![通过"ncurses"显示的文本模式下的客户机启动命令行和登录界面](images/90.png)

2. -vga type

选择为客户机模拟的VGA卡的类别, 可选类别有如下4种. 

(1) cirrus

为客户机模拟出"Cirrus Logic GD5446"显卡, 在客户机启动后, 可以在客户机中看到VGA卡的型号, 如在Linux中可用"lspci"查看到VGA卡信息. 这个选项对图形显示不是很优秀, 不过绝大多数系统都支持这个系列的显卡, 所以这种类型是目前QEMU对VGA的默认类型. 

在客户机查看VGA卡的类型: 

[root@kvm-guest ~]# lspci | grep VGA  
00:02.0 VGA compatible controller: Cirrus Logic GD 5446

(2) std

模拟标准的VGA卡, 带有Bochs VBE扩展. 当客户机支持VBE2.0及以上标准时, 如果需要支持更高的分辨率和彩色显示深度, 就需要该选项. 

(3)vmware

提供对"VMWare SVGA-II"兼容显卡的支持. 

(4) none

关闭VGA卡, 使SDL或VNC窗口中无任何显示, 一般不用该参数. 

3. -no-frame

使SDL显示时没有边框. 

4. -full-screen

在启动客户机时, 就自动使用全屏显示. 

5. -alt-grab

使用"Ctrl+Alt+Shift"组合键去抢占和释放鼠标. 从而"Ctrl+Alt—+Shift"组合键也会成为QEMU中一个特殊功能键. 在QEMU中默认使用"Ctrl+Alt"组合键, 所以之前在SDL或VNC中用"Ctrl+Alt+2"
组合键切换到QEMU monitor的窗口, 而使用了"-alt-grab"选项后, 应该相应用"Ctrl+Alt+Shift+2"切换到QEMU monitor. 

6. -ctrl-grab
 
使用"右Ctrl"键去抢占和释放鼠标, 使其成为QEMU中特殊功能键, 这与前面的"-alt-grab"功能类似. 

4.6 小结

本章主要介绍了QEMU/KVM中关于CPU、内存、磁盘、网络、图形显示等计算机系统的最核心最基本部件的简单原理、详细配置及实践操作, 同时还在其中提及了一些命令行工具(如ps、brctl、lspci、fdisk等等)和展示了几个配置脚本(如查看CPU信息的cpu-info.sh、建立网络链接的qemu-ifup等等). 相信通过阅读本章的内容, 你已经可以创建自己的客户机, 并且可以配置成功其中的CPU、内存、磁盘、网络等基本部件从而满足普通的应用需求. 而在下一章中, 将会介绍KVM中一些提高性能的特性(如virtio、VT-d等)、其他高级功能(如KSM、动态迁移、热插拔、嵌套虚拟化等)以及一些并不太常用的特性(如SMEP功能、AVX、AES新指令等), 让大家对KVM的各项功能有更完整的了解, 并能根据实际情况选择适当的优化方法. 

4.7 本章注释和参考阅读

本章注释

[1] Romley是Intel的一个平台的代号, 在它里面配置有SandyBridge-EP的CPU和Patsburg芯片组, 这里提及的Romley-EP 4S是有4个CPU socket的系统. 关于Intel各种CPU、芯片组、网卡、主板等硬件的代号, 可参见: http://en.wikipedia.org/wiki/List_of_Intel_codenames . 

[2] CPU支持了超线程(HT)技术, 还需要在BIOS中打开它的开关才能使用. 在BIOS中, 超线程的设置可能会在"Advanced à CPU Configuration"下设置, 通常标识为"Hyper-Threading". 另外, 直到2012年作者写本书时, AMD还没提供Hyper-Threading的支持(AMD目前走是mult-core的路线), 而Intel是mult-core和hyper-threading一起使用. 

[3] NUMA(Non-Uniform Memory Access 非一致性内存访问)是一种在多处理系统中的内存设计架构, 在多处理器中CPU访问系统上各个物理内存的速度可能是不一样的, 一个CPU访问其本地内存的速度比访问(同一系统上)其他CPU对应的本地内存快一些. 

[4] Nehaem是Intel的第二代45nm微处理器架构, 如下的网址可以让你了解更多关于Nehalem平台的知识 http://www.realworldtech.com/nehalem/ . 

[5] TLB(translation lookaside buffer, 旁路转换缓冲)是内存管理硬件用以提高虚拟地址转换速度的缓存. TLB是页表(page table)的缓存, 保存了一部分页表. 关于TLB的更多信息, 可以查看wikipedia中的介绍http://en.wikipedia.org/wiki/Translation_lookaside_buffer . 

[6] sysfs是一个虚拟的文件系统, 它存在于内存之中, 它将Linux系统中设备和驱动的信息从内核导出到了用户空间. sysfs也用于配置当前系统, 此时其作用类似于sysctl命令. systfs通常是挂载在/sys目录的, 通过"mount"命令可以查看到sysfs的挂载情况. 

[7] 关于swap space, Redhat公司的文档曾有这样的推荐: 在其RHEL5.x或RHEL6.x系列的Linux发行版中, 物理内存小于4GB时, 推荐不少于2GB的交换空间; 物理内存4GB~16GB, 推荐不少于4GB的交换空间; 物理内存16GB~64GB, 推荐不少于8GB的交换空间; 内存64GB~256GB, 推荐不少于16GB的交换空间. 

[8] 一个QEMU bug: Guest boot failed when the drive interface is scsi

https://bugs.launchpad.net/qemu/+bug/485251

[9] 稀疏文件(sparse file), 是计算机系统块设备中能有效利用磁盘空间的文件类型, 它用元数据(metadata)中的简要描述来标识哪些块是空的, 只有在空间被实际数据占用时, 才实际写到磁盘中. 可以参考wikipedia上的"sparse file"词条, http://en.wikipedia.org/wiki/Sparse_file . 

[10] 在QEMU的源代码中有一个专门的"slirp"目录使用了Slirp的实现. Slirp是通过普通的终端模拟PPP、SLIP等连接到Internet的开源软件程序, 更多信息请参考其官方网站: http://slirp.sourceforge.net/. 

[11] BOOTP即Bootstrap Protocol, 是一种比较早出现的远程启动协议, 后来DHCP协议实现了BOOTP几乎所有的功能(再加了很多自有功能), 已基本把BOOTP取代了, 可参考如下wikipedia上的描述http://en.wikipedia.org/wiki/Bootstrap_Protocol . 

[12] Samba是类Unix(包括Linux)系统与Windows系统进行相互操作的标准软件, 主要用于文件的跨系统共享. 其官方主页为: http://www.samba.org/ . 

[13] Use-Mode Linux是一种安全地运行Linux各个版(单独的kernel)和Linux进程的方式. 该项目主页为: http://user-mode-linux.sourceforge.net/ . 

[14] VDE(Virtual Distributed Ethernet), 详情参考其项目主页: http://vde.sourceforge.net/ . 

[15] 对于QEMU获取和释放鼠标的组合键(如"Alt+Ctrl"), 一般来说是指位于键盘左边的键而不是键盘右边的键, 而需要提到键盘右边的Ctrl、Alt、Shift键时, 一般都会特别指明是右边的键(如"右Ctrl键"). 

[16] VNC一般采用TCP端口5900+N(N为vncserver启动时指定的桌面端口号)作为服务端的端口, 在VNC Viewer远程连接时, 既可以用5900+N(如5901)这样的端口, 也可以直接用桌面号N来连接, 一般情况下VNC Viewer程序会自适应地做相应的转换. 

[17] 关于VNC的安全问题及其设置, 如果对安全性要求很高, 请参考如下网页链接: http://wiki.qemu.org/download/qemu-doc.html#vnc_005fsecurity

[18] 了解更多关于Unix Domain Socket的内容, 可以参考如下链接: 

http://beej.us/guide/bgipc/output/html/multipage/unixsock.html

[19] VGA(Video Graphics Array)是IBM于1987年在其计算机中引入的显示硬件接口, 后来也成为事实上的工业标准, 目前绝大多数多数的PC和x86服务器都支持VGA接口及其显示标准. 

[20] curses是类Unix系统(包括: Unix、Linux、BSD等等)上终端控制的库, 应用程序能够基于它去构建文本模式下的用户界面. 而ncures是指"new curses", 发布于1993年, 是目前curses最著名的实现, 也是在目前类Unix系统中使用非常广泛的. 

[21] VBE(VESA BIOS Extensions)是VESA的一个标准, 目前版本是3.0, 请参考如下链接获取关于"VBE"的更多信息, http://en.wikipedia.org/wiki/VESA_BIOS_Extensions. 

参考阅读: 

[a] 关于Sandy Bridge处理器架构: http://www.realworldtech.com/sandy-bridge/

[b] 关于huge page的介绍: http://lwn.net/Articles/374424/

[c] 关于QEMU的网络, 也可参考: http://people.gnome.org/~markmc/qemu-networking.html

[d] 最好不要在qcow2格式做镜像文件时使用"aio=native", Redhat对其有说明, 如下: 

https://access.redhat.com/knowledge/articles/41313

[e] 关于QEMU的QED文件格式, 可以见如下链接中的介绍. 

http://www.linux-kvm.com/content/qed-qemu-enhanced-disk-format

http://wiki.qemu.org/Features/QED

[f] iSCSI请查看: http://en.wikipedia.org/wiki/ISCSI

[g] Fibre Channel请查看: http://en.wikipedia.org/wiki/Fibre_Channel