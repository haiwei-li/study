
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. vq 的 owner](#1-vq-的-owner)
  - [1.1. 数据流角度](#11-数据流角度)
  - [1.2. 地址转换角度](#12-地址转换角度)
- [2. vq 初始化](#2-vq-初始化)
  - [2.1. 驱动获取 vq 配置](#21-驱动获取-vq-配置)
  - [2.2. 驱动初始化 vq](#22-驱动初始化-vq)
    - [2.2.1. 驱动选择队列](#221-驱动选择队列)
      - [2.2.1.1. 设备记录队列](#2211-设备记录队列)
    - [2.2.2. 驱动获取描述符信息](#222-驱动获取描述符信息)
      - [2.2.2.1. 设备返回信息](#2221-设备返回信息)
    - [2.2.3. 驱动为 vq 分配内存](#223-驱动为-vq-分配内存)
    - [2.2.4. 驱动对 vq 结构初始化](#224-驱动对-vq-结构初始化)
    - [2.2.5. 驱动将 vq 地址告知设备](#225-驱动将-vq-地址告知设备)
      - [2.2.5.1. 设备初始化 vq](#2251-设备初始化-vq)

<!-- /code_chunk_output -->

# 1. vq 的 owner

在执行具体的 I/O 前, 需要先搭建好承载数据的基础设施 Virtqueue. **Virtio 协议**规定, **Guest** 的内核**驱动**是 **Virtqueue** 的 **owner**.

## 1.1. 数据流角度

在前面 I/O 栈部分探讨**驱动**时, 我们看到,

* 在**向设备写数据**时, **驱动**负责将 **cache** 中的数据**写入硬盘的寄存器**;

* 从设备**读取数据**时, **驱动**负责**从硬盘的寄存器读取**数据, 然后**写入 cache** 中对应的 **buffer**.

无论是**读**还是**写**, **设备**都**不参与 buffer 的管理**, 所以从这个角度讲, **Virtqueue** 更适合由 **Guest** 内核中的**驱动**管理.

## 1.2. 地址转换角度

从另外一个角度, Guest 驱动分配了地址以后, 从 **Guest** 一侧可以方便地将**虚拟地址**(`GVA`)转换为**物理地址**(`GPA`), **VMM** 拿到 **GPA** 后, 很容易将 GPA 转换为 **HPA**.

但是反过来, 如果是在 VMM 中分配一块地址, **几乎不可能**将 **HPA** 转换为 Guest 可以识别的虚拟地址(**GVA**).

# 2. vq 初始化

既然**驱动**是 Virtqueue 的 owner, 那么 Virqueue 的**初始化**就需要**驱动来负责**.

## 2.1. 驱动获取 vq 配置

我们在**驱动**一个**真实的物理硬盘**时, 需要**从硬盘**获取具体的**参数**, 比如硬盘的磁头、柱面等信息.

**Virtio 协议**也规定如 **Virtqueue** 的 size 等**配置参数**由**模拟设备负责定义**, 而**这些参数**在**设备的配置空间**中, 因此, **驱动**首先使用 `pci_iomap` 函数将 Virtio 设备的配置**映射到内核**, Virtio 标准**约定**从**第 1 个 I/O 区域**的起始位置开始放置设备的配置, 所以驱动传给 `pci_iomap` 的第 2 个参数的值为 0, 即使用**第一个 bar**, 也就是第一个 I/O 区域. 这样, **驱动**就可以**访问 Virtio header** 获取 **Virtqueue** 的**各种参数**了.

```cpp
commit 3343660d8c62c6b00b2f15324ef3fcb6be207bfa
virtio: PCI device
linux.git/drivers/virtio/virtio_pci.c
/* the PCI probing function */
static int __devinit virtio_pci_probe(struct pci_dev *pci_dev,
                                     const struct pci_device_id *id)
{
    ...
    vp_dev->ioaddr = pci_iomap(pci_dev, 0, 0);
    ...
}
```

## 2.2. 驱动初始化 vq

真正的 vq 内存在 `struct vring_virtqueue` -> vring(`struct vring`) 中的 **desc**, **avail** 和 **used**, `struct vring_virtqueue` 中也有 `struct virtqueue` 项, 一般获取的 code path 是

1. `struct virtio_blk` -> vq(`struct virtqueue`)

2. `container_of` 得到 vq(`struct virtqueue`) 对应的 `struct vring_virtqueue`

3. 再使用 `struct vring_virtqueue` -> vring(`struct vring`) 中的 **desc**, **avail** 和 **used**

接下来, **驱动**调用函数 `find_vq` 开启 **Virtqueue** 的**初始化**过程:

```cpp
commit 3343660d8c62c6b00b2f15324ef3fcb6be207bfa
virtio: PCI device
linux.git/drivers/block/virtio_blk.c
static int virtblk_probe(struct virtio_device *vdev)
{
    ...
    /* We expect one virtqueue, for output. */
    vblk->vq = vdev->config->find_vq(vdev, 0, blk_done);
    ...
}
```

`find_vq` 对应的是 `vp_find_vq`.

### 2.2.1. 驱动选择队列

**设备**可能有**多个队列**, 例如典型的**网络设备**中可能有**分别**用于**收发的队列**, 且**每个收发**也可能使用**多个队列**. 因此, 在**初始化队列前**, 首先需要**通知设备**接下来的初始化过程是**针对哪个队列**的. 以 Virtio blk 为例, 其**仅使用了一个队列**, 所以上面代码中传递给函数 `find_vq` 的**第 2 个参数**是 0, 表示初始化第 1 个队列.

**驱动**通过**写 Virtio header** 中的 **Queue Select** 寄存器的方式**通知设备**后续的操作是针对**哪个队列**的:

```cpp
commit 3343660d8c62c6b00b2f15324ef3fcb6be207bfa
virtio: PCI device
linux.git/drivers/virtio/virtio_pci.c
/* the config->find_vq() implementation */
static struct virtqueue *vp_find_vq(struct virtio_device *vdev, unsigned index,
                                   void (*callback)(struct virtqueue *vq))
{
    ...
    /* Select the queue we're interested in */
    iowrite16(index, vp_dev->ioaddr + VIRTIO_PCI_QUEUE_SEL);
    ...
}
```

#### 2.2.1.1. 设备记录队列

**模拟设备**收到**驱动写寄存器** `VIRTIO_PCI_QUEUE_SEL` 后, 将**记录**下驱动操作的**队列索引**, 后续**驱动操作队列**时, 使用这次设置的索引去 Virtqueue 中**索引对应的队列**:

```cpp
commit a2c8c69686be7bb224b278d4fd452fdc56b52c3c
kvm,virtio: add scatter-gather support

kvmtool.git/include/kvm/virtio_pci.h
/* A 16-bit r/w queue selector */
#define VIRTIO_PCI_QUEUE_SEL		14

kvmtool.git/blk-virtio.c
static bool blk_virtio_out(struct kvm *self, uint16_t port, void *data, int size, uint32_t count)
{
    unsigned long offset;
    offset		= port - IOPORT_VIRTIO;
    switch (offset) {
    ...
    case VIRTIO_PCI_QUEUE_SEL:
        device.queue_selector	= ioport__read16(data);
        break;
    ...
}
```

### 2.2.2. 驱动获取描述符信息

接下来, 驱动就需要为 Virtqueue 分配**内存空间**了.

我们知道, Virtqueue的**主体**是**描述符表**, 就像驱动一个真实的物理硬盘时, 需要从硬盘获取磁头、柱面等信息一样, Virtio 驱动需要从设备读取 Virtqueue 的**描述符信息**:

```cpp
commit 3343660d8c62c6b00b2f15324ef3fcb6be207bfa
virtio: PCI device
linux.git/drivers/virtio/virtio_pci.c
/* the config->find_vq() implementation */
static struct virtqueue *vp_find_vq(struct virtio_device *vdev, unsigned index,
                                   void (*callback)(struct virtqueue *vq))
{
    ...
    /* Check if queue is either not available or already active. */
    num = ioread16(vp_dev->ioaddr + VIRTIO_PCI_QUEUE_NUM);
    ...
}
```

#### 2.2.2.1. 设备返回信息

**模拟设备**收到**驱动写寄存器** `VIRTIO_PCI_QUEUE_NUM` 后, 将根据**驱动之前选择**的 Virtqueue, 将**相应的队列**的**描述符数量**返回给驱动:

```cpp
commit 258dd093dce7acc5abe7ce9bd55e586be01511e1
kvm: Implement virtio block device write support
kvmtool.git/blk-virtio.c
#define VIRTIO_BLK_QUEUE_SIZE	16
static bool blk_virtio_in(struct kvm *self, uint16_t port, void *data, int size, uint32_t count)
{
    unsigned long offset;
    offset		= port - IOPORT_VIRTIO;
    switch (offset) {
    ...
    case VIRTIO_PCI_QUEUE_NUM:
        ioport__write16(data, VIRTIO_BLK_QUEUE_SIZE);
        break;
    ...
```

根据宏 `VIRTIO_BLK_QUEUE_SIZE` 的定义可知, **Virtio blk 设备**的 Virtqueue **队列**包含 **16 个描述符**.

### 2.2.3. 驱动为 vq 分配内存

获得 Virtqueue 的**描述符的的数量**后, 就可以为 Virtqueue 分配内存了:

```cpp
commit 3343660d8c62c6b00b2f15324ef3fcb6be207bfa
virtio: PCI device
linux.git/drivers/virtio/virtio_pci.c

struct virtio_pci_vq_info
{
    ...
    /* the virtual address of the ring queue */
    void *queue;
    ...
}

/* the config->find_vq() implementation */
static struct virtqueue *vp_find_vq(struct virtio_device *vdev, unsigned index,
                                   void (*callback)(struct virtqueue *vq))
{
    // virtio_pci设备信息
    struct virtio_pci_device *vp_dev = to_vp_device(vdev);
    struct virtio_pci_vq_info *info;
    ...
    info->queue = kzalloc(PAGE_ALIGN(vring_size(num,PAGE_SIZE)), GFP_KERNEL);
    ...
    list_add(&info->node, &vp_dev->virtqueues);
    ...
}
```

其中 `vring_size` 是计算 Virtqueue **占用内存**的函数:

```cpp
commit 3343660d8c62c6b00b2f15324ef3fcb6be207bfa
virtio: PCI device
linux.git/include/linux/virtio_ring.h
static inline unsigned vring_size(unsigned int num, unsigned long pagesize)
{
    return ((sizeof(struct vring_desc) * num + sizeof(__u16) * (2 + num)
         + pagesize - 1) & ~(pagesize - 1))
        + sizeof(__u16) * 2 + sizeof(struct vring_used_elem) * num;
}
```

详细说明如下:

1) **描述符表**: 结构体为 `vring_desc` 描述其中**一个描述符**, 所以 `sizeof(struct vring_desc) × num` 是 `num(VIRTIO_BLK_QUEUE_SIZE)` 个描述符需要的内存.

```cpp
commit 3343660d8c62c6b00b2f15324ef3fcb6be207bfa
virtio: PCI device
linux.git/include/linux/virtio_ring.h
/* Virtio ring descriptors: 16 bytes.  These can chain together via "next". */
struct vring_desc
{
    /* Address (guest-physical). */
    __u64 addr;
    /* Length. */
    __u32 len;
    /* The flags as indicated above. */
    __u16 flags;
    /* We chain unused descriptors via this, too */
    __u16 next;
};
```

2) **可用描述符区域**对应的结构体如下:

```cpp
commit 3343660d8c62c6b00b2f15324ef3fcb6be207bfa
virtio: PCI device
linux.git/include/linux/virtio_ring.h
struct vring_avail
{
    __u16 flags;
    __u16 idx;
    __u16 ring[];
};
```

**数组 ring** 为**可用描述符的集合**, **每个可用描述符**占用 **2 字节**(`__u16`, 元素是描述符表中的索引), 当 Virtqueue 为**空**时, 最多有 num 个可用描述符, 加上变量 flags 和 idx, 所以**可用描述符区域**需要的**内存**为 `num + 2` 个 `__u16`, 即 `sizeof(__u16) × (2 + num)` 是可用描述符区域需要的内存.

3) **已用描述符区域**对应的结构体如下:

```cpp
commit 3343660d8c62c6b00b2f15324ef3fcb6be207bfa
virtio: PCI device
linux.git/include/linux/virtio_ring.h

struct vring_used {
    __u16 flags;
    __u16 idx;
    struct vring_used_elem ring[];
};
/* u32 is used here for ids for padding reasons. */
struct vring_used_elem {
    /* Index of start of used descriptor chain. */
    __u32 id;
    /* Total length of the descriptor chain which was used (written to) */
    __u32 len;
};
```

数组 ring 为已用描述符的集合, 每个已用描述符为一个结构体 `vring_used_elem` 的实例, 当 Virtqueue 为满时, 最多将有 num 个已用描述符, 加上变量 **flags** 和 **idx**, 所以**可用描述符区域**需要的**内存**为 num 个 `sizeof(struct vring_used_elem)`, 以及 2 个 `__u16`, 即 `sizeof(__u16) × 2 + sizeof(struct vring_used_elem) × num`.

### 2.2.4. 驱动对 vq 结构初始化

分配好内存后, 驱动需要按照 **Virtio** 的**协议**约定进行**结构化**:

```cpp
commit 3343660d8c62c6b00b2f15324ef3fcb6be207bfa
virtio: PCI device
linux.git/drivers/virtio/virtio_pci.c
/* the config->find_vq() implementation */
static struct virtqueue *vp_find_vq(struct virtio_device *vdev, unsigned index,
                                   void (*callback)(struct virtqueue *vq))
{
    struct virtqueue *vq;
    ...
    /* create the vring */
    // info->queue 是分配的 vq 内存
    vq = vring_new_virtqueue(info->num, vdev, info->queue,
                             vp_notify, callback);
    ...
    return vq;
}

linux.git/drivers/virtio/virtio_ring.c
struct virtqueue *vring_new_virtqueue(unsigned int num,
                      struct virtio_device *vdev,
                      void *pages,...)
{
    struct vring_virtqueue *vq;
    ...
    vq = kmalloc(sizeof(*vq) + sizeof(void *)*num, GFP_KERNEL);
    // pages 是分配的 vq 内存
    vring_init(&vq->vring, num, pages, PAGE_SIZE);
    ...
    vq->vq.vq_ops = &vring_vq_ops;

    /* Put everything in free lists. */
    // 空闲数目
    vq->num_free = num;
    vq->free_head = 0;
    for (i = 0; i < num-1; i++)
        vq->vring.desc[i].next = i+1;
    ...
    return &vq->vq;
}

linux.git/include/linux/virtio_ring.h
static inline void vring_init(struct vring *vr, unsigned int num, void *p, unsigned long pagesize)
{
    // 描述符数目
    vr->num = num;
    // p 是分配的 vq 内存
    // 描述符表
    vr->desc = p;
    // 可用描述符表
    vr->avail = p + num*sizeof(struct vring_desc);
    // 已用描述符表
    vr->used = (void *)(((unsigned long)&vr->avail->ring[num]
        + pagesize-1) & ~(pagesize - 1));
}
```

真正的 vq 内存在 `struct vring_virtqueue` -> vring(`struct vring`) 中的 desc, avail 和 used, `struct vring_virtqueue` 中也有 `struct virtqueue` 项, 一般获取的 code path 是

1. `struct virtio_blk` -> vq(`struct virtqueue`)

2. `container_of` 得到 vq(`struct virtqueue`) 对应的 `struct vring_virtqueue`

3. 再使用 `struct vring_virtqueue` -> vring(`struct vring`) 中的 desc, avail 和 used

初始状态, **所有的描述符**都是**空闲**的, 所以可以看到 `free_head` 指向**第 1 个描述符**, 并且所有描述符都在 **free 链表**中.

* **结构体 vring** 的字段 desc 指向字符**描述符表**, 这里参数 p 指向的就是前面分配的**内存区的起始位置**.

* **可用描述符区域**位于 num 个描述符之后的位置, 所以我们可以看到 avail 指向的是从 p 开始预留了 num 个描述符的位置.

* 在可用描述符区域之后是**已用描述符区域**, 所以 used 指向 avail 中数组 ring 的最后一个元素之后, 而且是按照页面对齐的位置.

```cpp
commit 3343660d8c62c6b00b2f15324ef3fcb6be207bfa
virtio: PCI device
linux.git/drivers/virtio/virtio_ring.c
struct vring_virtqueue
{
    struct virtqueue vq;

    /* Actual memory layout for this queue */
    struct vring vring;

    /* Other side has made a mess, don't try any more. */
    bool broken;

    /* Number of free buffers */
    unsigned int num_free;
    /* Head of free buffer list. */
    unsigned int free_head;
    /* Number we've added since last sync. */
    unsigned int num_added;

    /* Last used index we've seen. */
    u16 last_used_idx;

    /* How to notify other side. FIXME: commonalize hcalls! */
    void (*notify)(struct virtqueue *vq);

#ifdef DEBUG
    /* They're supposed to lock for us. */
    unsigned int in_use;
#endif

    /* Tokens for callbacks. */
    void *data[];
};

linux.git/include/linux/virtio_ring.h
struct vring {
    unsigned int num;

    struct vring_desc *desc;

    struct vring_avail *avail;

    struct vring_used *used;
};

/* The standard layout for the ring is a continuous chunk of memory which looks
 * like this.  We assume num is a power of 2.
 *
 * struct vring
 * {
 *	// The actual descriptors (16 bytes each)
 *	struct vring_desc desc[num];
 *
 *	// A ring of available descriptor heads with free-running index.
 *	__u16 avail_flags;
 *	__u16 avail_idx;
 *	__u16 available[num];
 *
 *	// Padding to the next page boundary.
 *	char pad[];
 *
 *	// A ring of used descriptor heads with free-running index.
 *	__u16 used_flags;
 *	__u16 used_idx;
 *	struct vring_used_elem used[num];
 * };
 */
```

### 2.2.5. 驱动将 vq 地址告知设备

分配好 Virtqueue 的内存后, 需要将 Virtqueue 的地址**告知设备**.

根据前面讨论的 Virtio 设备的配置空间可知, 在 **Virtio header** 的偏移 **0x8** 处, 即 Virtqueue 地址寄存器 `VIRTIO_PCI_QUEUE_PFN` 处, 约定的是 **Queue Address**, 所以驱动向这个寄存器中写入 Virtqueue 的地址:

```cpp
commit 3343660d8c62c6b00b2f15324ef3fcb6be207bfa
virtio: PCI device
linux.git/drivers/virtio/virtio_pci.c
/* the config->find_vq() implementation */
static struct virtqueue *vp_find_vq(struct virtio_device *vdev, unsigned index,
                                   void (*callback)(struct virtqueue *vq))
{
    ...
    /* activate the queue */
    iowrite32(virt_to_phys(info->queue) >> PAGE_SHIFT,
              vp_dev->ioaddr + VIRTIO_PCI_QUEUE_PFN);
    ...
}
```

这里传递过去的是 Guest 物理地址, 即 GPA.

#### 2.2.5.1. 设备初始化 vq

设备侧收到驱动侧传递过来的 Virtqueue 的地址后, 也将开启**设备侧**的 **Virtqueue** 的**初始化**工作, 为后续基于 Virtqueue 的具体操作做好准备:

```cpp
commit 258dd093dce7acc5abe7ce9bd55e586be01511e1
kvm: Implement virtio block device write support
kvmtool.git/include/kvm/virtio_pci.h
/* A 32-bit r/w PFN for the currently selected queue */
#define VIRTIO_PCI_QUEUE_PFN		8

kvmtool.git/blk-virtio.c
#define VIRTIO_BLK_QUEUE_SIZE	16
static bool blk_virtio_out(struct kvm *self, uint16_t port, void *data, int size, uint32_t count)
{
    unsigned long offset;
    offset		= port - IOPORT_VIRTIO;
    switch (offset) {
    ...
    case VIRTIO_PCI_QUEUE_PFN: {
        struct virt_queue *queue;
        void *p;
        // 选择队列
        queue = &device.virt_queues[device.queue_selector];
        // 读取驱动侧分配的 vq 的地址, GPA
        queue->pfn = ioport__read32(data);
        // vq 的 HVA
        p = guest_flat_to_host(self, queue->pfn << 12);

        vring_init(&queue->vring, VIRTIO_BLK_QUEUE_SIZE, p, 4096);
        break;
    }
    ...
}
```

**设备**首先根据**之前驱动设置的队列索引** `queue_selector` 选择**对应的队列**.

然后读取**驱动侧**为 Virtqueue 分配的地址. 驱动传递过来的是以页面尺寸为单位的地址, 这里首先通过 `queue-＞pfn＜＜12` 将其转换为**线性地址**, 转换后的线性地址是 **Guest 的物理地址**, 即 **GPA**, 所以还需要将 GPA 转换为 Host 的虚拟地址, 即 **HVA**. 转换逻辑非常直接, 即从 kvmtool 为 Guest 分配的"物理内存"起始, 加上这个线性偏移即可, 函数 `guest_flat_to_host` 就是用来完成这个转换的:

```cpp
commit 258dd093dce7acc5abe7ce9bd55e586be01511e1
kvm: Implement virtio block device write support
kvmtool.git/include/kvm/kvm.h
static inline void *guest_flat_to_host(struct kvm *self, unsigned long offset)
{
    return self->ram_start + offset;
}
```

最后设备按照 Virtio 标准的约定, 调用函数 `vring_init` 分别计算并设置了**描述符区域**、**可用描述符区域**、**已用描述符区域**的地址. `vring_init` 与我们前面讨论的**驱动侧的代码一致**, 不再赘述.

```cpp
commit 258dd093dce7acc5abe7ce9bd55e586be01511e1
kvm: Implement virtio block device write support
kvmtool.git/include/kvm/virtio_ring.h

/* The standard layout for the ring is a continuous chunk of memory which looks
 * like this.  We assume num is a power of 2.
 *
 * struct vring
 * {
 *	// The actual descriptors (16 bytes each)
 *	struct vring_desc desc[num];
 *
 *	// A ring of available descriptor heads with free-running index.
 *	uint16_t avail_flags;
 *	uint16_t avail_idx;
 *	uint16_t available[num];
 *
 *	// Padding to the next align boundary.
 *	char pad[];
 *
 *	// A ring of used descriptor heads with free-running index.
 *	uint16_t used_flags;
 *	uint16_t used_idx;
 *	struct vring_used_elem used[num];
 * };
 */
static inline void vring_init(struct vring *vr, unsigned int num, void *p,
                  unsigned long align)
{
    vr->num = num;
    vr->desc = p;
    vr->avail = p + num*sizeof(struct vring_desc);
    vr->used = (void *)(((unsigned long)&vr->avail->ring[num] + align-1)
                & ~(align - 1));
}
```

`vr->desc/avail/used` 就是 **驱动侧和设备侧** 共享的 virtqueue 的内存.

至此, 驱动侧和设备侧协商好了传递数据的基础设施 Virtqueue.