
Introduce Virtio based Dmabuf driver(s): https://lwn.net/Articles/846810/

`[RFC,v3,0/3] Introduce Virtio based Dmabuf driver`: https://patchwork.kernel.org/project/linux-media/cover/20210203073517.1908882-1-vivek.kasireddy@intel.com/


`[RFC 0/1] vhost-vdmabuf: Add virtio based Dmabuf device`: https://patchew.org/QEMU/20210208233225.2084469-1-vivek.kasireddy@intel.com/

Vdmabuf: https://liujunming.top/2024/05/04/Notes-about-Vdmabuf/

[PATCH 0/4] Virtio shared dma-buf: https://patchew.org/QEMU/20230503081911.119168-1-aesteve@redhat.com/

[PATCH v3 0/4] Virtio shared dma-buf: https://patchew.org/QEMU/20230524091333.201767-1-aesteve@redhat.com/








在 Linux 虚拟化环境中, virtio 结合 `dmabuf`(DMA 缓冲区)是实现高效跨设备数据传输的核心技术, 其使用与实现涉及硬件抽象, 内存管理和同步机制. 以下是具体分析:

一, virtio 与 dmabuf 的使用方式

1. DMA-Buf 的申请与共享

• 申请 DMA 缓冲区:

  通过 Linux 内核的 `dma-heap` 接口申请 dmabuf. 例如, 使用 `/dev/dma-heap/system` 节点生成缓冲区文件描述符(fd), 代码示例如下:
  ```c
  int fd = open("/dev/dma-heap/system", O_RDWR);
  struct dma_heap_allocation_data data = { .len = buffer_size };
  ioctl(fd, DMA_HEAP_IOCTL_ALLOC, &data); // 返回的 data.fd 即为 dmabuf 的文件描述符
  ```
  此操作由硬件驱动 (如 GPU 或视频编解码器) 实现物理内存分配, 并映射到用户空间.

• 共享至 virtio 设备:

  将 dmabuf 的 fd 传递给 virtio 前端驱动(如 virtio-gpu 或 virtio-blk),virtio 通过 `virtqueue` 将 fd 传递到后端驱动(如 QEMU 或 Host 内核驱动), 实现零拷贝数据传输.

2. 多平面 (Multi-plane) 支持

  对于视频处理等场景, 需创建多平面 dmabuf(如 YUV 图像的 Y,U,V 平面). 通过 `dma_buf_export` 时指定多个平面, 并在驱动中分别映射每个平面的物理地址. 例如:

  ```c
  struct dma_buf_export_info exp_info = {
    .ops = &multiplane_ops, // 包含多平面操作的回调函数
    .size = total_size,
    .flags = O_CLOEXEC,
  };
  int buf_fd = dma_buf_export(&exp_info);
  ```


二, virtio 与 dmabuf 的实现机制

1. virtio 的数据流架构

• 前端驱动:

  运行于 Guest OS(虚拟机), 通过标准 virtio 接口 (如 `virtio-net`, `virtio-gpu`) 将 dmabuf 封装为 `virtqueue` 的描述符, 并触发中断通知后端.

• 后端驱动:

  运行于 Host 或 QEMU, 接收 `virtqueue` 中的描述符, 解析 dmabuf 的 fd 并映射到 Host 物理内存, 通过 DMA 引擎直接访问数据.

2. virtio 的 virtqueue 与 dmabuf 映射

• virtqueue 结构:

  包含 `desc`(描述符数组), `avail`(可用缓冲区环), `used`(已用缓冲区环)三部分. dmabuf 的物理地址通过描述符链关联到 virtqueue, 由前端驱动填充, 后端驱动消费.

• DMA 映射与同步:

  使用 `dma_map_sg` 将 dmabuf 的散列表 (`sg_table`) 映射到设备地址空间, 并通过 `dma_sync_sg_for_device/cpu` 保证缓存一致性.

三, 前后端数据一致性保障

1. 硬件级同步机制


• IOMMU 与 SMMU:

  通过 IOMMU/SMMU 实现设备地址到物理地址的安全转换, 隔离不同虚拟机或进程的 DMA 访问, 防止越界操作.
• 缓存一致性协议:

  在 ARM 等架构中, 依赖硬件维护缓存一致性(如 CCI 或 CMN), 确保 CPU 与设备看到的内存视图一致.

2. 软件级同步机制

• dma-fence:

  在图形渲染等场景中, 通过 `dma_fence` 标记 DMA 操作完成状态. 例如, GPU 渲染完成后触发 fence 信号, virtio 后端驱动据此启动数据传输.

• Virtio 的同步原语:

  • 中断通知: 后端处理完数据后, 通过虚拟中断 (如 MSI-X) 通知前端更新 `used` 环.

  • 内存屏障: 在更新 `avail` 和 `used` 环时插入内存屏障指令(如 `mb()`), 防止乱序执行导致状态不一致.

3. 多核间通信协议(RPMsg)

  在异构多核 (如 A 核与 M 核) 场景中, 基于 `rpmsg` 框架实现核间消息传递. 数据通过 `virtio` 的 `vring` 传输, 结合信号量或互斥锁确保操作原子性.

四, 典型应用场景示例

1. 虚拟机图形渲染

  • Guest 中的 virtio-gpu 前端将 OpenGL 指令和 dmabuf 传递给 Host 的 virgl 后端.

  • Host GPU 直接渲染到 dmabuf, 通过 `dma-fence` 同步后, 将结果传递回 Guest 显示.


2. 视频编解码

  • 使用多平面 dmabuf 存储 YUV 帧, virtio-video 前端将编码后的数据通过 `virtqueue` 发送到后端.

  • 后端调用硬件编解码器处理, 完成后通过中断通知前端读取结果.


总结

virtio 与 dmabuf 的结合通过以下核心设计实现高效, 安全的数据传输:

1. 零拷贝机制: 通过 fd 传递物理内存地址, 避免数据复制.
2. 标准化接口: virtio 提供统一的前后端驱动框架, dmabuf 定义跨设备缓冲区协议.
3. 同步与隔离: 依赖硬件 (IOMMU) 和软件 (fence, 中断) 机制保障数据一致性.

实际开发中需根据具体硬件 (如是否支持 IOMMU) 和场景 (如实时性要求) 调整同步策略.



通常情况下, virtio-dmabuf 的工作原理涉及以下几个步骤:

1. 创建缓冲区: 由一方 (可能是 Guest VM 或 Host) 创建 DMA 缓冲区.

2. 导出为文件描述符: 将该缓冲区转换为可供传递的文件描述符形式.

3. 通过 Virtio 通道传输: 利用 Virtio 提供的机制, 安全地将此文件描述符从一端传输到另一端.

4. 导入并使用: 接收方导入这个文件描述符, 并将其映射到自己的地址空间以便访问.



DMA-BUF 机制

在 Linux 中, DMA-BUF 提供了一种方法来在不同的驱动程序或进程间共享缓冲区. 这通过创建一个代表共享缓冲区的文件描述符 (fd) 来实现, 该文件描述符可以被传递给其他进程或设备, 以便它们可以直接访问这个共享的缓冲区.

Virtio DMA-BUF 的具体实现

1. 缓冲区创建: 在虚拟机内部或外部(取决于具体的使用场景), 需要有一个实体负责创建 DMA 缓冲区. 这通常是某个设备驱动程序或特定的应用逻辑.

2. 导出为 FD: 一旦缓冲区被创建, 下一步是将其导出为文件描述符 (fd). 在 Linux 中, dma-buf API 提供了这样的功能. 例如, 可以通过调用 dma_buf_export() 函数将一个缓冲区转换成一个可供共享的文件描述符.

3. 传输文件描述符: 接下来, 这个文件描述符需要被安全地传输到目标端 (比如另一个虚拟机或者宿主机). 在 Virtio 的上下文中, 这通常涉及到利用 Virtio 提供的通道(如 virtqueue) 来发送包含此 fd 的消息.

4. 接收并映射: 当另一方接收到包含文件描述符的信息后, 它需要导入这个描述符, 并将其映射到自己的地址空间中. 这一步可能涉及到使用 dma_buf_get()和 dma_buf_mmap()等函数来完成.

5. 同步控制: 由于多个实体可能同时访问相同的缓冲区, 因此需要适当的同步机制来确保数据一致性. DMA-BUF 提供了相关的 API 来支持同步操作, 如 dma_buf_begin_cpu_access()和 dma_buf_end_cpu_access()等.








---

Virtio DMA-BUF 的同步控制是其核心实现难点之一，主要解决虚拟机（Guest VM）与宿主机（Host）之间共享 DMA 缓冲区时的数据一致性问题。同步控制的目标是确保多个设备或进程在访问共享缓冲区时，数据的读写顺序和缓存状态是协调的。以下是其同步控制的详细实现机制：

---

### 1. 缓存同步（CPU 与设备间的一致性）
在 Virtio DMA-BUF 中，缓存同步是基础，确保 CPU 缓存与设备直接访问的物理内存（DMA 缓冲区）之间的数据一致性。以下是关键实现步骤：

#### (1) 显式缓存操作 API
Linux 内核提供了以下 API 来管理缓存同步：
- `dma_sync_single_for_device()`
  在 CPU 向设备写入数据前，刷新（Flush）CPU 缓存，确保设备能读取到最新数据。
- `dma_sync_single_for_cpu()`
  在设备向 CPU 写入数据后，使 CPU 缓存失效（Invalidate），确保 CPU 能读取到设备修改后的数据。

示例场景：
- Guest VM 向 Host 发送数据：
  Guest 驱动在将数据写入 DMA 缓冲区后，调用 `dma_sync_single_for_device()` 刷新缓存，确保 Host 设备能读取到最新数据。
- Host 向 Guest 回传数据：
  Host 设备写入数据后，Guest 驱动调用 `dma_sync_single_for_cpu()` 使缓存失效，确保 Guest CPU 能读取到更新后的数据。

#### (2) 非一致性架构下的手动管理
在 ARM 等非一致性架构中，DMA 设备无法自动维护缓存一致性，必须手动调用上述 API。Virtio DMA-BUF 驱动会根据硬件特性决定是否需要显式同步。

---

### 2. 设备间同步（DMA 操作的顺序控制）
Virtio DMA-BUF 通过以下机制确保多个设备或虚拟机之间的访问顺序正确性：

#### (1) `dma_fence` 同步原语
`dma_fence` 是 Linux 内核用于跟踪 DMA 操作完成状态的同步机制。它通过以下方式工作：
- 标记操作序列：每个 DMA 操作（如读/写）生成一个 `dma_fence` 对象，表示该操作的完成状态。
- 依赖链管理：设备驱动通过 `dma_fence_wait()` 等待其他设备的 DMA 操作完成，避免数据竞争。

在 Virtio 中的应用：
- 当 Guest VM 和 Host 设备共享 DMA 缓冲区时，双方会通过 `dma_fence` 协调访问顺序。例如，Guest 发送数据后，会等待 Host 完成 DMA 读取操作（通过 `dma_fence` 通知），再进行后续处理。

#### (2) Virtio 的 Virtqueue 机制
Virtio 通过 virtqueue 实现生产者-消费者模型，隐式协调数据访问顺序：
- 生产者（Guest）：将 DMA 缓冲区的文件描述符（FD）和操作指令（如读/写）放入 virtqueue，并通知消费者（Host）。
- 消费者（Host）：处理 virtqueue 中的任务，完成后通过 virtqueue 通知生产者。
- 同步点：
  - Guest 通过 `virtio_notify()` 触发 Host 处理任务。
  - Host 完成处理后，通过 `virtio_kick()` 通知 Guest 数据已就绪。

示例流程：
1. Guest 创建 DMA 缓冲区并导出 FD。
2. Guest 将 FD 和操作指令（如 "read"）放入 virtqueue，并调用 `virtio_notify()` 通知 Host。
3. Host 从 virtqueue 中取出任务，使用 FD 访问缓冲区，并生成 `dma_fence` 标记操作完成。
4. Host 调用 `virtio_kick()` 通知 Guest 操作完成。
5. Guest 通过 `dma_fence_wait()` 等待 Host 的操作完成，再继续处理数据。

---

### 3. 虚拟化环境中的扩展机制
在虚拟化环境中，Virtio DMA-BUF 还需解决跨虚拟机的同步问题：

#### (1) 特性协商（Feature Negotiation）
Virtio 驱动和后端（Host）通过 virtio_pci_common_cfg 协商支持的同步特性：
- `VIRTIO_F_NOTIFY_ON_EMPTY`：当 virtqueue 为空时通知消费者。
- `VIRTIO_F_RING_EVENT_IDX`：支持事件索引（Event Index），减少不必要的中断。
- `VIRTIO_F_IN_ORDER`：保证 virtqueue 中的操作按顺序执行。

#### (2) 中断与轮询的混合模式
- 中断模式：Host 完成 DMA 操作后触发中断通知 Guest。
- 轮询模式：Guest 主动检查 virtqueue 状态，适用于高性能场景（如 DPDK 中的 virtio-pmd 驱动）。

DPDK 中的优化：
DPDK 的 `virtio-pmd` 驱动通过轮询 virtqueue 和批量处理数据包，减少中断开销，同时结合 `dma_fence` 确保数据一致性。

---

### 4. 实际应用中的同步策略
#### (1) 图形/GPU 场景
- Guest 使用 GPU 渲染数据：
  Guest 将渲染结果写入 DMA 缓冲区，并调用 `dma_sync_single_for_device()` 刷新缓存。
- Host GPU 读取数据：
  Host 通过 `dma_fence` 等待 Guest 的渲染操作完成，再进行后续处理（如显示输出）。

#### (2) 视频编码/解码场景
- Host 编码器写入数据：
  Host 将编码后的视频帧写入 DMA 缓冲区，并通过 virtqueue 通知 Guest。
- Guest 读取数据：
  Guest 通过 `dma_sync_single_for_cpu()` 使缓存失效，确保读取到最新数据。

---

### 5. 潜在问题与优化

- 缓存同步开销：频繁调用 `dma_sync` API 可能影响性能。可通过批量处理数据或使用一致性内存（如 `DMA_BIDIRECTIONAL`）优化。
- 死锁风险：多个设备的 `dma_fence` 依赖链需避免循环依赖。
- 跨虚拟机同步延迟：在复杂虚拟化环境中，需结合 `dma_fence` 和 virtqueue 的高效通知机制降低延迟。

---

### 总结
Virtio DMA-BUF 的同步控制依赖于 Linux 内核的 dma-buf 机制（如 `dma_sync` 和 `dma_fence`）和 Virtio 的 virtqueue 通信模型。通过显式缓存操作、DMA 操作序列跟踪（`dma_fence`）、virtqueue 的生产者-消费者协调，以及虚拟化特性协商，实现了跨虚拟机和设备的安全高效数据共享。在实际应用中，需根据硬件特性和性能需求选择同步策略（如中断 vs 轮询），并合理管理缓存一致性。



