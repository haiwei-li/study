

中断虚拟化的关键在于对中断控制器的模拟, 我们知道 x86 上中断控制器主要有旧的中断控制器 PIC(intel 8259a) 和适应于 SMP 框架的 IOAPIC/LAPIC 两种.

# 可编程中断控制器 8259A

参见 `Architecture\0. 基本架构 \ x86\0. Learning\2. x86_64 编程体系探索及编程 \ 第 4 篇 中断体系 \ 第 17 章 8259 中断控制器 `

中断控制器能够全部在 KVM 中模拟, 也能够全部在 QEMU 中, 还能够部分在 QEMU 中模拟, 部分在 KVM 中模拟, 通过参数 kernel-irqchip=x 可以控制中断芯片由谁模拟. x 可以取如下值.

on: 表示由 KVM 模拟所有中断芯片.(default)

split: 表示由 QEMU 模拟 I/O APIC 和 PIC,KVM 模拟 LAPIC.

off: 表示由 QEMU 模拟全部.

# KVM 中的创建

由于在 KVM 中进行中断模拟的性能更高, 所以通常都会在 KVM 中模拟, 这里仅介绍 KVM 下的中断模拟. 本节首先介绍 PIC 的模拟.

QEMU 代码中, KVM 模拟 PIC 时创建的中断设备是 TYPE_KVM_I8259 类型的 QOM 对象, QEMU 模拟 PIC 时创建的是 TYPE_I8259 类型的对象, 这两种类型有一个共同的抽象父类对象 TYPE_PIC_COMMON.

PIC 设备的创建分为 KVM 和 QEMU 两个部分, 首先分析 KVM 部分的创建. kvm_init 函数会调用 kvm_irqchip_create 函数, 后者在 vm fd 上调用 ioctl(KVM_CREATE_IRQCHIP) 告诉内核需要在 KVM 中模拟 PIC 中断芯片. KVM 中处理该 ioctl 的函数是 kvm_arch_vm_ioctl, 相关代码如下.

```cpp
// arch/x86/kvm/x86.c
int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
{
    ...
	case KVM_CREATE_IRQCHIP: {
		...
		r = kvm_pic_init(kvm);
		...
		r = kvm_ioapic_init(kvm);
		...
		r = kvm_setup_default_irq_routing(kvm);
    ...
    }
```

KVM 模块在处理 `KVM_CREATE_IRQCHIP` 时主要调用了 3 个函数:

* `kvm_pic_init` 创建 PIC;

* `kvm_ioapic_init` 创建 I/O APIC;

* `kvm_setup_default_irq_routing` 设置默认的中断路由.

这里先分析 `kvm_pic_init` 和 `kvm_setup_default_irq_routing` 函数, `kvm_ioapic_init` 留到下一节分析.

`kvm_pic_init` 代码如下.

```cpp
// arch/x86/kvm/i8259.c
int kvm_pic_init(struct kvm *kvm)
{
	struct kvm_pic *s;
	int ret;

	s = kzalloc(sizeof(struct kvm_pic), GFP_KERNEL_ACCOUNT);
	...
	spin_lock_init(&s->lock);
	s->kvm = kvm;
	s->pics[0].elcr_mask = 0xf8;
	s->pics[1].elcr_mask = 0xde;
	s->pics[0].pics_state = s;
	s->pics[1].pics_state = s;

	/*
	 * Initialize PIO device
	 */
	kvm_iodevice_init(&s->dev_master, &picdev_master_ops);
	kvm_iodevice_init(&s->dev_slave, &picdev_slave_ops);
	kvm_iodevice_init(&s->dev_elcr, &picdev_elcr_ops);
	mutex_lock(&kvm->slots_lock);
    // 重点 1
	ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x20, 2,
				      &s->dev_master);
    // 重点 2
	ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0xa0, 2, &s->dev_slave);
    // 重点 3
	ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x4d0, 2, &s->dev_elcr);

	mutex_unlock(&kvm->slots_lock);

	kvm->arch.vpic = s;

	return 0;
    ...
}
```

kvm_create_pic 函数首先分配一个 kvm_pic 结构体, 然后在 KVM_PIO_BUS 上创建 3 个设备, 介绍如下.

1) master PIC 设备: 保存在 kvm_pic 的 dev_master 成员中, 其读写 ops 为 picdev_master_ops, 注册了 0x20～0x21 两个端口.

2) slave PIC 设备: 保存在 kvm_pic 的 dev_slave 成员中, 其读写 ops 为 picdev_slave_ops, 注册了 0xa0～0xa1 两个端口.

3) eclr 设备: 保存在 kvm_pic 的 dev_eclr 成员中, 其读写 ops 为 picdev_eclr_ops, 注册了 0x4d0～0x4d1 两个端口.

kvm_create_pic 创建好了之后, 调用 kvm_setup_default_irq_routing 来设置默认的中断路由表. 这其中涉及多个数据结构, 先来分析用来描述中断路由信息的数据结构, KVM 的中断路由表中的每一项包含每一个中断线的相关信息, 其定义如下.

```cpp
// include/uapi/linux/kvm.h
struct kvm_irq_routing_entry {
	__u32 gsi;
	__u32 type;
	__u32 flags;
	__u32 pad;
	union {
		struct kvm_irq_routing_irqchip irqchip;
		struct kvm_irq_routing_msi msi;
		struct kvm_irq_routing_s390_adapter adapter;
		struct kvm_irq_routing_hv_sint hv_sint;
		struct kvm_irq_routing_xen_evtchn xen_evtchn;
		__u32 pad[8];
	} u;
};

// 中断路由, 用来记录中断路由信息的数据结构
struct kvm_irq_routing {
	__u32 nr;
	__u32 flags;
	struct kvm_irq_routing_entry entries[];
};
```

gsi 表示该中断在系统全局范围内的中断号, type 用来决定中断的种类, 也就是后面 union 的解释, 如果 type 是 KVM_IRQ_ROUTING_IRQCHIP, 则表示 u.irqchip 有效. kvm_irq_routing_irqchip 结构体包含了该中断线的基本情况, 如所属的中断芯片以及引脚值.

```cpp
// include/uapi/linux/kvm.h
struct kvm_irq_routing_irqchip {
	__u32 irqchip;
	__u32 pin;
};
```

KVM 中有一个默认路由信息, 以 default_routing 数组表示, 其定义以及涉及的宏如下所示.

```cpp
// arch/x86/kvm/irq_comm.c
#define IOAPIC_ROUTING_ENTRY(irq) \
	{ .gsi = irq, .type = KVM_IRQ_ROUTING_IRQCHIP,	\
	  .u.irqchip = { .irqchip = KVM_IRQCHIP_IOAPIC, .pin = (irq) } }
#define ROUTING_ENTRY1(irq) IOAPIC_ROUTING_ENTRY(irq)

#define PIC_ROUTING_ENTRY(irq) \
	{ .gsi = irq, .type = KVM_IRQ_ROUTING_IRQCHIP,	\
	  .u.irqchip = { .irqchip = SELECT_PIC(irq), .pin = (irq) % 8 } }
#define ROUTING_ENTRY2(irq) \
	IOAPIC_ROUTING_ENTRY(irq), PIC_ROUTING_ENTRY(irq)

static const struct kvm_irq_routing_entry default_routing[] = {
	ROUTING_ENTRY2(0), ROUTING_ENTRY2(1),
	ROUTING_ENTRY2(2), ROUTING_ENTRY2(3),
	ROUTING_ENTRY2(4), ROUTING_ENTRY2(5),
	ROUTING_ENTRY2(6), ROUTING_ENTRY2(7),
	ROUTING_ENTRY2(8), ROUTING_ENTRY2(9),
	ROUTING_ENTRY2(10), ROUTING_ENTRY2(11),
	ROUTING_ENTRY2(12), ROUTING_ENTRY2(13),
	ROUTING_ENTRY2(14), ROUTING_ENTRY2(15),
	ROUTING_ENTRY1(16), ROUTING_ENTRY1(17),
	ROUTING_ENTRY1(18), ROUTING_ENTRY1(19),
	ROUTING_ENTRY1(20), ROUTING_ENTRY1(21),
	ROUTING_ENTRY1(22), ROUTING_ENTRY1(23),
};
```

default_routing 中的前面 16 个成员使用 ROUTING_ENTRY2 定义, ROUTING_ENTRY2 宏定义包括 IOAPIC_ROUTING_ENTRY 和 PIC_ROUTING_ENTRY 两个宏, 这两个宏定义了 irq 的相关信息.

kvm_setup_default_irq_routing 以 default_routing 数组及其大小调用 kvm_set_irq_routing 函数. 后者将 kvm_irq_routing_entry 信息转换为 kvm_kernel_irq_routing_entry 信息. kvm_kernel_irq_routing_entry 用于在内核中记录中断路由信息, 该结构除了包括 kvm_irq_routing_entry 中的 irq,type 等信息外, 其还是 set 回调函数成员. kvm_set_irq_routing 函数还要创建虚拟机的中断路由表, 虚拟机的中断路由表保存在 KVM 结构中的 irq_routing 成员中, 其类型为 kvm_irq_routing_table, 定义如下.

```cpp
// include/linux/kvm_host.h
#ifdef CONFIG_HAVE_KVM_IRQ_ROUTING
struct kvm_irq_routing_table {
	int chip[KVM_NR_IRQCHIPS][KVM_IRQCHIP_NUM_PINS];
	u32 nr_rt_entries;
	/*
	 * Array indexed by gsi. Each entry contains list of irq chips
	 * the gsi is connected to.
	 */
	struct hlist_head map[] __counted_by(nr_rt_entries);
};
#endif
```

chip 是一个二维数组, 第一维表示芯片, KVM_NR_IRQCHIPS 为 3, 表示 master PIC,slave PIC 和 I/O APIC 3 个芯片, 第二维表示中断芯片对应的引脚, KVM_IRQCHIP_NUM_PINS 为 24, 表示 I/O APIC 的引脚数目, 当然对于 PIC 芯片只使用前 8 个. chip 中每一项表示芯片引脚对应的全局中断号 gsi.kvm_irq_routing_table 中的最后一个成员 map 是一个零长数组, 对于每一个 gsi, 都会分配一个 map 成员, 在 kvm_set_irq_routing 函数中会计算出该数组的大小保存在 nr_rt_entries 中. map 中的每一项是一个哈希链表头, 其作用是链接 gsi 对应的所有 kvm_kernel_irq_routing_entry 项, 通过 kvm_kernel_irq_routing_entry 的 link 成员链接. 相关数据结构关系如图 6-3 所示.

kvm_kernel_irq_routing_entry 相关数据结构:

![2024-10-09-16-34-38.png](./images/2024-10-09-16-34-38.png)

将 kvm_irq_routing_entry 结构体转换为 kvm_kernel_irq_routing_entry 是通过函数 setup_routing_entry 完成的, 该函数代码如下.

```cpp
// virt/kvm/irqchip.c
static int setup_routing_entry(struct kvm *kvm,
			       struct kvm_irq_routing_table *rt,
			       struct kvm_kernel_irq_routing_entry *e,
			       const struct kvm_irq_routing_entry *ue)
{
	struct kvm_kernel_irq_routing_entry *ei;
	int r;
	u32 gsi = array_index_nospec(ue->gsi, KVM_MAX_IRQ_ROUTES);

	/*
	 * Do not allow GSI to be mapped to the same irqchip more than once.
	 * Allow only one to one mapping between GSI and non-irqchip routing.
	 */
	hlist_for_each_entry(ei, &rt->map[gsi], link)
		if (ei->type != KVM_IRQ_ROUTING_IRQCHIP ||
		    ue->type != KVM_IRQ_ROUTING_IRQCHIP ||
		    ue->u.irqchip.irqchip == ei->irqchip.irqchip)
			return -EINVAL;

	e->gsi = gsi;
	e->type = ue->type;
    // 重点 1
	r = kvm_set_routing_entry(kvm, e, ue);
	if (r)
		return r;
	if (e->type == KVM_IRQ_ROUTING_IRQCHIP)
		rt->chip[e->irqchip.irqchip][e->irqchip.pin] = e->gsi;

	hlist_add_head(&e->link, &rt->map[e->gsi]);

	return 0;
}
```

setup_routing_entry 函数首先判断需要转换的 kvm_irq_routing_entry 和 kvm_kernel_irq_routing_entry 是否有效, 一个 gsi 只能映射到不同芯片上的引脚. 接下来将 kvm_irq_routing_entry 的 gsi 和 type 复制到 kvm_kernel_irq_routing_entry 中. kvm_set_routing_entry 函数会根据 kvm_irq_routing_entry 中指定的中断类型设置 kvm_kernel_irq_routing_entry 中对应的 set 回调函数. PIC 芯片中断的 set 函数为 kvm_set_pic_irq,I/O APIC 中断的 set 函数是 kvm_set_ioapic_irq,MSI 中断的 set 函数是 kvm_set_msi.setup_routing_entry 函数最后设置根据中断芯片和引脚设置中断路由表 kvm_irq_routing_table 的 chip 成员中的相应元素的值为对应的 gsi, 然后将该 kvm_kernel_irq_routing_entry 加入到中断路由表的 map[e->gsi] 上的链表上.

# QEMU 中 PIC 的初始化

QEMU 虚拟机的中断状态由 GSIState 结构体表示, 其定义如下.

```cpp
// include/hw/i386/x86.h
typedef struct GSIState {
    qemu_irq i8259_irq[ISA_NUM_IRQS];
    qemu_irq ioapic_irq[IOAPIC_NUM_PINS];
    qemu_irq ioapic2_irq[IOAPIC_NUM_PINS];
} GSIState;

// hw/core/irq.c
struct IRQState {
    Object parent_obj;

    qemu_irq_handler handler;
    void *opaque;
    int n;
};

// include/qemu/typedefs.h
typedef struct IRQState *qemu_irq;
```

GSIState 结构体包含了 PIC 和 I/O APIC 两个芯片的中断信息, 其中的 qemu_irq 是一个指向 IRQState 结构的指针, IRQState 可以表示一个中断引脚, 其中的 handler 表示执行的函数, opaque 是一个创建者指定的结构, n 是引脚号.

pc_init1 函数会分配一个 GSIState 结构保存在 gsi_state 变量中, 然后调用 qemu_allocate_irqs 分配一组 qemu_irq, 其中 qemu_irq 的 handler 会设置为 gsi_handler, 而 opaque 会设置为 gsi_state.

```cpp
// hw/i386/pc_piix.c
static void pc_init1(MachineState *machine, const char *pci_type)
{
    ...
    gsi_state = pc_gsi_create(&x86ms->gsi, pcmc->pci_enabled);
    ...
}

// hw/i386/pc.c
GSIState *pc_gsi_create(qemu_irq **irqs, bool pci_enabled)
{
    GSIState *s;

    s = g_new0(GSIState, 1);
    if (kvm_ioapic_in_kernel()) {
        kvm_pc_setup_irq_routing(pci_enabled);
    }
    *irqs = qemu_allocate_irqs(gsi_handler, s, IOAPIC_NUM_PINS);

    return s;
}
```

qemu_allocate_irqs 调用 qemu_extend_irqs 分配一组 qemu_irq.qemu_extend_irqs 代码如下.

```cpp
// hw/core/irq.c
qemu_irq *qemu_extend_irqs(qemu_irq *old, int n_old, qemu_irq_handler handler,
                           void *opaque, int n)
{
    qemu_irq *s;
    int i;

    if (!old) {
        n_old = 0;
    }
    s = old ? g_renew(qemu_irq, old, n + n_old) : g_new(qemu_irq, n);
    for (i = n_old; i < n + n_old; i++) {
        // 重点 1
        s[i] = qemu_allocate_irq(handler, opaque, i);
    }
    return s;
}
```

首先分配 n 个 qemu_irq, 然后调用 qemu_allocate_irq 函数, qemu_allocate_irq 函数会创建一个 TYPE_IRQ 对象, 也就是 qemu_irq 对象, 并且使用 handler 和 opaque,i 初始化新创建的 qemu_irq 对象.

pcms->gsi 分配完成之后相关数据结构如下图所示.

![2024-10-09-16-52-05.png](./images/2024-10-09-16-52-05.png)

pcms->gsi 表示整个虚拟机中断路由的起点, 在 i440fx_init 函数中, pcms->gsi 会被赋值给南桥 piix3 的 PIC 成员, PCI 设备的中断会从这里开始分发. 南桥 piix3 在具现化的时候会创建一条 ISA 总线并保存在全局变量 isabus 中, 在 pc_init1 中, 会调用 isa_bus_irqs 将 pcms->gsi 赋值给 isabus 的 irqs 成员, isa 设备的中断会从这里开始分发.

pc_init 接下来进行 PIC 设备的创建与初始化. PIC 可以在 KVM 或者 QEMU 中模拟, 当 PIC 在 QEMU 中模拟的时候, 会创建类型为 TYPE_I8259 的设备, 该类型会完整地模拟 PIC 设备, 包括 IO 端口的读写等. 当 PIC 在 KVM 中模拟的时候, QEMU 会创建类型为 TYPE_KVM_I8259 的设备, TYPE_I8259 和 TYPE_KVM_I8259 有一个共同的父类型 TYPE_PIC_COMMON.TYPE_KVM_I8259 的模拟简单很多, 它只是作为 QEMU 设备与内核态 PIC 设备的一个桥梁.

pc_init1 函数如果判断 PIC 在内核模拟, 就会调用 kvm_i8259_init, 该函数代码如下.

```cpp
// hw/i386/pc_piix.c
static void pc_init1(MachineState *machine, const char *pci_type)
{
    ...
    if (x86ms->pic == ON_OFF_AUTO_ON || x86ms->pic == ON_OFF_AUTO_AUTO) {
        pc_i8259_create(isa_bus, gsi_state->i8259_irq);
    }
    ...
}

// hw/i386/pc.c
void pc_i8259_create(ISABus *isa_bus, qemu_irq *i8259_irqs)
{
    qemu_irq *i8259;

    if (kvm_pic_in_kernel()) {
        i8259 = kvm_i8259_init(isa_bus);
    } else if (xen_enabled()) {
        i8259 = xen_interrupt_controller_init();
    } else {
        i8259 = i8259_init(isa_bus, x86_allocate_cpu_irq());
    }

    for (size_t i = 0; i < ISA_NUM_IRQS; i++) {
        i8259_irqs[i] = i8259[i];
    }

    g_free(i8259);
}

// hw/i386/kvm/i8259.c
qemu_irq *kvm_i8259_init(ISABus *bus)
{
    // 重点 1
    i8259_init_chip(TYPE_KVM_I8259, bus, true);
    // 重点 2
    i8259_init_chip(TYPE_KVM_I8259, bus, false);
    // 重点 3
    return qemu_allocate_irqs(kvm_pic_set_irq, NULL, ISA_NUM_IRQS);
}
```

两次 i8259_init_chip 函数的调用用来创建 TYPE_KVM_I8259 对象并实现具现化, 第一次调用创建 master PIC, 第二次调用创建 slave PIC.kvm_i8259_init 还会调用 qemu_allocate_irqs 函数分配 ISA_NUM_IRQS(16) 个 qemu_irq 结构, 随后 pc_init 函数将这些 qemu_irq 赋值给 gsi_state 的 i8259_irq 成员, 注意这些 qemu_irq 的 handler 函数是 kvm_pic_set_irq.

这个循环的赋值完成之后, 相关的数据结构如图 6-5 所示.

![2024-10-09-17-00-33.png](./images/2024-10-09-17-00-33.png)

# 设备使用 PIC 中断

本节分析一下设备如何申请 PIC 下的中断资源以及实现中断的触发. isa 设备通过 `isa_bus_get_irq` 函数申请 irq 资源. 如在软盘设备的具现化函数 `isabus_fdc_realize` 中, 有如下申请中断的代码.

```cpp
// hw/block/fdc-isa.c
static Property isa_fdc_properties[] = {
    ...
    DEFINE_PROP_UINT32("irq", FDCtrlISABus, irq, 6),
    ...
}
static void isabus_fdc_realize(DeviceState *dev, Error **errp)
{
    ISABus *bus = isa_bus_from_device(isadev);
    ...
    fdctrl->irq = isa_bus_get_irq(bus, isa->irq);
    ...
}
```

其中 isa->irq 在软盘的属性 `isa_fdc_properties` 定义中被设置为 6,`isa_bus_get_irq` 定义如下.

```cpp
// hw/isa/isa-bus.c
qemu_irq isa_bus_get_irq(ISABus *bus, unsigned irqnum)
{
    assert(irqnum < ISA_NUM_IRQS);
    assert(bus->irqs_in);
    return bus->irqs_in[irqnum];
}
```

第一个参数是 isa 总线, 第二个表示申请的中断引脚号. isa_bus_get_irq 函数判断合法性之后直接返回了 `bus->irqs_in[irqnum]`​, 这也是上一节中的 pcms->gsi 数组.

接下来分析中断的触发, 设备可以调用 qemu_set_irq 来触发中断, 其代码如下.

```cpp
// hw/core/irq.c
void qemu_set_irq(qemu_irq irq, int level)
{
    if (!irq)
        return;

    irq->handler(irq->opaque, irq->n, level);
}
```

irq 参数是设备自己保存的 qemu_irq,level 表示触发时候的电平. 以 fdctrl_reset_irq 函数中的 `qemu_set_irq(fdctrl->irq, 0)` 为例, 从上面的分析可知, `fdctrl->irq` 的 handler 为 gsi_handler, 代码如下.

```cpp
// include/hw/isa/isa.h
#define ISA_NUM_IRQS 16

// hw/i386/x86.c
void gsi_handler(void *opaque, int n, int level)
{
    GSIState *s = opaque;

    trace_x86_gsi_interrupt(n, level);
    switch (n) {
    // 0 ~ 15
    case 0 ... ISA_NUM_IRQS - 1:
        if (s->i8259_irq[n]) {
            /* Under KVM, Kernel will forward to both PIC and IOAPIC */
            qemu_set_irq(s->i8259_irq[n], level);
        }
        /* fall through */
    case ISA_NUM_IRQS ... IOAPIC_NUM_PINS - 1:
        qemu_set_irq(s->ioapic_irq[n], level);
        break;
    case IO_APIC_SECONDARY_IRQBASE
        ... IO_APIC_SECONDARY_IRQBASE + IOAPIC_NUM_PINS - 1:
        qemu_set_irq(s->ioapic2_irq[n - IO_APIC_SECONDARY_IRQBASE], level);
        break;
    }
}
```

这里的 s 是 `gsi_state`, 如果 n 小于 16, 说明是 PIC 中断, 触发 `s->i8259_irq[n]` 中断, 从上一节分析可知, 这个 handler 是 `kvm_pic_set_irq`, 其代码如下.

```cpp
// hw/i386/kvm/i8259.c
static void kvm_pic_set_irq(void *opaque, int irq, int level)
{
    int delivered;

    pic_stat_update_irq(irq, level);
    delivered = kvm_set_irq(kvm_state, irq, level);
    kvm_report_irq_delivered(delivered);
}
```

kvm_pic_set_irq 函数会调用 kvm_set_irq 触发中断, 其代码如下.

```cpp
// accel/kvm/kvm-all.c
int kvm_set_irq(KVMState *s, int irq, int level)
{
    struct kvm_irq_level event;
    int ret;

    assert(kvm_async_interrupts_enabled());

    event.level = level;
    event.irq = irq;
    // 重点 1
    ret = kvm_vm_ioctl(s, s->irq_set_ioctl, &event);
    if (ret < 0) {
        perror("kvm_set_irq");
        abort();
    }

    return (s->irq_set_ioctl == KVM_IRQ_LINE) ? 1 : event.status;
}
```

在 kvm_init 函数中, 会检查 KVM 是否具有 KVM_CAP_IRQ_INJECT_STATUS 扩展 (通常都有)​, 如果有这个扩展, 则会设置 s->irq_set_ioctl 为 KVM_IRQ_LINE_STATUS.kvm_set_irq 构造一个 kvm_irq_level, 其中的 level 设置为电平, irq 设置为中断后, 用这个 kvm_irq_level 作为参数调用 `ioctl(KVM_IRQ_LINE_STATUS)`, 向 KVM 模块提交中断信息.

KVM 将用户态参数复制到内核态之后会调用 `kvm_vm_ioctl_irq_line` 函数处理 `ioctl(KVM_IRQ_LINE_STATUS)`, `kvm_vm_ioctl_irq_line` 调用 `kvm_set_irq`, 该函数定义如下.

```cpp
// virt/kvm/irqchip.c
int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
		bool line_status)
{
	struct kvm_kernel_irq_routing_entry irq_set[KVM_NR_IRQCHIPS];
	int ret = -1, i, idx;

	trace_kvm_set_irq(irq, level, irq_source_id);

	/* Not possible to detect if the guest uses the PIC or the
	 * IOAPIC.  So set the bit in both. The guest will ignore
	 * writes to the unused one.
	 */
	idx = srcu_read_lock(&kvm->irq_srcu);
    // 重点 1
	i = kvm_irq_map_gsi(kvm, irq_set, irq);
	srcu_read_unlock(&kvm->irq_srcu, idx);

	while (i--) {
		int r;
        // 重点 2
		r = irq_set[i].set(&irq_set[i], kvm, irq_source_id, level,
				   line_status);
		if (r < 0)
			continue;

		ret = r + ((ret < 0) ? 0 : ret);
	}

	return ret;
}
```

由于 KVM 无法知道虚拟机操作系统使用的中断控制器是 PIC 还是 I/O APIC, 所以会向所有的中断控制器发送中断. 当然, 虚拟机操作系统在某一时刻只会使用一个中断控制器, 所以只会收到一次中断. kvm_set_irq 函数首先调用 kvm_irq_map_gsi,kvm_irq_map_gsi 从 kvm->irq_routing 中得到虚拟机中断路由表, 然后从表的 map 成员中根据用户态 QEMU 指定的中断号作为索引获取该中断号对应的所有路由信息, 所有的中断路由信息存放在 kvm_set_irq 的 irq_set 数组中, 该数组中的每一项是一个 kvm_kernel_irq_routing_entry, 表示该中断号在中断控制器的路由信息.

kvm_set_irq 在获取了对应中断号的所有中断路由信息之后会调用中断路由信息中的 set 回调函数, 对于 PIC 来说, 该函数是 kvm_set_pic_irq, 这个函数直接调用了 kvm_pic_set_irq, 该函数代码如下.

```cpp
// arch/x86/kvm/i8259.c
int kvm_pic_set_irq(struct kvm_pic *s, int irq, int irq_source_id, int level)
{
	int ret, irq_level;

	BUG_ON(irq < 0 || irq >= PIC_NUM_PINS);

	pic_lock(s);
    // 重点 1
	irq_level = __kvm_irq_line_state(&s->irq_states[irq],
					 irq_source_id, level);
	// 重点 2
    ret = pic_set_irq1(&s->pics[irq >> 3], irq & 7, irq_level);
	pic_update_irq(s);
	trace_kvm_pic_set_irq(irq >> 3, irq & 7, s->pics[irq >> 3].elcr,
			      s->pics[irq >> 3].imr, ret == 0);
	pic_unlock(s);

	return ret;
}
```

__kvm_irq_line_state 函数计算出中断信号电平的高低并保存在 irq_level 变量中, 一般情况下, irq_level 与 level 是一致的. kvm_pic_set_irq 接着会调用 pic_set_irq1 函数设置对应中断芯片的状态, 具体来说, pic_set_irq1 函数会根据该中断芯片的触发类型设置对应的 s->irr 和 s->last_irr.kvm_pic_set_irq 接着调用 pic_update_irq 更新中断控制器的状态, 其代码如下.

```cpp
// arch/x86/kvm/i8259.c
static void pic_update_irq(struct kvm_pic *s)
{
	int irq2, irq;

	irq2 = pic_get_irq(&s->pics[1]);
	if (irq2 >= 0) {
		/*
		 * if irq request by slave pic, signal master PIC
		 */
		pic_set_irq1(&s->pics[0], 2, 1);
		pic_set_irq1(&s->pics[0], 2, 0);
	}
	irq = pic_get_irq(&s->pics[0]);
	pic_irq_request(s->kvm, irq >= 0);
}
```

pic_get_irq 返回当前挂在中断控制器上的 irq 中断号, 如果是 slave PIC 触发的中断, 还需要将这个中断发送到 master 中断, 最后调用 pic_irq_request, 设置 kvm_pic 结构体的 output 成员为中断电平.

kvm_pic_set_irq 函数最后会调用 pic_unlock, 该函数中会进行判断, 如果需要唤醒 CPU, 也就是需要注入一个中断, 则调用 kvm_make_request 在对应的 VCPU 上挂上一个 KVM_REQ_EVENT 请求.

VCPU 准备进入虚拟机模式时, 会调用 kvm_cpu_get_interrupt->kvm_cpu_get_extint->kvm_pic_read_irq 这一系列函数, 在 kvm_pic_read_irq 函数中, 调用 pic_get_irq 获得中断号 irq, 然后将 PIC 控制器的 irq_base 与 irq 相加得到中断向量号, 返回给 kvm_cpu_get_interrupt 函数, 当然 irq_base 是在 PIC 的配置端口中写入的, 具体函数是 pic_ioport_write.











操作系统会将中断对应的服务程序地址等信息存储在一个数组中, 数组中的每一个元素对应一个中断. 在实模式下, 这个数组称为 IVT(interrupt vector table). 在保护模式下, 这个数组称为 IDT(Interrupt descriptor table). 在响应中断时, CPU 使用中断向量从 IVT/IDT 中索引中断服务程序. 但是, x86 体系约定, 前 32 个中断号 (0~31) 是留给处理器自己使用的, 比如第 0 个中断号是处理器出现除 0 异常的, 因此, 外设的中断向量只能从 32 号中断开始. 所以, 在初始化 8259A 时, 操作系统通常会设置 8259A 的中断向量从 32 号中断开始, 因此当 8259A 收到管脚 IR0 的中断请求时, 其将向 CPU 发出的中断向量是 32, 当收到管脚 IR1 的中断请求时, 其将向 CPU 发出的中断向量是 33, 以此类推. 在 CPU 初始化 8259A 时, 通过第 2 个初始化命令字 (ICW2) 设置 8259A 起始中断向量号, 下面代码中的变量 irq_base 记录的就是起始中断向量号:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/i8259.c
static void pic_ioport_write(void *opaque, u32 addr, u32 val)
{
    ...
        switch (s->init_state) {
        ...
        case 1:
            s->irq_base = val & 0xf8;
            s->init_state = 2;
            break;
        ...
        }
    ...
}
```

后来, 随着 APIC 和 MSI 的出现, 中断向量设置得更为灵活, 可以为每个 PCI 设备设置其中断向量, 这个中断向量存储在 PCI 设备的配置空间中.

内核中抽象了一个结构体 `kvm_kpic_state` 来记录每个 8259A 的状态:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/irq.h
struct kvm_kpic_state {
    u8 last_irr;	/* edge detection */
    u8 irr;		/* interrupt request register */
    u8 imr;		/* interrupt mask register */
    u8 isr;		/* interrupt service register */
    ...
};

struct kvm_pic {
    struct kvm_kpic_state pics[2]; /* 0 is master pic, 1 is slave pic */
    irq_request_func *irq_request;
    void *irq_request_opaque;
    int output;		/* intr from master PIC */
    struct kvm_io_device dev;
};
```

一片 8259A 只能连接最多 8 个外设, 如果需要支持更多外设, 需要多片 8259A 级联. 在结构体 kvm_pic 中, 我们看到有两片 `8259A:pic[0]` 和 `pic[1]`. KVM 定义了结构体 kvm_kpic_state 记录 8259A 的状态, 其中包括我们之前提到的 IRR,IMR,ISR 等.

# 虚拟设备向 PIC 发送中断请求

如同物理外设请求中断时拉高与 8259A 连接的管脚的电压一样, 虚拟设备请求中断的方式是通过虚拟 8259A 芯片对外提供的一个 API, 以 kvmtool 中的 virtio blk 虚拟设备为例:

```cpp
commit 4155ba8cda055b7831489e4c4a412b073493115b
kvm: Fix virtio block device support some more
kvmtool.git/blk-virtio.c
static bool blk_virtio_out(struct kvm *self, uint16_t port, void *data, int size, uint32_t count)
{
    ...
    switch (offset) {
    case VIRTIO_PCI_QUEUE_NOTIFY: {
        ...
        while (queue->vring.avail->idx != queue->last_avail_idx) {
            if (!blk_virtio_read(self, queue))
                return false;
        }
        kvm__irq_line(self, VIRTIO_BLK_IRQ, 1);
        ...
    }
    ...
}
```

当 Guest 内核的块设备驱动需要从块设备读取数据时, 其通过写 I/O 端口 VIRTIO_PCI_QUEUE_NOTIFY 触发 CPU 从 Guest 模式切换到 Host 模式, KVM 中的块模拟设备开始 I/O 操作, 上述代码中的 while 循环就是处理 I/O 的, 函数 blk_virtio_read 从保存 Guest 文件系统的镜像文件中读取 I/O 数据. 在这个提交中, 块设备的 I/O 处理是同步的, 也就是说, 一直要等到虚拟设备 I/O 操作完成后, 才会向 Guest 发送中断, 返回 Guest. 显然, 阻塞在这里是不合理的, 更合理的方式是马上返回 Guest, 这样 Guest 可以执行其他任务, 待虚拟设备完成 I/O 操作后, 再通过中断通知 Guest, kvmtool 后来已经改进为异步的方式.

virtio blk 在处理完 I/O 后, 调用函数 kvm__irq_line 向虚拟 8259A 发送中断请求, 其中 VIRTIO BLK IRQ 对应管脚号, 第 3 个参数 "1" 代表高电平, 其代码如下:

```cpp
commit 4155ba8cda055b7831489e4c4a412b073493115b
kvm: Fix virtio block device support some more
kvmtool.git/kvm.c
void kvm__irq_line(struct kvm *self, int irq, int level)
{
    struct kvm_irq_level irq_level;

    irq_level	= (struct kvm_irq_level) {
        {
            .irq		= irq,
        },
        .level		= level,
    };

    if (ioctl(self->vm_fd, KVM_IRQ_LINE, &irq_level) < 0)
        die_perror("KVM_IRQ_LINE failed");
}
```

函数 `kvm__irq_line` 将管脚号和管脚电平信息封装到结构体 `kvm_irq_level` 中, 传递给内核中的 KVM 模块:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/kvm_main.c
static long kvm_vm_ioctl(struct file *filp,
               unsigned int ioctl, unsigned long arg)
{
    ...
    switch (ioctl) {
    case KVM_IRQ_LINE: {
        struct kvm_irq_level irq_event;

        r = -EFAULT;
        if (copy_from_user(&irq_event, argp, sizeof irq_event))
            goto out;
        if (irqchip_in_kernel(kvm)) {
            if (irq_event.irq < 16)
                kvm_pic_set_irq(pic_irqchip(kvm),
                    irq_event.irq,
                    irq_event.level);
            /* TODO: IOAPIC */
            r = 0;
        }
        break;
    }
    ...
}
```

KVM 模块将 kvmtool 中组织的中断信息从用户空间复制到内核空间中, 然后调用虚拟 8259A 的模块中提供的 API kvm_pic_set_irq, 向 8259A 发出中断请求.

# 记录中断到 IRR

当中断到来时, CPU 可能正在处理其他中断, 或者多个中断同时到来, 需要排队依次请求 CPU 处理, 因此, 当外设中断请求到来时, 8259A 首先需要将它们记录下来, 这个寄存器就是 IRR(Interrupt Request Register), 8259A 用它来记录有哪些中断需要处理.

当 KVM 模块收到外设的请求, 调用虚拟 8259A 的 API kvm_pic_set_irq 时, 其第一件事情就是将中断记录到 IRR 中:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/i8259.c
void kvm_pic_set_irq(void *opaque, int irq, int level)
{
    struct kvm_pic *s = opaque;

    pic_set_irq1(&s->pics[irq >> 3], irq & 7, level);
    pic_update_irq(s);
}

static inline void pic_set_irq1(struct kvm_kpic_state *s, int irq, int level)
{
    int mask;
    mask = 1 << irq;
    if (s->elcr & mask)	/* level triggered */
        if (level) {
            s->irr |= mask;
            s->last_irr |= mask;
        } else {
            s->irr &= ~mask;
            s->last_irr &= ~mask;
        }
    else	/* edge triggered */
        if (level) {
            if ((s->last_irr & mask) == 0)
                s->irr |= mask;
            s->last_irr |= mask;
        } else
            s->last_irr &= ~mask;
}
```

信号有边缘触发和水平触发, 在物理上可以理解为, 8329A 在前一个周期检测到管脚信号是 0, 当前周期检测到管脚信号是 1, 如果是上升沿触发模式, 那么 8259A 就认为外设有请求了, 这种触发模式就是边缘触发. 对于水平触发, 以高电平触发为例, 当 8259A 检测到管脚处于高电平, 则认为外设来请求了.

在虚拟 8259A 的结构体 kvm_kpic_state 中, 寄存器 elcr 是用来记录 8259A 被设置的触发模式的, 我们以边缘触发为例进行讨论, 即代码第 16~22 行. 参数 level 即相当于硬件层面的电信号, 0 表示低电平, 1 表示高电平. 当管脚收到一个低电平时, 即 level 的值为 0, 代码进入 else 分支, 即代码第 21,22 行, 结构体 kvm_kpic_state 中的字段 last_irr 会清除该 IRQ 对应 IRR 的位, 即相当于设置该中断管脚为低电平状态. 当管脚收到高电平时, 即 level 的值为 1, 代码进入 if 分支, 即代码第 17~20 行, 此时 8259A 将判断之前该管脚的状态, 即代码第 18 行, 也就是判断结构体 kvm_kpic_state 中的字段 last_irr 中该 IRQ 对应 IRR 的位, 如果之前管脚为低电平, 而现在管脚是高电平, 那么显然管脚电平有一个跳变, 说明中断源发出了中断请求, 8259A 在字段 irr 中记录下中断请求. 当然, 同时需要在字段 last_irr 记录下当前该管脚的状态.

# 设置待处理中断标识

当 8259A 将中断请求记录到 IRR 中后, 下一步就是开启一个中断评估 (evaluate) 过程, 包括评估中断是否被屏蔽, 多个中断请求的优先级等, 最后将通过管脚 INTA 通知 CPU 处理外部中断. 与物理 8259A 主动发起中断不同, 虚拟中断的发起方式不再是由虚拟中断芯片主动发起, 而是在每次准备切入 Guest 时, KVM 查询中断芯片, 如果有待处理的中断, 则执行中断注入. 模拟 8259A 在将收到的中断请求记录到 IRR 后, 将设置一个变量 "output", 后面在切入 Guest 前 KVM 会查询这个变量:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/i8259.c
void kvm_pic_set_irq(void *opaque, int irq, int level)
{
    struct kvm_pic *s = opaque;

    pic_set_irq1(&s->pics[irq >> 3], irq & 7, level);
    pic_update_irq(s);
}

static void pic_update_irq(struct kvm_pic *s)
{
    ...
    irq = pic_get_irq(&s->pics[0]);
    if (irq >= 0)
        s->irq_request(s->irq_request_opaque, 1);
    else
        s->irq_request(s->irq_request_opaque, 0);
}

static void pic_irq_request(void *opaque, int level)
{
    struct kvm *kvm = opaque;

    pic_irqchip(kvm)->output = level;
}
```

在函数 vmx_vcpu_run 中, 在准备切入 Guest 之前将调用函数 vmx_intr_assist 去检查虚拟中断芯片是否有等待处理的中断, 相关代码如下:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/vmx.c
static int vmx_vcpu_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
{
    ...
    if (irqchip_in_kernel(vcpu->kvm))
        vmx_intr_assist(vcpu);
    ...
}

static void vmx_intr_assist(struct kvm_vcpu *vcpu)
{
    ...
    has_ext_irq = kvm_cpu_has_interrupt(vcpu);
    ...
    if (!has_ext_irq)
        return;
    interrupt_window_open =
        ((vmcs_readl(GUEST_RFLAGS) & X86_EFLAGS_IF) &&
         (vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & 3) == 0);
    if (interrupt_window_open)
        vmx_inject_irq(vcpu, kvm_cpu_get_interrupt(vcpu));
    ...
}
```

其中函数 `kvm_cpu_has_interrupt` 查询 8259A 设置的变量 output:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/irq.c
int kvm_cpu_has_interrupt(struct kvm_vcpu *v)
{
    struct kvm_pic *s = pic_irqchip(v->kvm);

    if (s->output)	/* PIC */
        return 1;
    /*
     * TODO: APIC
     */
    return 0;
}
```

如果变量 output 非 0, 就说明有外部中断等待处理. 然后接下来还需要判断 Guest 是否可以被中断, 比如 Guest 是否正在执行一些不能被中断的指令, 如果 Guest 可以被中断, 则调用 `vmx_inject_irq` 完成中断的注入. 其中, 传递给函数 `vmx_inject_irq` 的第 2 个参数是函数 `kvm_cpu_get_interrupt` 返回的结果, 该函数获取需要注入的中断. 这个过程就是中断评估过程, 我们下一节讨论.

# 中断评估

在上一节我们看到在执行注入前, vmx_inject_irq 调用函数 `kvm_cpu_get_interrupt` 获取需要注入的中断. 函数 `kvm_cpu_get_interrupt` 的核心逻辑就是中断评估, 包括待处理的中断有没有被屏蔽? 待处理的中断的优先级是否比 CPU 正在处理的中断优先级高? 等等. 代码如下:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/irq.c
int kvm_cpu_get_interrupt(struct kvm_vcpu *v)
{
    ...
    vector = kvm_pic_read_irq(s);
    if (vector != -1)
        return vector;
    /*
     * TODO: APIC
     */
    return -1;
}

linux.git/drivers/kvm/i8259.c
int kvm_pic_read_irq(struct kvm_pic *s)
{
    int irq, irq2, intno;

    irq = pic_get_irq(&s->pics[0]);
    if (irq >= 0) {
        pic_intack(&s->pics[0], irq);
        if (irq == 2) {
            ...
        } else
            intno = s->pics[0].irq_base + irq;
    } else {
        ...
    }
    pic_update_irq(s);

    return intno;
}
```

kvm_pic_read_irq 调用函数 pic_get_irq 获取评估后的中断. 根据由上面代码中黑体标识的部分, 我们可以清楚地看到中断向量是在中断管脚的基础上叠加了一个 irq_base. 这个 irq_base 就是初始化 8259A 时通过 ICW 设置的, 完成从 IRn 到中断向量的转换.

一个中断芯片通常连接多个外设, 所以在某一个时刻, 可能会有多个设备请求到来, 这时就有一个优先处理哪个请求的问题, 因此, 中断就有了优先级的概念. 8259A 支持多种优先级模式, 典型的有两种中断优先级模式:

1) 固定优先级 (fixed priority), 即优先级是固定的, 从 IR0 到 IR7 依次降低, IR0 的优先级永远最高, IR7 的优先级永远最低.

2) 循环优先级 (rotating priority), 即当前处理完的 IRn 优先级调整为最低, 当前处理的下一个, 即 IRn + 1, 调整为优先级最高. 比如, 当前处理的中断是 irq2, 那么紧接着 irq3 的优先级设置为最高, 然后依次是 irq4,irq5,irq6,irq7,irq1,irq2,irq3. 假设此时 irq5 和 irq2 同时来了中断, 那么 irq5 会被优先处理. 然后 irq6 被设置为优先级最高, 接下来依次是 irq7,irq1,irq2,irq3,irq4,irq5.

理解了循环优先级算法后, 从 8259A 中获取最高优先级请求的代码就很容易理解了:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/i8259.c
static int pic_get_irq(struct kvm_kpic_state *s)
{
    int mask, cur_priority, priority;

    mask = s->irr & ~s->imr;
    priority = get_priority(s, mask);
    if (priority == 8)
        return -1;
    /*
     * compute current priority. If special fully nested mode on the
     * master, the IRQ coming from the slave is not taken into account
     * for the priority computation.
     */
    mask = s->isr;
    if (s->special_fully_nested_mode && s == &s->pics_state->pics[0])
        mask &= ~(1 << 2);
    cur_priority = get_priority(s, mask);
    if (priority < cur_priority)
        /*
         * higher priority found: an irq should be generated
         */
        return (priority + s->priority_add) & 7;
    else
        return -1;
}

static inline int get_priority(struct kvm_kpic_state *s, int mask)
{
    int priority;
    if (mask == 0)
        return 8;
    priority = 0;
    while ((mask & (1 << ((priority + s->priority_add) & 7))) == 0)
        priority++;
    return priority;
}
```

函数 pic_get_irq 分成两部分, 第一部分是从当前待处理的中断中取得最高优先级的中断, 需要过滤掉被屏蔽的中断, 即第 5,6 行代码, mask 中的值是去除掉 IMR 的 IRR. 第二部分是获取正在被 CPU 处理的中断的优先级, 即第 10~12 行代码, mask 中的值就是 ISR 中记录的, CPU 正在处理的中断. 然后比较两个中断的优先级, 见第 13~17 行代码, 如果待处理的优先级高, 那么就向上层调用者返回待处理的中断的管脚号.

我们再来看一下计算优先级的函数 get_priority, 这是一个典型的循环优先级算法. 其中变量 priority_add 记录的是当前最高优先级中断对应的管脚号, 所以逻辑上就是从当前的管脚开始, 依次检查后面的管脚是否有 pending 的中断. 比如当前处理的是 IR4 管脚的中断请求, priority_add 的值就是 4, 那么接下来就从紧接在 IR4 后面的管脚开始, 按照顺序处理 IR5,IR6, 一直到 IR3. 在这个过程中, 只要遇到管脚有中断请求, 则跳出循环. 如果 IR5 没有中断请求, 但是 IR6 有中断请求, 则 priority 累加后的值为 2, 函数 get_priority 返回 2. 那么下一个需要处理的中断管脚就是 4+2, 即管脚 IR6 对应的中断.

# 中断 ACK

物理 CPU 在准备处理一个中断请求后, 将通过管脚 INTA 向 8259A 发出确认脉冲. 同样, 软件模拟上也需要类似的处理. 在完成中断评估后, 准备向 Guest 注入中断前, KVM 向虚拟 8259A 执行确认状态的操作. 代码如下:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/i8259.c
int kvm_pic_read_irq(struct kvm_pic *s)
{
    int irq, irq2, intno;

    irq = pic_get_irq(&s->pics[0]);
    if (irq >= 0) {
        pic_intack(&s->pics[0], irq);
    ...
}

static inline void pic_intack(struct kvm_kpic_state *s, int irq)
{
    if (s->auto_eoi) {
        if (s->rotate_on_auto_eoi)
            s->priority_add = (irq + 1) & 7;
    } else
        s->isr |= (1 << irq);
    /*
     * We don't clear a level sensitive interrupt here
     */
    if (!(s->elcr & (1 << irq)))
        s->irr &= ~(1 << irq);
}
```

函数 `kvm_pic_read_irq` 获取评估的中断结果后, 调用函数 `pic_intack` 完成了中断确认的动作. 在收到中断确认后, 8259A 需要更新自己的状态. 因为中断已经开始得到服务了, 所以要从 IRR 中清除等待服务请求. 另外, 如果 8259A 工作在非 AEOI 模式, 那么还需要在 ISR 中记录 CPU 正在处理的中断. 如果 8259A 工作在 AEOI 模式, 那么就无须设置 ISR 了.

# 关于 EOI 的处理

如果 8259A 工作在非 AEOI 模式, 在中断服务程序执行的最后, 需要向 8259A 发送 EOI, 告知 8259A 中断处理完成. 8259A 在收到这个 EOI 时, 复位 ISR, 如果采用的是循环优先级, 还需要设置变量 `priority_add`, 使其指向当前处理 IRn 的下一个:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/i8259.c
static void pic_ioport_write(void *opaque, u32 addr, u32 val)
{
    ...
            cmd = val >> 5;
            switch (cmd) {
            ...
            case 1:	/* end of interrupt */
            case 5:
                priority = get_priority(s, s->isr);
                if (priority != 8) {
                    irq = (priority + s->priority_add) & 7;
                    s->isr &= ~(1 << irq);
                    if (cmd == 5)
                        s->priority_add = (irq + 1) & 7;
                    pic_update_irq(s->pics_state);
                }
                break;
    ...
}
```

如果 8259A 被设置为 AEOI 模式, 不会再收到后续中断服务程序的 EOI, 那么 8259A 在收到 ACK 后, 就必须立刻处理收到 EOI 命令时执行的逻辑, 调整变量 priority_add, 记录最高优先级位置:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/i8259.c
static inline void pic_intack(struct kvm_kpic_state *s, int irq)
{
    if (s->auto_eoi) {
        if (s->rotate_on_auto_eoi)
            s->priority_add = (irq + 1) & 7;
    } else
    ...
}
```

# 中断注入

对于外部中断, CPU 在每个指令周期结束后, 将会去检查 INTR 是否有中断请求. 那么对于处于 Guest 模式的 CPU, 如何知道有中断请求呢? Intel 在 VMCS 中设置了一个字段: VM-entry interruptioninformation, 在 VM entry 时 CPU 将会检查这个字段, 其格式表 3-1 所示.

![2024-03-04-21-04-14.png](./images/2024-03-04-21-04-14.png)

在 VM entry 前, KVM 模块检查虚拟 8259A, 如果有待处理的中断需要处理, 则将需要处理的中断信息写入 VMCS 中的字段 VM-entry interruption-information:

```cpp
commit 85f455f7ddbed403b34b4d54b1eaf0e14126a126
KVM: Add support for in-kernel PIC emulation
linux.git/drivers/kvm/vmx.c
static void vmx_inject_irq(struct kvm_vcpu *vcpu, int irq)
{
    ...
    vmcs_write32(VM_ENTRY_INTR_INFO_FIELD,
            irq | INTR_TYPE_EXT_INTR | INTR_INFO_VALID_MASK);
}
```

我们看到, 中断注入是在每次 VM entry 时, KVM 模块检查 8259A 是否有待处理的中断等待处理. 这样就有可能给中断带来一定的延迟, 典型如下面两类情况:

1) CPU 可能正处在 Guest 模式, 那么就需要等待下一次 VM exit 和 VM entry.

2) VCPU 这个线程也许正在睡眠, 比如 Guest VCPU 运行 hlt 指令时, 就会切换回 Host 模式, 线程挂起.

对于第 1 种情况, 是多处理器系统下的一个典型情况. 目标 CPU 正在运行 Guest, KVM 需要想办法触发 Guest 发生一次 VM exit, 切换到 Host. 我们知道, 当处于 Guest 模式的 CPU 收到外部中断时, 会触发 VM exit, 由 Host 来处理这次中断. 所以, KVM 可以向目标 CPU 发送一个 IPI 中断, 触发目标 CPU 发生一次 VM exit.

对于第 2 种情况, 首先需要唤醒睡眠的 VCPU 线程, 使其进入 CPU 就绪队列, 准备接受调度. 如果宿主系统是多处理器系统, 那么还需要再向目标 CPU 发送一个 "重新调度" 的 IPI 中断, 促使其尽快发生调度, 加快被唤醒的 VCPU 线程被调度, 执行切入 Guest 的过程, 从而完成中断注入.

所以当有中断请求时, 虚拟中断芯片将主动 "kick" 一下目标 CPU, 这个 "踢" 的函数就是 kvm_vcpu_kick:

```cpp
commit b6958ce44a11a9e9425d2b67a653b1ca2a27796f
KVM: Emulate hlt in the kernel
linux.git/drivers/kvm/i8259.c
static void pic_irq_request(void *opaque, int level)
{
    ...
    pic_irqchip(kvm)->output = level;
    if (vcpu)
        kvm_vcpu_kick(vcpu);
}
```

如果虚拟 CPU 线程在睡眠, 则 "踢醒" 它; 如果目标 CPU 运行在 Guest 模式, 则将其从 Guest 模式 "踢" 到 Host 模式, 在 VM entry 时完成中断注入. kick 的手段就是我们刚刚提到的 IPI, 代码如下:

```cpp
commit b6958ce44a11a9e9425d2b67a653b1ca2a27796f
KVM: Emulate hlt in the kernel
linux.git/drivers/kvm/irq.c
void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
{
    int ipi_pcpu = vcpu->cpu;

    if (waitqueue_active(&vcpu->wq)) {
        wake_up_interruptible(&vcpu->wq);
        ++vcpu->stat.halt_wakeup;
    }
    if (vcpu->guest_mode)
        smp_call_function_single(ipi_pcpu, vcpu_kick_intr, vcpu, 0, 0);
}
```

如果 VCPU 线程睡眠在等待队列上, 则唤醒使其进入 CPU 的就绪任务队列. 如果宿主系统是多处理器系统且目标 CPU 处于 Guest 模式, 则需要发送核间中断, 触发目标 CPU 发生 VM exit, 从而在下一次进入 Guest 时, 完成中断注入.

事实上, 由于目标 CPU 无须执行任何 callback, 也无须等待 IPI 返回, 所以也无须使用 `smp_call_function_single`, 而是直接发送一个请求目标 CPU 重新调度的 IPI 即可, 因此后来 KVM 模块直接使用了函数 `smp_send_reschedule`. 函数 smp_send_reschedule 简单直接地发送了一个 RESCHEDULE 的 IPI:

```cpp
commit 32f8840064d88cc3f6e85203aec7b6b57bebcb97
KVM: use smp_send_reschedule in kvm_vcpu_kick
linuxt.git/arch/x86/kvm/x86.c
void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
{
    int me;
    int cpu = vcpu->cpu;

    if (waitqueue_active(&vcpu->wq)) {
        wake_up_interruptible(&vcpu->wq);
        ++vcpu->stat.halt_wakeup;
    }

    me = get_cpu();
    if (cpu != me && (unsigned)cpu < nr_cpu_ids && cpu_online(cpu))
        if (!test_and_set_bit(KVM_REQ_KICK, &vcpu->requests))
            smp_send_reschedule(cpu);
    put_cpu();
}
linuxt.git/arch/x86/kernel/smp.c
static void native_smp_send_reschedule(int cpu)
{
    ...
    apic->send_IPI_mask(cpumask_of(cpu), RESCHEDULE_VECTOR);
}
```