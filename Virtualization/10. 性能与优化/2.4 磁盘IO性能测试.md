
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 磁盘 I/O 性能测试工具](#1-磁盘-io-性能测试工具)
  - [1.1. DD](#11-dd)
  - [1.2. fio](#12-fio)
  - [1.3. Bonnie++](#13-bonnie)
  - [1.4. hdparm](#14-hdparm)
- [2. 测试环境配置](#2-测试环境配置)
- [3. 性能测试方法](#3-性能测试方法)
  - [3.1. DD](#31-dd)
  - [3.2. fio](#32-fio)
  - [3.3. Bonnie++](#33-bonnie)
- [4. 性能测试数据](#4-性能测试数据)
  - [4.1. DD](#41-dd)
  - [4.2. fio](#42-fio)
  - [4.3. Bonnie++](#43-bonnie)

<!-- /code_chunk_output -->

# 1. 磁盘 I/O 性能测试工具

在一个计算机系统中, CPU 获取**自身缓存数据**的速度非常快, 读写内存的速度也比较快, 内部局域网速度也比较快(特别是使用万兆以太网), 但是磁盘 I/O 的速度是相对比较慢的.

很多的日常软件的运行都会读写磁盘, 而且大型的数据库应用(如 Oracle、MySQL 等)都是磁盘 I/O 密集型的应用, 所以在**KVM 虚拟化**中磁盘 I/O 的性能也是比较关键的.

测试磁盘 I/O 性能的工具有很多, 如 DD、Bonnie\+\+、fio、iometer、hdparm 等. 下面简单介绍其中几个工具.

## 1.1. DD

DD(命令为 dd)是 Linux 系统上一个非常流行的**文件复制工具**, 在复制文件的同时可以根据其具体选项进行转换和格式化等操作.

通过 DD 工具复制同一个文件(相同数据量)所需要的时间长短即可粗略评估磁盘 I/O 的性能.

一般的 Linux 系统中都自带这个工具, 用 man dd 命令即可查看 DD 工具的使用手册.

## 1.2. fio

fio 是一个被广泛使用的进行**磁盘性能及压力测试的工具**. 它功能强大而灵活, 可以用它定义(模拟)出**各种工作负载(workload**), 模拟**真实使用场景**, 以更准确地衡量磁盘的性能.

除了测试磁盘**读写的带宽**以外, 它还统计**IOPS**并且以**不同的延迟时间**分布表示;

除了**总的延迟时间**, 它还分别统计**I/O 递交的时间延迟**和**I/O 完成的时间延迟**.

fio 的主页是: http://git.kernel.dk/?p=fio.git;a=summary . 它可以运行在 Linux、UNIX、Windows 等多个操作系统平台上, 多数 Linux 发行版也包含它的安装包.

## 1.3. Bonnie++

Bonnie\+\+是以 Bonnie 的代码为基础编写而成的软件, 它使用一系列对硬盘驱动器和文件系统的简单测试来衡量其性能.

Bonnie\+\+可以模拟像数据库那样去访问一个单一的大文件, 也可以模拟像 Squid 那样创建、读取和删除许许多多的小文件.

它可以实现有序地读写一个文件, 也可以随机地查找一个文件中的某个部分, 而且支持按字符方式和按块方式读写.

## 1.4. hdparm

hdparm 是一个用于**获取和设置 SATA 和 IDE 设备参数**的工具, 在 RHEL 7.3 中可以用 yum install hdparm 命令来安装 hdparm 工具.

hdparm 也可以**粗略地测试磁盘的 I/O 性能**, 通过如下的命令即可粗略评估 sdb 这个磁盘的**读性能**.

```
hdparm -tT /dev/sdb
```

本节选择了 DD、FIO、Bonnie\+\+这 3 种工具用于 KVM 虚拟化的磁盘 I/O 性能测试.

# 2. 测试环境配置

对 KVM 的磁盘 I/O 虚拟化性能测试的环境配置, 与前面 CPU、内存、网络性能测试的环境配置基本相同, 下面仅说明一下不同之处和需要强调的地方.

在本次对磁盘 I/O 的性能测试中, **客户机第 2 块硬盘**专门用于磁盘性能测试, 这块硬盘是**宿主机的一个 LVM 分区**.

我们不选择前面通常使用的 raw image 文件, 是为了**避免宿主机文件系统这一层的消耗**, 从而使得客户机与宿主机直接的磁盘性能比较更加公平.

如图 10\-10 所示, 因为笔者的宿主机环境是 LVM 分区, 所以本节采用中间的一种方式. 当然, 最右边采用直接的物理硬盘分区的方式层次更简洁, 可以预见其绝对性能应该更好. 但如同前面几节的测试目标一样, 我们这里注重测量虚拟化层的性能损耗, 也就是客户机环境中基准测试结果, 与同样资源、软件堆栈层次的原生系统中同样基准测试的结果进行对比.

![](./images/2019-05-11-23-01-25.png)

另外, 磁盘性能测试与磁盘分区(包括物理分区和 LVM 分区)时候的参数设定、在硬盘分区上创建文件系统时候的参数设定, 以及关联与具体磁盘的 IO Scheduler 都有密切关系. 但这些不是我们这里测试的关注点, 我们只要注意原始环境的参数设置和客户机里的参数设置一致即可. 具体来说, 我们这里的 IO Scheduler、分区设置(LVM)、文件系统(XFS)的参数都选用最简单的默认值, 这也覆盖了大多数用户的使用场景.

```
[root@kvm-host current]# lvcreate --size 32G --name perf-test-lvm rhel
[root@kvm-host current]# lvdisplay /dev/rhel/perf-test-lvm
    --- Logical volume ---
    LV Path                /dev/rhel/perf-test-lvm
    LV Name                perf-test-lvm
    VG Name                rhel
    LV UUID                x8hVqL-U0Z8-i8fl-rFz1-tqrR-sAxW-vHV0d2
    LV Write Access        read/write
    LV Creation host, time kvm-host, 2017-04-29 21:43:12 +0800
    LV Status              available
    # open                 1
    LV Size                32.00 GiB
    Current LE             8192
    Segments               1
    Allocation             inherit
    Read ahead sectors     auto
    - currently set to     8192
    Block device           253:2

[root@kvm-guest current]# mkfs -t xfs -f /dev/sda
```

在实验中使用的希捷 2 TB 大小的 SATA 硬盘, 型号如下:

```
ST2000DL003-9VT166
```

本次测试评估了 QEMU/KVM 中的纯软件模拟的 IDE 磁盘和使用 virtio\-blk 驱动的磁盘(见其他内容), 启动客户机的 QEMU 命令行分别如下(注意, 客户机有两块硬盘, 一块是系统盘, 一直是 virtio\-blk 接口, 第二块才是我们的测试硬盘, /dev/rhel/perf\-test\-lvm):

```
#1. 使用 IDE 磁盘
qemu-system-x86_64 -enable-kvm -cpu host -smp cpus=4,cores=4,sockets=1 -m 16G -drive file=./rhel7.img,format=raw,if=none,media=disk,id=virtio_drive0 -device virtio-blk-pci,drive=virtio_drive0,bootindex=0 -drive file=/dev/rhel/perf-test-lvm,media=disk,if=ide,format=raw,id=ide_drive,cache=none -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -name perf_test -display sdl

#2. 使用 virtio-blk 磁盘
qemu-system-x86_64 -enable-kvm -cpu host -smp cpus=4,cores=4,sockets=1 -m 16G -drive file=./rhel7.img,format=raw,if=none,media=disk,id=virtio_drive0 -device virtio-blk-pci,drive=virtio_drive0,bootindex=0 -drive file=/dev/rhel/perf-test-lvm,media=disk,if=none,format=raw,id=virtio_drive1,cache=none -device virtio-blk-pci,drive=virtio_drive1 -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -name perf_test -display sdl
```

从上面的命令行可以看出, 测试磁盘镜像文件是 raw 格式的, 并且配置有"cache=none"来绕过页面缓存. 配置为"cache=none"绕过了页面缓存, 但是没有绕过磁盘自身的磁盘缓存; 如果要在宿主机中彻底绕过这两种缓存, 可以在启动客户机时配置为"cache=directsync". 不过由于"cache=directsync"配置会让客户机中磁盘 I/O 效率比较低, 所以这种配置用得比较少, 常用的配置一般为"cache=writethrough""cache=none"等. 关于"cache=xx"选项的配置, 可以参考 5.4.1 节.

由于启动客户机时使用的磁盘配置选项"cache=xx"的设置对磁盘 I/O 测试结果的影响非常大, 所以本次结果仅能代表"cache=none"这样配置下的一次基准测试.

# 3. 性能测试方法

对非虚拟化的原生系统和 KVM 客户机都执行相同的磁盘 I/O 基准测试, 然后对比其测试结果.

## 3.1. DD

用 DD 工具对读取磁盘文件进行测试, 测试 4 种不同的块大小. 使用的命令如下:

```
dd if=file.dat of=/dev/null iflag=direct bs=1K count=100K
dd if=file.dat of=/dev/null iflag=direct bs=8K count=100K
dd if=file.dat of=/dev/null iflag=direct bs=1M count=10K
dd if=file.dat of=/dev/null iflag=direct bs=8M count=2K
```

在上面命令中, if=xx 表示输入文件(即被读取的文件), of=xx 表示输出文件(即写入的文件).

这里为了测试读磁盘的速度, 所以读取一个磁盘上的文件, 然后将其写到/dev/null 这个空设备中. iflag=xx 表示打开输入文件时的标志, 此处设置为**direct 是为了绕过页面缓存**, 以得到更真实的读取磁盘的性能. bs=xx 表示一次读写传输的数据量大小, count=xx 表示执行多少次数据的读写.

用 DD 工具向磁盘上写入文件的测试, 也测试 4 种不同的块大小. 使用的命令如下:

```
dd if=/dev/zero of=dd1.dat conv=fsync oflag=direct bs=1K count=100K
dd if=/dev/zero of=dd1.dat conv=fsync oflag=direct bs=8K count=100K
dd if=/dev/zero of=dd1.dat conv=fsync oflag=direct bs=1M count=10K
dd if=/dev/zero of=dd1.dat conv=fsync oflag=direct bs=8M count=2K
```

在上面的命令中, 为了测试磁盘写入的性能, 使用了/dev/zero 这个提供空字符的特殊设备作为输入文件. conv=fsync 表示每次写入都要同步到物理磁盘设备后才返回, oflag=direct 表示使用直写的方式绕过页面缓存. conv=fsync 和 oflag=direct 这两个配置都是为了写入数据时尽可能地绕过缓存, 从而尽可能真实地反映磁盘的实际 I/O 性能.

关于 dd 命令的详细参数, 可以用 man dd 命令查看其帮助文档.

## 3.2. fio

fio 是广泛使用的对硬盘进行基准和压力测试的工具. 它支持多达 30 种 IO 引擎(ioengine), 如 sync、psync、posixaio、libaio 等.

下载 fio 源代码( https://github.com/axboe/fio/release , 本书使用写作时最新版本为 3.0 版), 解压、configure、make、make install 之后, 就可以使用 fio 命令了.

在后面的磁盘 I/O 性能中, 具体使用 fio 命令如下:

```
# 原生环境
# 顺序读
fio -filename=/dev/rhel/perf-test-lvm -rw=read -direct=1 -ioengine=sync -size=2g -numjobs=1 -bs=8k -name=robert_read
# 顺序写
fio -filename=/dev/rhel/perf-test-lvm -rw=write -direct=1 -ioengine=sync -size=2g -numjobs=1 -bs=8k -name=robert_write
# 随机读
fio -filename=/dev/rhel/perf-test-lvm -rw=randread -direct=1 -ioengine=sync -size=128m -numjobs=1 -bs=8k -name=robert_randread
# 随机写
fio -filename=/dev/rhel/perf-test-lvm -rw=randwrite -direct=1 -ioengine=sync -size=128m -numjobs=1 -bs=8k -name=robert_randwrite

# ide guest
fio -filename=/dev/sda -rw=read -direct=1 -ioengine=sync -size=2g -numjobs=1 -bs=8k -name=robert_read
fio -filename=/dev/sda -rw=write -direct=1 -ioengine=sync -size=2g -numjobs=1 -bs=8k -name=robert_write
fio -filename=/dev/sda -rw=randread -direct=1 -ioengine=sync -size=128m -num-jobs=1 -bs=8k -name=robert_randread
fio -filename=/dev/sda -rw=randwrite -direct=1 -ioengine=sync -size=128m -num-jobs=1 -bs=8k -name=robert_randwrite

# virtio guest
fio -filename=/dev/vdb -rw=read -direct=1 -ioengine=sync -size=2g -numjobs=1 -bs=8k -name=robert_read
fio -filename=/dev/vdb -rw=write -direct=1 -ioengine=sync -size=2g -numjobs=1 -bs=8k -name=robert_write
fio -filename=/dev/vdb -rw=randread -direct=1 -ioengine=sync -size=128m -numjobs=1 -bs=8k -name=robert_randread
fio -filename=/dev/vdb -rw=randwrite -direct=1 -ioengine=sync -size=128m -numjobs=1 -bs=8k -name=robert_randwrite
```

在上面的命令中, 以原生系统情况为例, 我们对/dev/rhel/perf-test-lvm 分区进行 fio 测试(\-filename), 进行的操作包括顺序读(\-rw=read)、顺序写(\-rw=write)、随机读(\-rw=randread)、随机写(\-rw=randwrite). size 参数指定了 I/O 操作的大小, 因为不同的 I/O 操作其速度不同. 为了在有限时间内完成测试, 我们指定了不同的大小(2g、128m).

I/O 引擎采用的是最传统的 sync, 以便与 dd 的数据进行类比. direct 参数指示绕过页面缓存, 与 dd 命令中的 oflag=direct 类似. bs 参数的意思是块大小(block size), 我们采用 8k 也是为了与 dd 的速率进行类比(不指定的话, 默认是 4k, 这也是绝大多数文件系统默认的块大小). numjobs 参数决定了会同时启动多少个进程并行进行 I/O 操作, 这里采用 1 也是为了与上面 dd 测试保持一致. name 参数只是给这次 fio 测试取个名字.

在 IDE guest 以及 virtio guest 中, 命令与原生系统一样, 只是将测试对象改成了/dev/sda 和/dev/vdb, 它们是/dev/rhel/perf\-test\-lvm 分区以不同形式传入客户机后, 在客户机里看到的虚拟磁盘的路径(名字).

## 3.3. Bonnie++

从 http://www.coker.com.au/bonnie++/ 网页下载 bonnie\+\+\-1.03e.tgz 文件, 然后解压, 对其进行配置、编译、安装的命令行操作如下:

```
[root@kvm-guest ~]# cd bonnie++-1.03e
[root@kvm-guest bonnie++-1.03e]# ./configure
[root@kvm-guest bonnie++-1.03e]# make
[root@kvm-guest bonnie++-1.03e]# make install
```

也可以从 Fedora EPEL(Extra Packages for Enterprise Linux) https://fedoraproject.org/wiki/EPEL , 通过 yum 直接在 RHEL7 中安装 Bonnie++.

本次测试使用 Bonnie\+\+的命令如下:

```
bonnie++ -d /mnt/raw_disk -D -b -m kvm-guest -x 3 -u root
```

其中, \-d 指定测试对象是哪个目录(我们测试用的分区 mount 到哪里), \-D 表示在批量 I/O 测试时使用直接 I/O 的方式(O\_DIRECT), \-b 的含义等同于 fio 的'\-direct=1'和 dd 工具的'conv=fsync', 指绕过写缓存, 每次写操作都同步到磁盘, \-m kvm\-guest 表示 Bonnie\+\+得到的主机名为 kvm\-guest, \-x 3 表示循环执行 3 遍测试, \-u root 表示以 root 用户运行测试.
在执行完测试后, 默认会在当前终端上输出测试结果. 可以将其 CSV 格式的测试结果通过 Bonnie\+\+提供的 bon\_csv2html 转化为更容易读的 HTML 文档. 命令行操作如下:

```
[root@kvm-guest bonnie++-1.03e]# echo "native,4G,102817,88,58631,25,56712,4,108330,91,151383,7,299.0,1,16,+++++,+++,+++++,+++,++++"| perl bon_csv2html > native-bonnie-1.html
```

Bonnie\+\+是一个强大的测试硬盘和文件系统的工具, 关于 Bonnie++命令的用法, 可以用 man bonnie++命令获取帮助手册, 关于 Bonnie\+\+工具的原理及测试方法的简介, 可以参考其源代码中的 readme.html 文档.

# 4. 性能测试数据

分别用 DD、fio、Bonnie\+\+这 3 个工具在原生系统和 KVM 客户机中进行测试, 然后对比其测试结果数据. 为了尽量减小误差, 每个测试项目都收集了 3 次测试数据, 下面提供的测试数据都是根据 3 次测试计算出的平均值.

## 4.1. DD

使用 DD 工具测试磁盘读写性能, 将得到的测试数据进行对比, 如图 10\-11 和图 10\-12 所示. 读写速率越大, 说明磁盘 I/O 性能越好.

![](./images/2019-05-11-23-19-52.png)

![](./images/2019-05-11-23-39-19.png)

根据图 10-11 和图 10-12 中所示的数据可知, virtio 方式的磁盘比纯模拟的 IDE 磁盘在小数据块时(bs=1K, bs=8K)读写性能要好, 在大数据块时(bs=1M, bs=8M)virtio 的读写速度与纯模拟的 IDE 情况差不多, 甚至在 8M 数据块情况下还逊于后者. 在数据块较大时(如 bs=1M, bs=8M), KVM 客户机中 virtio 和 IDE 两种方式的磁盘的读写性能都与原生系统相差不大.

## 4.2. fio

fio 的基准测试数据对比如图 10-13 所示. 顺序读、写的速度与上面 DD 的数据一致: Qemu 模拟的 IDE 硬盘, 性能只有原生的大约一半左右, 而 virtio 的情况则几乎和原生系统一样, 甚至顺序写的性能还略好于原生系统一点点. 随机读、写是 DD 没有涵盖的测试项, 从 fio 的结果我们可以看到, 随机读写的速率要比顺序读写的速率低得多, 在很低速的情况下, 原生系统、模拟 IDE、virtio 的性能都差不多, 这也与上面 DD 在 bs=1k 的低速情况下的性能表现相吻合.

## 4.3. Bonnie++

图 10-14 所示为 Bonnie++中文件读写操作的测试结果对比, 其中的 Seq-O-Per-Char 表示顺序按字节写、Seq-O-Blk 表示顺序按块写, Seq-O-Rewr 就表示先读后写并 lseek, Seq-I-Per-Char 表示顺序按字节读, Seq-I-Blk 表示顺序按块读. 图 10-15 为文件创建、删除操作的测试结果对比, 其中 Rand、Seek 表示随机改变文件读写指针偏移量(使用 lseek()和 random()函数), Seq-Create、Seq-Del 等顾名思义, 不赘述了.

本次 Bonnie++测试没有指定一次读写的数据块的大小, 默认值是 8 KB, 所以, 图 10-15 所示的读写速率测试结果与前面 DD、fio 工具测试每次读写 8 KB 数据时的测试结果大体一致, 相差不大. 但仔细观察可以看到, 不同于 DD 和 fio, Bonnie++中的不少测试项原生系统的数据反而不如虚拟环境的模拟 IDE 和 Virtio-Blk, 具体的原因要分析其源代码, 这里不深究了.

![](./images/2019-05-11-23-40-49.png)

![](./images/2019-05-11-23-41-04.png)

![](./images/2019-05-11-23-41-18.png)

从 DD、fio、Bonnie\+\+的测试结果可以看出, 当一次读写的数据块较小时, KVM 客户机中的磁盘读写速率比非虚拟化原生系统慢得多, 而一次读写数据块较大时, 磁盘 I/O 性能则差距不大. 在一般情况下, 用 virtio 方式的磁盘 I/O 性能比纯模拟的 IDE 磁盘要好一些.
