
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1 前景回顾](#1-前景回顾)
  - [1.1 Linux 的调度器组成](#11-linux-的调度器组成)
    - [1.1.1 2 个调度器](#111-2-个调度器)
    - [1.1.2 6 种调度策略](#112-6-种调度策略)
    - [1.1.3 5 个调度器类](#113-5-个调度器类)
    - [1.1.4 3 个调度实体](#114-3-个调度实体)
  - [1.2 调度工作](#12-调度工作)
- [2 进程上下文](#2-进程上下文)
  - [2.1 进程上下文的概念](#21-进程上下文的概念)
  - [2.2 上下文切换](#22-上下文切换)
- [3 context\_switch 进程上下文切换](#3-context_switch-进程上下文切换)
  - [3.1 context\_switch 完全注释](#31-context_switch-完全注释)

<!-- /code_chunk_output -->

前面我们了解了 linux 进程调度器的设计思路和注意框架

- 周期调度器 scheduler\_tick 通过 linux 定时器周期性的被激活, 进行程序调度

- 进程主动放弃 CPU 或者发生阻塞时, 则会调用主调度器 schedule 进行程序调度

在分析的过程中, 我们提到了内核抢占和用户抢占的概念, 但是并没有详细讲, 因此我们在这里详细分析一下子

CPU 抢占分两种情况, **用户抢占**, **内核抢占**

其中内核抢占是在 Linux2.5.4 版本发布时加入,同 SMP(Symmetrical Multi-Processing,对称多处理器),作为**内核的可选配置**.

# 1 前景回顾

## 1.1 Linux 的调度器组成

### 1.1.1 2 个调度器

可以用**两种方法来激活调度**

- 一种是**直接的**, 比如**进程打算睡眠**或出于**其他原因放弃 CPU**

- 另一种是通过**周期性的机制**, 以**固定的频率运行**, 不时的检测是否有必要

因此当前**linux 的调度程序**由**两个调度器组成**:

- **主调度器**

- **周期性调度器**

两者又统称为**通用调度器(generic scheduler)**或**核心调度器(core scheduler)**

并且**每个调度器**包括两个内容: **调度框架**(其实质就是**两个函数框架**)及**调度器类**

### 1.1.2 6 种调度策略

linux 内核目前实现了**6 种调度策略(即调度算法**), 用于对不同类型的进程进行调度,或者支持某些特殊的功能

- **SCHED\_NORMAL**和**SCHED\_BATCH**调度**普通的非实时进程**

- **SCHED\_FIFO**和**SCHED\_RR**和**SCHED\_DEADLINE**则采用不同的调度策略**调度实时进程**

- **SCHED\_IDLE**则在**系统空闲时调用 idle 进程**.

### 1.1.3 5 个调度器类

而依据其调度策略的不同实现了**5 个调度器类**,一个调度器类可以用**一种或者多种调度策略**调度某一类进程, 也可以用于特殊情况或者调度特殊功能的进程.

其所属进程的优先级顺序为
```c
stop_sched_class -> dl_sched_class -> rt_sched_class -> fair_sched_class -> idle_sched_class
```

### 1.1.4 3 个调度实体

调度器**不限于调度进程**, 还可以调度更大的实体, 比如实现**组调度**.

这种一般性要求**调度器不直接操作进程**,而是**处理可调度实体**,因此需要一个通用的数据结构描述这个调度实体,即**seched\_entity 结构**,其实际上就代表了一个**调度对象**, 可以为**一个进程**, 也可以为**一个进程组**.

linux 中针对当前**可调度的实时**和**非实时进程**, 定义了类型为**seched\_entity 的 3 个调度实体**

- **sched\_dl\_entity** 采用**EDF 算法调度的实时调度实体**

- **sched\_rt\_entity** 采用**Roound-Robin 或者 FIFO 算法调度的实时调度实体**

- **sched\_entity** 采用 CFS 算法调度的**普通非实时进程的调度实体**

## 1.2 调度工作

周期性调度器通过调用各个调度器类的 task\_tick 函数完成周期性调度工作

- 如果当前进程是**完全公平队列**中的进程,则首先根据当前就绪队列中的进程数算出一个**延迟时间间隔**, 大概每个进程分配 2ms 时间, 然后按照该进程在队列中的总权重中占得比例, 算出它**该执行的时间 X**, 如果该进程**执行物理时间超过了 X**, 则**激发延迟调度**; 如果**没有超过 X**, 但是红黑树就绪队列中**下一个进程优先级更高**, 即 curr->vruntime-leftmost->vruntime > X,也将**延迟调度**

- 如果当前进程是**实时调度类**中的进程: 则如果该进程是 SCHED\_RR, 则递减时间片[为 HZ/10], 到期, 插入到队列尾部, 并激发延迟调度, 如果是 SCHED\_FIFO, 则什么也不做, 直到该进程执行完成

**延迟调度**的真正调度过程在: **schedule 中实现**, 会按照**调度类顺序**和**优先级**挑选出一个**最高优先级的进程执行**

而对于主调度器则直接**关闭内核抢占后**, 通过**调用 schedule**来完成**进程的调度**

可见**不管是周期性调度器还是主调度器**,内核中的许多地方,如果要将 CPU 分配给与当前活动进程不同的另外一个进程(即抢占), 都会直接或者调用调度函数,包括 schedule 或者其子函数\_\_schedule,其中 schedule 在关闭内核抢占后调用\_\_schedule 完成了抢占.

而\_\_schedule 则执行了如下操作

**函数__schedule 如何完成内核抢占**

1. 完成一些**必要的检查**, 并设置进程状态, 处理进程所在的就绪队列

2. 调度全局的 pick\_next\_task 选择抢占的进程

 - 如果当前 cpu 上所有的进程都是 cfs 调度的普通非实时进程, 则直接用 cfs 调度, 如果无程序可调度则调度 idle 进程

 - 否则从优先级最高的调度器类 sched\_class\_highest(目前是 stop\_sched\_class)开始依次遍历所有调度器类的 pick\_next\_task 函数, 选择最优的那个进程执行

3. context\_switch 完成进程上下文切换

即进程的抢占或者切换工作是由 context\_switch 完成的

那么我们今天就详细讲解一下 context\_switch 完成进程上下文切换的原理

# 2 进程上下文

## 2.1 进程上下文的概念

操作系统管理**很多进程的执行**.有些进程是来自各种程序、系统和应用程序的单独进程, 而某些进程来自被分解为很多进程的应用或程序. 当一个进程从内核中移出, 另一个进程成为活动的,这些进程之间便发生了上下文切换.操作系统必须记录重启进程和启动新进程使之活动**所需要的所有信息**.这些信息被称作**上下文**,它描述了进程的现有状态,进程上下文是可执行程序代码是进程的重要组成部分,实际上是进程执行活动全过程的静态描述,可以看作是用户进程传递给内核的这些参数以及内核要保存的那一整套的变量和寄存器值和当时的环境等

进程的**上下文信息**包括: **指向可执行文件的指针**, **栈**, **内存(数据段和堆**), **进程状态**, **优先级**, **程序 I/O 的状态**, **授予权限**, **调度信息, 审计信息, 有关资源的信息(文件描述符和读/写指针), 关事件和信号的信息, 寄存器组(栈指针, 指令计数器)等等**, 诸如此类.

**处理器总处于以下三种状态之一(！！！**)

1. **内核态**, 运行于**进程上下文**, 内核**代表进程**运行于**内核空间**;

2. **内核态**, 运行于**中断上下文**, 内核**代表硬件**运行于**内核空间**;

3. **用户态**, 运行于**用户空间**.

**用户空间的应用程序**, 通过**系统调用**, 进入**内核空间**. 这个时候**用户空间**的进程要**传递 很多变量、参数的值给内核**, **内核态运行**的时候也要**保存用户进程的一些寄存器值、变量**等. 所谓的"进程上下文"

**硬件**通过**触发信号**, 导致内核**调用中断处理程序**, 进入**内核空间**. 这个过程中, 硬件的 一些**变量和参数也要传递给内核**, 内核通过这些参数进行中断处理. 所谓的"中断上下文", 其实也可以看作就是硬件传递过来的这些参数和内核需要保存的一些**其他环境**(主要是当前**被打断执行的进程环境**).

>LINUX 完全注释中的一段话
>
>当一个进程在执行时,CPU 的所有寄存器中的值、进程的状态以及堆栈中的内容被称为该进程的上下文. 当内核需要切换到另一个进程时, 它需要保存当前进程的所有状态, 即保存当前进程的上下文, 以便在再次执行该进程时, 能够必得到切换时的状态执行下去. 在 LINUX 中, **当前进程上下文**均保存在进程的**任务数据结构**中. 在**发生中断**时,内核就在被中断进程的上下文中, 在内核态下**执行中断服务例程**. 但同时会保留所有需要用到的资源, 以便中继服务结束时能恢复被中断进程的执行.

## 2.2 上下文切换

进程被抢占 CPU 时候, 操作系统保存其上下文信息, 同时将新的活动进程的上下文信息加载进来, 这个过程其实就是**上下文切换**, 而当一个被抢占的进程再次成为活动的, 它可以恢复自己的上下文继续从被抢占的位置开始执行. 参见维基百科-[context](https://en.wikipedia.org/wiki/Context_(computing), [context switch](https://en.wikipedia.org/wiki/Context_switch)

**上下文切换**(有时也称做**进程切换**或**任务切换**)是指 CPU 从一个进程或线程切换到另一个进程或线程

稍微详细描述一下, 上下文切换可以认为是内核(操作系统的核心)在**CPU 上对于进程(包括线程)进行以下的活动**:

1. 挂起一个进程, 将这个进程在 CPU 中的状态(上下文)存储于内存中的某处,

2. 在内存中检索下一个进程的上下文并将其在 CPU 的寄存器中恢复

3. 跳转到程序计数器所指向的位置(即跳转到进程被中断时的代码行), 以恢复该进程

因此上下文是指**某一时间点 CPU 寄存器和程序计数器**的内容,广义上还包括**内存中进程的虚拟地址映射信息**.

**上下文切换只能发生在内核态中**,上下文切换通常是计算密集型的. 也就是说, 它需要相当可观的处理器时间, 在每秒几十上百次的切换中, 每次切换都需要纳秒量级的时间. 所以, 上下文切换对系统来说意味着消耗大量的 CPU 时间, 事实上, 可能是操作系统中时间消耗最大的操作.

Linux 相比与其他操作系统(包括其他类 Unix 系统)有很多的优点, 其中有一项就是, 其上下文切换和模式切换的时间消耗非常少.

# 3 context\_switch 进程上下文切换

linux 中进程调度时, 内核在选择新进程之后进行抢占时, 通过 context\_switch 完成进程上下文切换.

>**注意** 进程调度与抢占的区别
>
>**进程调度不一定发生抢占**, 但是**抢占时却一定发生了调度**
>
>在**进程发生调度**时, 只有当前内核发生**当前进程因为主动或者被动需要放弃 CPU**时, 内核才会选择一个与当前活动进程不同的进程来抢占 CPU

context\_switch 其实是一个**分配器**, 他会**调用所需的特定体系结构的方法**

- 调用 switch\_mm(), 把**虚拟内存**从一个进程映射切换到新进程中

 - switch\_mm 更换通过 task\_struct->mm 描述的**内存管理上下文**,该**工作的细节取决于处理器(！！！**),主要包括**加载页表**,刷出**地址转换后备缓冲器**(部分或者全部),向**内存管理单元(MMU**)提供新的信息

- 调用 switch\_to(),从上一个进程的处理器状态切换到新进程的处理器状态. 这包括保存、恢复栈信息和寄存器信息

 - switch\_to 切换处理器**寄存器的内容和内核栈**(虚拟地址空间的用户部分已经通过 switch\_mm 变更, 其中也包括了用户状态下的栈, 因此 switch\_to 不需要变更用户栈, 只需变更内核栈), 此段代码**严重依赖于体系结构**, 且代码通常都是用汇编语言编写.

context\_switch 函数**建立 next 进程的地址空间**. 进程描述符的 active\_mm 字段指向进程**所使用**的内存描述符, 而 mm 字段指向进程**所拥有**的内存描述符. 对于一般的进程, 这两个字段有相同的地址, 但是, **内核线程**没有它自己的**地址空间**而且它的**mm 字段总是被设置为 NULL;active\_mm 成员被初始化为前一个运行进程的 mm 值,如果当前内核线程被调度之前运行的也是另外一个内核线程时候, 那么其 mm 和 avtive\_mm 都是 NULL**.

context\_switch()函数保证: **如果 next 是一个内核线程, 它使用 prev 所使用的地址空间**

由于不同架构下地址映射的机制有所区别,而寄存器等信息弊病也是依赖于架构的,因此 switch\_mm 和 switch\_to 两个函数均是**体系结构相关**的

## 3.1 context\_switch 完全注释

context\_switch 定义在[kernel/sched/core.c#L2711](http://lxr.free-electrons.com/source/kernel/sched/core.c#L2711), 如下所示

```cpp
/*
 * context_switch - switch to the new MM and the new thread's register state.
 */
static __always_inline struct rq *
context_switch(struct rq *rq, struct task_struct *prev,
           struct task_struct *next)
{
    struct mm_struct *mm, *oldmm;

    /*  完成进程切换的准备工作  */
    prepare_task_switch(rq, prev, next);

    mm = next->mm;
    oldmm = prev->active_mm;
    /*
     * For paravirt, this is coupled with an exit in switch_to to
     * combine the page table reload and the switch backend into
     * one hypercall.
     */
    arch_start_context_switch(prev);

    /*  如果 next 是内核线程, 则线程使用 prev 所使用的地址空间
     *  schedule( )函数把该线程设置为懒惰 TLB 模式
     *  内核线程并不拥有自己的页表集(task_struct->mm = NULL)
     *  它使用一个普通进程的页表集
     *  不过, 没有必要使一个用户态线性地址对应的 TLB 表项无效
     *  因为内核线程不访问用户态地址空间.
    */
    if (!mm)        /*  内核线程无虚拟地址空间, mm = NULL*/
    {
        /*  内核线程的 active_mm 为上一个进程的 mm
         *  注意此时如果 prev 也是内核线程,
         *  则 oldmm 为 NULL, 即 next->active_mm 也为 NULL  */
        next->active_mm = oldmm;
        /*  增加 mm 的引用计数  */
        atomic_inc(&oldmm->mm_count);
        /*  通知底层体系结构不需要切换虚拟地址空间的用户部分
         *  这种加速上下文切换的技术称为惰性 TBL  */
        enter_lazy_tlb(oldmm, next);
    }
    else  /* 不是内核线程, 则需要切切换虚拟地址空间 */
        switch_mm(oldmm, mm, next);

    /*  如果 prev 是内核线程或正在退出的进程
     *  就重新设置 prev->active_mm
     *  然后把指向 prev 内存描述符的指针保存到运行队列的 prev_mm 字段中
     */
    if (!prev->mm)
    {
        /*  将 prev 的 active_mm 赋值和为空  */
        prev->active_mm = NULL;
        /*  更新运行队列的 prev_mm 成员  */
        rq->prev_mm = oldmm;
    }
    /*
     * Since the runqueue lock will be released by the next
     * task (which is an invalid locking op but in the case
     * of the scheduler it's an obvious special-case), so we
     * do an early lockdep release here:
     */
    lockdep_unpin_lock(&rq->lock);
    spin_release(&rq->lock.dep_map, 1, _THIS_IP_);

    /* Here we just switch the register state and the stack.
     * 切换进程的执行环境, 包括堆栈和寄存器
     * 同时返回上一个执行的程序
     * 相当于 prev = switch_to(prev, next)  */
    switch_to(prev, next, prev);

    /*  switch_to 之后的代码只有在
     *  当前进程再次被选择运行(恢复执行)时才会运行
     *  而此时当前进程恢复执行时的上一个进程可能跟参数传入时的 prev 不同
     *  甚至可能是系统中任意一个随机的进程
     *  因此 switch_to 通过第三个参数将此进程返回
     */


    /*  路障同步, 一般用编译器指令实现
     *  确保了 switch_to 和 finish_task_switch 的执行顺序
     *  不会因为任何可能的优化而改变  */
    barrier();

    /*  进程切换之后的处理工作  */
    return finish_task_switch(prev);
}
````

## 3.2 prepare\_arch\_switch 切换前的准备工作

在进程切换之前,首先执行调用每个体系结构都必须定义的 prepare\_task\_switch 挂钩,这使得内核执行特定于体系结构的代码,为切换做事先准备.**大多数支持的体系结构都不需要该选项**

```c
struct mm_struct *mm, *oldmm;

prepare_task_switch(rq, prev, next); /*  完成进程切换的准备工作  */
```

prepare\_task\_switch 函数定义在[kernel/sched/core.c, line 2558](http://lxr.free-electrons.com/source/kernel/sched/core.c?v=4.6#L2558), 如下所示

```c
/**
 * prepare_task_switch - prepare to switch tasks
 * @rq: the runqueue preparing to switch
 * @prev: the current task that is being switched out
 * @next: the task we are going to switch to.
 *
 * This is called with the rq lock held and interrupts off. It must
 * be paired with a subsequent finish_task_switch after the context
 * switch.
 *
 * prepare_task_switch sets up locking and calls architecture specific
 * hooks.
 */
static inline void
prepare_task_switch(struct rq *rq, struct task_struct *prev,
            struct task_struct *next)
{
    sched_info_switch(rq, prev, next);
    perf_event_task_sched_out(prev, next);
    fire_sched_out_preempt_notifiers(prev, next);
    prepare_lock_switch(rq, next);
    prepare_arch_switch(next);
}
```

## 3.3 next 是内核线程时的处理

由于**用户空间进程的寄存器**内容在**进入核心态**时**保存在内核栈**中,在**上下文切换**期间**无需显式操作**. 而因为**每个进程首先都是从核心态开始执行(！！！**)(在**调度期间控制权传递给新进程**),在**返回用户空间**时, 会使用**内核栈上保存的值自动恢复寄存器数据**.

另外需要注意, **内核线程没有自身的用户空间上下文**, 其 task\_struct\->mm 为 NULL, 参见[Linux 内核线程 kernel thread 详解--Linux 进程的管理与调度(十)](http://blog.csdn.net/gatieme/article/details/51589205#t3), 从当前进程"借来"的地址空间记录在 active\_mm 中

```c
/*  如果 next 是内核线程, 则线程使用 prev 所使用的地址空间
 *  schedule( )函数把该线程设置为懒惰 TLB 模式
 *  内核线程并不拥有自己的页表集(task_struct->mm = NULL)
 *  它使用一个普通进程的页表集
 *  不过, 没有必要使一个用户态线性地址对应的 TLB 表项无效
 *  因为内核线程不访问用户态地址空间.
*/
if (!mm)        /*  内核线程无虚拟地址空间, mm = NULL*/
{
    /*  内核线程的 active_mm 为上一个进程的 mm
     *  注意此时如果 prev 也是内核线程,
     *  则 oldmm 为 NULL, 即 next->active_mm 也为 NULL  */
    next->active_mm = oldmm;
    /*  增加 mm 的引用计数  */
    atomic_inc(&oldmm->mm_count);
    /*  通知底层体系结构不需要切换虚拟地址空间的用户部分
     *  这种加速上下文切换的技术称为惰性 TLB  */
    enter_lazy_tlb(oldmm, next);
}
else            /*  不是内核线程, 则需要切切换虚拟地址空间  */
    switch_mm(oldmm, mm, next);
````

其中**enter\_lazy_tlb**通知**底层体系结构**不需要切换**虚拟地址空间的用户空间部分(！！！**),这种加速上下文切换的技术称之为**惰性 TLB**

## 3.4 switch\_mm 切换进程虚拟地址空间

### 3.4.1 switch\_mm 函数

switch\_mm 主要完成了进程 prev 到 next**虚拟地址空间**的映射,由于**内核虚拟地址空间是不许切换**的,因此切换的主要是**用户态的虚拟地址空间**

这个是一个体系结构相关的函数,其实现在对应体系结构下的[arch/对应体系结构/include/asm/mmu\_context.h](http://lxr.free-electrons.com/ident?v=4.6;i=switch_mm)文件中, 我们下面列出了几个常见体系结构的实现

| 体系结构 | switch\_mm 实现 |
| ------- |:-------|
| x86 | [arch/x86/include/asm/mmu_context.h, line 118](http://lxr.free-electrons.com/source/arch/x86/include/asm/mmu_context.h?v=4.6#L118) |
| arm | [arch/arm/include/asm/mmu_context.h, line 126](http://lxr.free-electrons.com/source/arch/arm/include/asm/mmu_context.h?v=4.6#L126) |
| arm64 | [arch/arm64/include/asm/mmu_context.h, line 183](http://lxr.free-electrons.com/source/arch/arm64/include/asm/mmu_context.h?v=4.6#L183)

其主要工作就是**切换了进程的 CR3**

### 3.4.2 CPU-CR0\~CR4 寄存器

控制寄存器(CR0~CR3)用于控制和确定处理器的操作模式以及当前执行任务的特性

| 控制寄存器 | 描述 |
| ------- |:-------|
| CR0 | 含有**控制处理器操作模式**和**状态的系统控制标志** |
| CR1 | **保留不用**, 未定义的控制寄存器, 供将来的处理器使用 |
| CR2 | 错误的线性地址 |
| CR3 | 含有**页目录表物理内存基地址**, 因此该寄存器也被称为页目录基地址寄存器 PDBR(Page-Directory Base address Register), 保存**页目录表的物理地址**, 页目录表总是放在**以 4K 字节为单位的存储器边界**上, 因此, 它的地址的低 12 位总为 0, 不起作用, 即使写上内容, 也不会被理会 |
| CR4 | 在 Pentium 系列(包括**486 的后期版本**)处理器中才实现, 它处理的事务包括诸如何时**启用虚拟 8086 模式**等 |

### 3.4.3 保护模式下的 GDT、LDT 和 IDT

**保护模式**下**三个重要的系统表 GDT、LDT 和 IDT**

这三个表是在**内存**中由**操作系统**或**系统程序员所建**, 并不是固化在哪里, 所以从理论上是**可以被读写**的.

这三个表都是**描述符表**. 描述符表是由若**干个描述符**组成, 每个描述符占用**8 个字节**的内存空间, **每个描述符表**内最多可以有**8129 个**描述符. 描述符是**描述一个段的大小, 地址及各种状态**的.

描述符表有三种,分别为**全局描述符表 GDT**、**局部描述符表 LDT**和**中断描述符表 IDT**

| 描述符表 | 描述 |
| ------- |:-------|
| 全局描述符表 GDT | 全局描述符表在**系统中只能有一个**,且可以被每一个任务所**共享**.任何描述符都可以放在 GDT 中,但**中断门和陷阱门放在 GDT 中是不会起作用**的.能被**多个任务共享的内存区**就是**通过 GDT 完成**的 |
| 局部描述符表 LDT | 局部描述符表在系统中**可以有多**个,通常情况下是与任务的数量保持对等,但**任务可以没有局部描述符表**.<br>任务间不相干的部分也是通过 LDT 实现的.这里涉及到**地址映射**的问题.<br><br>和 GDT 一样,**中断门和陷阱门放在 LDT 中是不会起作用**的. |
| 中断描述符表 IDT | 和 GDT 一样,中断描述符表在系统最多**只能有一个**,中断描述符表内可以存放**256 个描述符**,分别对应**256 个中断**.因为每个描述符占用 8 个字节,所以**IDT 的长度可达 2K**.<br>中断描述符表中可以有任务门、中断门、陷阱门三个门描述符, 其它的描述符在中断描述符表中无意义 |

**中断门和陷阱门描述符只允许放在 IDT**中,**任务门可以位于 GDT,LDT 和 IDT 中！！！**

**段选择子**

在保护模式下,段寄存器的内容已不是段值,而称其为选择子.该选择子指示描述符在上面这三个表中的位置,所以说选择子即是索引值. 当我们把段选择子装入寄存器时不仅使该寄存器值, 同时 CPU 将该选择子所对应的 GDT 或 LDT 中的描述符装入了不可见部分. 这样只要我们不进行代码切换(不重新装入新的选择子)CPU 就不会对不可见部分存储的描述符进行更新, 可以直接进行访问, 加快了访问速度. 一旦寄存器被重新赋值, 不可见部分也将被重新赋值.

**关于选择子的值是否连续**

关于选择子的值, 我认为**不一定要连续**. 但是**每个描述符的起始地址**相对于第一个描述符(即空描述符)的首地址的偏移必须是**8 的倍数**, 即二进制最后三位为 0. 这样通过全局描述符表寄存器 GDTR 找到全局描述符表的首地址后, 使用段选择子的高 13 位索引到正确的描述符表项(段选择子的高 13 位左移 3 位加上 GDTR 的值即为段选择子指定的段描述符的逻辑首地址)

也就是说在两个段选择符之间可以填充能被 8 整除个字节值. 当然, 如果有选择子指向了这些填充的字节, 一般会出错, 除非你有意填充一些恰当的数值, 呵呵.

**关于为什么 LDT 要放在 GDT 中 -LDT 中的描述符和 GDT 中的描述符**

除了选择子的 bit3 一个为 0 一个为 1 用于区分该描述符是在 GDT 中还是在 LDT 中外, 描述符本身的结构完全一样.
开始我考虑既然是这样, 为什么要将 LDT 放在 GDT 中而不是像 GDT 那样找一个 GDTR 寄存器呢?

后来终于明白了原因——很简单, GDT 表只有一个, 是固定的; 而 LDT 表每个任务就可以有一个, 因此有多个, 并且由于任务的个数在不断变化其数量也在不断变化.

如果只有一个 LDTR 寄存器显然不能满足多个 LDT 的要求. 因此 INTEL 的做法是把它放在放在 GDT 中.

### 3.4.4 switch\_mm 函数注释

下面我们提取了 x86 架构下的 switch\_mm 函数, 其定义在[arch/x86/include/asm/mmu_context.h, line 118](http://lxr.free-electrons.com/source/arch/x86/include/asm/mmu_context.h?v=4.6#L118)

```c
//  http://lxr.free-electrons.com/source/arch/x86/include/asm/mmu_context.h?v=4.6#L118
static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,
                 struct task_struct *tsk)
{
    unsigned cpu = smp_processor_id();


    /*  确保 prev 和 next 不是同一进程  */
    if (likely(prev != next))
    {
#ifdef CONFIG_SMP
        /*  刷新 cpu 地址转换后备缓冲器 TLB  */
        this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);
        this_cpu_write(cpu_tlbstate.active_mm, next);
#endif
        /*  设置当前进程的 mm->cpu_vm_mask 表示其占用 cpu  */
        cpumask_set_cpu(cpu, mm_cpumask(next));

        /*
         * Re-load page tables.
         *
         * This logic has an ordering constraint:
         *
         *  CPU 0: Write to a PTE for 'next'
         *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.
         *  CPU 1: set bit 1 in next's mm_cpumask
         *  CPU 1: load from the PTE that CPU 0 writes (implicit)
         *
         * We need to prevent an outcome in which CPU 1 observes
         * the new PTE value and CPU 0 observes bit 1 clear in
         * mm_cpumask.  (If that occurs, then the IPI will never
         * be sent, and CPU 0's TLB will contain a stale entry.)
         *
         * The bad outcome can occur if either CPU's load is
         * reordered before that CPU's store, so both CPUs must
         * execute full barriers to prevent this from happening.
         *
         * Thus, switch_mm needs a full barrier between the
         * store to mm_cpumask and any operation that could load
         * from next->pgd.  TLB fills are special and can happen
         * due to instruction fetches or for no reason at all,
         * and neither LOCK nor MFENCE orders them.
         * Fortunately, load_cr3() is serializing and gives the
         * ordering guarantee we need.
         *
         * 将新进程的 pgd 页目录表填写到 cpu 的 cr3 寄存器中
         */
        load_cr3(next->pgd);

        trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);

        /* Stop flush ipis for the previous mm
         * 除 prev 的 cpu_vm_mask, 表示 prev 放弃使用 cpu  */
        cpumask_clear_cpu(cpu, mm_cpumask(prev));

        /* Load per-mm CR4 state
         */
        load_mm_cr4(next);

#ifdef CONFIG_MODIFY_LDT_SYSCALL
        /*
         * Load the LDT, if the LDT is different.
         *
         * It's possible that prev->context.ldt doesn't match
         * the LDT register.  This can happen if leave_mm(prev)
         * was called and then modify_ldt changed
         * prev->context.ldt but suppressed an IPI to this CPU.
         * In this case, prev->context.ldt != NULL, because we
         * never set context.ldt to NULL while the mm still
         * exists.  That means that next->context.ldt !=
         * prev->context.ldt, because mms never share an LDT.
         *
         *
         */
        if (unlikely(prev->context.ldt != next->context.ldt))
            load_mm_ldt(next);
#endif
    }
#ifdef CONFIG_SMP
    else
    {
        this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);
        BUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);

        if (!cpumask_test_cpu(cpu, mm_cpumask(next)))
        {
            /*
             * On established mms, the mm_cpumask is only changed
             * from irq context, from ptep_clear_flush() while in
             * lazy tlb mode, and here. Irqs are blocked during
             * schedule, protecting us from simultaneous changes.
             */
            cpumask_set_cpu(cpu, mm_cpumask(next));

            /*
             * We were in lazy tlb mode and leave_mm disabled
             * tlb flush IPI delivery. We must reload CR3
             * to make sure to use no freed page tables.
             *
             * As above, load_cr3() is serializing and orders TLB
             * fills with respect to the mm_cpumask write.
             */
            load_cr3(next->pgd);
            trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
            load_mm_cr4(next);
            load_mm_ldt(next);
        }
    }
#endif
}
```

## 3.5 prev 是内核线程时的处理

如果**前一个进程 prev**是**内核线程(即 prev->mm 为 NULL**),则其**active\_mm 指针必须重置为 NULL**,已断开其与之前借用的地址空间的联系,而当 prev 重新被调度的时候,此时它成为 next 会在前面 next 是内核线程时的处理处重新用`next->active_mm = oldmm;`赋值, 这个我们刚讲过


```c
/*  如果 prev 是内核线程或正在退出的进程
 *  就重新设置 prev->active_mm
 *  然后把指向 prev 内存描述符的指针保存到运行队列的 prev_mm 字段中
 */
if (!prev->mm)
{
    /*  将 prev 的 active_mm 赋值和为空  */
    prev->active_mm = NULL;
    /*  更新运行队列的 prev_mm 成员  */
    rq->prev_mm = oldmm;
}
```

## 3.6 switch\_to 完成进程切换

### 3.6.1 switch\_to 函数

最后用 switch\_to 完成了进程的切换,该函数**切换了寄存器状态和栈**,新进程在**该调用后开始执行**,而 switch\_to 之后的代码只有在当前进程下一次被选择运行时才会执行

执行环境的切换是在 switch\_to()中完成的,switch\_to 完成最终的进程切换, 它保存原进程的所有寄存器信息, 恢复新进程的所有寄存器信息, 并执行新的进程

该函数往往通过宏来实现, 其原型声明如下

```c
/*
 * Saving eflags is important. It switches not only IOPL between tasks,
 * it also protects other tasks from NT leaking through sysenter etc.
*/
#define switch_to(prev, next, last)
```

| 体系结构 | switch_to 实现 |
| ------- |:-------|
| x86 | arch/x86/include/asm/switch\_to.h 中两种实现<br> [定义 CONFIG\_X86\_32 宏](http://lxr.free-electrons.com/source/arch/x86/include/asm/switch_to.h?v=4.6#L27)<br><br>[未定义 CONFIG\_X86\_32 宏](http://lxr.free-electrons.com/source/arch/x86/include/asm/switch_to.h?v=4.6#L103) |
| arm | [arch/arm/include/asm/switch_to.h, line 25](http://lxr.free-electrons.com/source/arch/arm/include/asm/switch_to.h?v=4.6#L18) |
| 通用 | [include/asm-generic/switch_to.h, line 25](http://lxr.free-electrons.com/source/include/asm-generic/switch_to.h?v=4.6#L25) |

内核在 switch\_to 中执行如下操作

1. **进程切换**, 即**esp 的切换**, 由于**从 esp 可以找到进程的描述符**

2. **硬件上下文切换**, 设置**ip 寄存器**的值, 并 jmp 到\_\_switch\_to 函数

3. **堆栈的切换**, 即**ebp 的切换**,ebp 是**栈底指针**,它确定了**当前用户空间属于哪个进程**

\_\_switch\_to 函数

| 体系结构 | \_\_switch\_to 实现 |
| ------- |:-------:|
| x86 | [arch/x86/kernel/process\_32.c, line 242](http://lxr.free-electrons.com/source/arch/x86/kernel/process_32.c?v=4.6#L242) |
| x86\_64 | [arch/x86/kernel/process\_64.c, line 277](http://lxr.free-electrons.com/source/arch/x86/kernel/process_64.c?v=4.6#L277) |
| arm64 | [arch/arm64/kernel/process.c, line 329](http://lxr.free-electrons.com/source/arch/arm64/kernel/process.c?v=4.6#L329)

### 3.6.2 为什么 switch\_to 需要 3 个参数

调度过程**可能选择了一个新的进程**,而**清理工作则是针对此前的活动进程**,请注意,**这不是发起上下文切换的那个进程(！！！**),而是系统中随机的某个其他进程,内核必须想办法使得**进程能够与 context\_switch 例程通信**,这就可以通过 switch\_to 宏实现.因此 switch\_to 函数通过 3 个参数提供 2 个变量.

在**新进程被选中**时,底层的**进程切换例程(！！！**)必须将**此前执行的进程**提供给 context\_switch,由于控制流会回到陔函数的中间,这无法用普通的函数返回值来做到, 因此提供了 3 个参数的宏

我们考虑这个样一个例子,假定多个进程 A,B,C...在系统上运行,在某个时间点,内核决定从**进程 A 切换到进程 B**, 此时**prev = A, next = B**,即执行了**switch\_to(A, B**),而后当**被抢占的进程 A**再次**被选择执行**的时候,系统可能**进行了多次进程切换/抢占(！！！**)(至少会经历一次即再次从 B 到 A),假设**A 再次被选择执行时**时**当前活动进程是 C**, 即此时**prev = C. next = A**.

在**每个 switch\_to 被调用**的时候,**prev**和**next 指针**位于**各个进程的内核栈**中,prev 指向了**当前运行的进程**,而**next 指向了将要运行的下一个进程**,那么为了执行**从 prev 到 next 的切换**,switcth\_to 使用前**两个参数 prev 和 next 就够**了.

在**进程 A 被选中再次执行**的时候,会出现一个问题,**此时控制权即将回到 A**,switch\_to**函数返回**,**内核开始执行 switch\_to 之后的点(！！！**),此时**内核栈准确的恢复到切换之前的状态**,即**进程 A 上次被切换出去时**的状态, **prev = A, next = B**. 此时,内核**无法知道实际上在进程 A 之前运行的是进程 C(！！！**).

因此, 在**新进程被选中执行时**, 内核恢复到进程**被切换出去的点**继续执行, 此时内核**只知道谁之前将新进程抢占了(上面例子就是知道了 B 抢占的是 A**), 但是却不知道**新进程再次执行是抢占了谁(A 的再次执行是抢占了 C,但是因为是状态恢复到原有的点,所以不会知道 A 这里抢占的是 C**),因此**底层的进程切换机制**必须将**此前执行的进程**(即**新进程抢占的那个进程！！！**)提供给 context\_switch. 由于**控制流会回到函数的该中间**, 因此无法通过普通函数的返回值来完成. 因此使用了一个 3 个参数,但是逻辑效果是相同的,仿佛是 switch\_to 是带有两个参数的函数, 而且返回了一个指向**此前运行的进程的指针**.

>switch\_to(prev, next, last);
>
>即
>
>prev = last = switch_to(prev, next);

其中**返回的 prev 值并不是做参数的 prev 值**,而是 prev 被再次调度的时候**抢占掉的那个进程 last(进程 C！！！**).

在上个例子中, 进程 A 提供给 switch\_to 的参数是 prev = A, next = B, 然后控制权从 A 交给了 B, 但是恢复执行的时候是通过 prev = C, next = A 完成了再次调度, 而后内核恢复了进程 A 被切换之前的内核栈信息, 即 prev = A, next = B. 内核为了通知调度机制 A 抢占了 C 的处理器, 就**通过 last 参数传递回来**, prev = last = C.

内核实现该行为特性的方式依赖于底层的体系结构,但内核显然可以通过考虑两个进程的内核栈来重建所需要的信息

### 3.6.3 switch\_to 函数注释

switch\_mm()进行**用户空间的切换**,更确切地说,是**切换地址转换表(pgd**), 由于**pgd(两个都包含了！！！**)包括**内核虚拟地址空间**和**用户虚拟地址空间地址映射**,linux 内核把**进程的整个虚拟地址空间**分成两个部分,一部分是**内核虚拟地址空间**,另外一部分是**内核虚拟地址空间**,各个进程的虚拟地址空间各不相同,但是却**共用了同样的内核地址空间**,这样在进程切换的时候,就**只需要切换虚拟地址空间的用户空间部分**.

**每个进程**都有其自身的**页目录表 pgd**

**进程本身尚未切换**,而存储管理机制**的页目录指针 cr3 却已经切换**了, 这样**不会造成问题吗?不会的**, 因为这个时候**CPU 在系统空间运行**, 而**所有进程的页目录表中与系统空间对应的目录项都指向相同的页表(！！！**), 所以, 不管切换到哪一个进程的页目录表都一样, **受影响的只是用户空间, 系统空间的映射则永远不变**

我们下面来分析一下子, x86\_32 位下的 switch\_to 函数, 其定义在[arch/x86/include/asm/switch\_to.h, line 27](http://lxr.free-electrons.com/source/arch/x86/include/asm/switch_to.h?v=4.6#L27)

先对**flags 寄存器**和**ebp 压入旧进程内核栈(原进程的内核栈！！！**), 并将**确定旧进程恢复执行的下一跳地址**, 并将**旧进程 ip, esp 保存到 task\_struct->thread\_info**中, 这样**旧进程保存完毕**; 然后用**新进程的 thread\_info->esp 恢复新进程的内核堆栈**, 用**thread->info 的 ip 恢复新进程地址执行**.

关键点: 内核寄存器[eflags、ebp 保存到内核栈; 内核栈 esp 地址、ip 地址保存到 thread\_info 中, task\_struct 在生命期中**始终是全局的**, 所以肯定能根据该结构恢复出其所有执行场景来]

```c
/*
 * Saving eflags is important. It switches not only IOPL between tasks,
 * it also protects other tasks from NT leaking through sysenter etc.
 */
#define switch_to(prev, next, last)                                     \
do {                                                                    \
        /*                                                              \
         * Context-switching clobbers all registers, so we clobber      \
         * them explicitly, via unused output variables.                \
         * (EAX and EBP is not listed because EBP is saved/restored     \
         * explicitly for wchan access and EAX is the return value of   \
         * __switch_to())                                               \
         */                                                             \
        unsigned long ebx, ecx, edx, esi, edi;                          \
                                                                        \
        asm volatile("pushfl\n\t" /* save flags 保存就的 ebp、和 flags 寄存器到旧进程的内核栈中*/   \
                     "pushl %%ebp\n\t"          /* save    EBP   */     \
                     "movl %%esp,%[prev_sp]\n\t"        /* save ESP  将旧进程 esp 保存到 thread_info 结构中 */ \
                     "movl %[next_sp],%%esp\n\t"        /* restore ESP 用新进程 esp 填写 esp 寄存器, 此时内核栈已切换  */ \
                     "movl $1f,%[prev_ip]\n\t"  /* save EIP 将该进程恢复执行时的下条地址保存到旧进程的 thread 中*/     \
                     "pushl %[next_ip]\n\t"     /* restore EIP 将新进程的 ip 值压入到新进程的内核栈中 */     \
                     __switch_canary                                    \
                     "jmp __switch_to\n"        /* regparm call  */     \
                     "1:\t"                                             \
                     "popl %%ebp\n\t"           /* restore EBP 该进程执行, 恢复 ebp 寄存器*/     \
                     "popfl\n"                  /* restore flags  恢复 flags 寄存器*/     \
                                                                        \
                     /* output parameters */                            \
                     : [prev_sp] "=m" (prev->thread.sp),                \
                       [prev_ip] "=m" (prev->thread.ip),                \
                       "=a" (last),                                     \
                                                                        \
                       /* clobbered output registers: */                \
                       "=b" (ebx), "=c" (ecx), "=d" (edx),              \
                       "=S" (esi), "=D" (edi)                           \
                                                                        \
                       __switch_canary_oparam                           \
                                                                        \
                       /* input parameters: */                          \
                     : [next_sp]  "m" (next->thread.sp),                \
                       [next_ip]  "m" (next->thread.ip),                \
                                                                        \
                       /* regparm parameters for __switch_to(): */      \
                       [prev]     "a" (prev),                           \
                       [next]     "d" (next)                            \
                                                                        \
                       __switch_canary_iparam                           \
                                                                        \
                     : /* reloaded segment registers */                 \
                        "memory");                                      \
} while (0)
```

## 3.7 barrier 路障同步

switch\_to 完成了进程的切换,**新进程在该调用后开始执行**,而 switch\_to 之后的代码只有在**当前进程下一次被选择运行时才会执行(重新调度到以后才会执行！！！**).

```c
/*  switch_to 之后的代码只有在
 *  当前进程再次被选择运行(恢复执行)时才会运行
 *  而此时当前进程恢复执行时的上一个进程可能跟参数传入时的 prev 不同
 *  甚至可能是系统中任意一个随机的进程
 *  因此 switch_to 通过第三个参数将此进程返回
*/

/*  路障同步, 一般用编译器指令实现
 *  确保了 switch_to 和 finish_task_switch 的执行顺序
 *  不会因为任何可能的优化而改变  */
barrier();

/*  进程切换之后的处理工作  */
return finish_task_switch(prev);
```

而为了**程序编译后指令的执行顺序不会因为编译器的优化而改变**,因此内核提供了路障同步 barrier 来保证程序的**执行顺序**.

barrier 往往通过编译器指令来实现, 内核中多处都实现了 barrier, 形式如下

```c
// http://lxr.free-electrons.com/source/include/linux/compiler-gcc.h?v=4.6#L15
/* Copied from linux/compiler-gcc.h since we can't include it directly
 * 采用内敛汇编实现
 *  __asm__用于指示编译器在此插入汇编语句
 *  __volatile__用于告诉编译器, 严禁将此处的汇编语句与其它的语句重组合优化.
 *  即: 原原本本按原来的样子处理这这里的汇编.
 *  memory 强制 gcc 编译器假设 RAM 所有内存单元均被汇编指令修改, 这样 cpu 中的 registers 和 cache 中已缓存的内存单元中的数据将作废. cpu 将不得不在需要的时候重新读取内存中的数据. 这就阻止了 cpu 又将 registers, cache 中的数据用于去优化指令, 而避免去访问内存.
 *  "":::表示这是个空指令. barrier()不用在此插入一条串行化汇编指令. 在后文将讨论什么叫串行化指令.
*/
#define barrier() __asm__ __volatile__("": : :"memory")
```

关于内存屏障的详细信息, 可以参见 [Linux 内核同步机制之(三): memory barrier](http://www.wowotech.net/kernel_synchronization/memory-barrier.html)

## 3.8 finish\_task\_switch 完成清理工作

finish\_task\_switch 完成一些**清理工作**,使得能够**正确的释放锁**,但我们不会详细讨论这些.他会向各个体系结构提供了另一个挂钩上下切换过程的可能性,当然这只在少数计算机上需要.

前面我们谅解 switch\_to 函数的 3 个参数时, 讲到

注: A 进程切换到 B, A 被切换, 而当 A 再次被选择执行, C 再次切换到 A, 此时 A 执行, 但是系统为了告知调度器 A 再次执行前的进程是 C, 通过 switch\_to 的**last 参数返回**的 prev 指向 C,在**A 调度时候**需要把**调用 A 的进程的信息清除掉**

由于从 C 切换到 A 时候, A 内核栈中保存的实际上是 A 切换出时的状态信息, 即 prev=A, next=B, 但是在 A 执行时, 其位于 context\_switch 上下文中, 该函数的 last 参数返回的 prev 应该是切换到 A 的进程 C,**A 负责对 C 进程信息进行切换后处理(！！！**), 比如, 如果切换到 A 后, **A 发现 C 进程已经处于 TASK\_DEAD 状态, 则将释放 C 进程的 TASK\_STRUCT 结构(！！！**)

函数定义在[kernel/sched/core.c, line 2715](http://lxr.free-electrons.com/source/kernel/sched/core.c#?v=4.6L2715)中, 如下所示

```c
/**
 * finish_task_switch - clean up after a task-switch
 * @prev: the thread we just switched away from.
 *
 * finish_task_switch must be called after the context switch, paired
 * with a prepare_task_switch call before the context switch.
 * finish_task_switch will reconcile locking set up by prepare_task_switch,
 * and do any other architecture-specific cleanup actions.
 *
 * Note that we may have delayed dropping an mm in context_switch(). If
 * so, we finish that here outside of the runqueue lock. (Doing it
 * with the lock held can cause deadlocks; see schedule() for
 * details.)
 *
 * The context switch have flipped the stack from under us and restored the
 * local variables which were saved when this task called schedule() in the
 * past. prev == current is still correct but we need to recalculate this_rq
 * because prev may have moved to another CPU.
 */
static struct rq *finish_task_switch(struct task_struct *prev)
        __releases(rq->lock)
{
        struct rq *rq = this_rq();
        struct mm_struct *mm = rq->prev_mm;
        long prev_state;

        /*
         * The previous task will have left us with a preempt_count of 2
         * because it left us after:
         *
         *      schedule()
         *        preempt_disable();                    // 1
         *        __schedule()
         *          raw_spin_lock_irq(&rq->lock)        // 2
         *
         * Also, see FORK_PREEMPT_COUNT.
         */
        if (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET,
                      "corrupted preempt_count: %s/%d/0x%x\n",
                      current->comm, current->pid, preempt_count()))
                preempt_count_set(FORK_PREEMPT_COUNT);

        rq->prev_mm = NULL;

        /*
         * A task struct has one reference for the use as "current".
         * If a task dies, then it sets TASK_DEAD in tsk->state and calls
         * schedule one last time. The schedule call will never return, and
         * the scheduled task must drop that reference.
         *
         * We must observe prev->state before clearing prev->on_cpu (in
         * finish_lock_switch), otherwise a concurrent wakeup can get prev
         * running on another CPU and we could rave with its RUNNING -> DEAD
         * transition, resulting in a double drop.
         */
        prev_state = prev->state;
        vtime_task_switch(prev);
        perf_event_task_sched_in(prev, current);
        finish_lock_switch(rq, prev);
        finish_arch_post_lock_switch();

        fire_sched_in_preempt_notifiers(current);
        if (mm)
                mmdrop(mm);
        if (unlikely(prev_state == TASK_DEAD))  /*  如果上一个进程已经终止, 释放其 task_struct 结构  */
        {
                if (prev->sched_class->task_dead)
                        prev->sched_class->task_dead(prev);

                /*
                 * Remove function-return probe instances associated with this
                 * task and put them back on the free list.
                 */
                kprobe_flush_task(prev);
                put_task_struct(prev);
        }

        tick_nohz_task_switch();
        return rq;
}
```

# 参考

https://blog.csdn.net/zsj1126/article/details/103092465 (未整理)