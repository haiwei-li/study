
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1 前言](#1-前言)
- [2 WQ\_POWER\_EFFICIENT 的处理](#2-wq_power_efficient-的处理)
- [3 分配 workqueue 的内存](#3-分配-workqueue-的内存)
  - [3.1 workqueue 和 pool workqueue 的关系](#31-workqueue-和-pool-workqueue-的关系)
  - [3.2 workqueue attribute](#32-workqueue-attribute)
  - [3.3 unbound workqueue 和 NUMA 之间的联系](#33-unbound-workqueue-和-numa-之间的联系)
- [4 初始化 workqueue 的成员](#4-初始化-workqueue-的成员)
- [5 分配 pool workqueue 的内存并建立 workqueue 和 pool workqueue 的关系](#5-分配-pool-workqueue-的内存并建立-workqueue-和-pool-workqueue-的关系)
- [6 应用新的 attribute 到 workqueue 中](#6-应用新的-attribute-到-workqueue-中)
  - [6.1 健康检查](#61-健康检查)
  - [6.2 分配内存并初始化](#62-分配内存并初始化)
  - [6.3 如何为 unbound workqueue 的 pool workqueue 寻找对应的线程池?](#63-如何为-unbound-workqueue-的-pool-workqueue-寻找对应的线程池)
  - [6.4 给各个 node 分配 pool workqueue 并初始化](#64-给各个-node-分配-pool-workqueue-并初始化)
  - [6.5 安装](#65-安装)

<!-- /code_chunk_output -->

# 1 前言

本文主要以\_\_alloc\_workqueue\_key 函数为主线, 描述 CMWQ 中的创建一个 workqueue 实例的代码过程.

# 2 WQ\_POWER\_EFFICIENT 的处理

\_\_alloc\_workqueue\_key 函数的一开始有如下的代码:

```c
if ((flags & WQ_POWER_EFFICIENT) && wq_power_efficient)
        flags |= WQ_UNBOUND;
```

在 kernel 中, 有两种线程池, 一种是线程池是 per cpu 的, 也就是说, 系统中有多少个 cpu, 就会创建多少个线程池, cpu x 上的线程池创建的 worker 线程也只会运行在 cpu x 上. 另外一种是 unbound thread pool, 该线程池创建的 worker 线程可以调度到任意的 cpu 上去. 由于 cache locality 的原因, per cpu 的线程池的性能会好一些, 但是对 power saving 有一些影响. 设计往往如此, workqueue 需要在 performance 和 power saving 之间平衡, 想要更好的性能, 那么最好让一个 cpu 上的 worker thread 来处理 work, 这样的话, cache 命中率会比较高, 性能会更好. 但是, 从电源管理的角度来看, 最好的策略是让 idle 状态的 cpu 尽可能的保持 idle, 而不是反复 idle, working, idle again.

我们来一个例子辅助理解上面的内容. 在 t1 时刻, work 被调度到 CPU A 上执行, t2 时刻 work 执行完毕, CPU A 进入 idle, t3 时刻有一个新的 work 需要处理, 这时候调度 work 到那个 CPU 会好些呢?是处于 working 状态的 CPU B 还是处于 idle 状态的 CPU A 呢?如果调度到 CPU A 上运行, 那么, 由于之前处理过 work, 其 cache 内容新鲜热辣, 处理起 work 当然是得心应手, 速度很快, 但是, 这需要将 CPU A 从 idle 状态中唤醒. 选择 CPU B 呢就不存在将 CPU 从 idle 状态唤醒, 从而获取 power saving 方面的好处.

了解了上面的基础内容之后, 我们再来检视 per cpu thread pool 和 unbound thread pool. 当 workqueue 收到一个要处理的 work, 如果该 workqueue 是 unbound 类型的话, 那么该 work 由 unbound thread pool 处理并把调度该 work 去哪一个 CPU 执行这样的策略交给系统的调度器模块来完成, 对于 scheduler 而言, 它会考虑 CPU core 的 idle 状态, 从而尽可能的让 CPU 保持在 idle 状态, 从而节省了功耗. 因此, 如果一个 workqueue 有 WQ\_UNBOUND 这样的 flag, 则说明该 workqueue 上挂入的 work 处理是考虑到 power saving 的. 如果 workqueue 没有 WQ\_UNBOUND flag, 则说明该 workqueue 是 per cpu 的, 这时候, 调度哪一个 CPU core 运行 worker thread 来处理 work 已经不是 scheduler 可以控制的了, 这样, 也就间接影响了功耗.

有两个参数可以控制 workqueue 在 performance 和 power saving 之间的平衡:

1、各个 workqueue 需要通过 WQ\_POWER\_EFFICIENT 来标记自己在功耗方面的属性

2、系统级别的内核参数 workqueue.power\_efficient.

使用 workqueue 的用户知道自己在电源管理方面的特点, 如果该 workqueue 在 unbound 的时候会极大的降低功耗, 那么就需要加上 WQ\_POWER\_EFFICIENT 的标记. 这时候, 如果没有标记 WQ\_UNBOUND, 那么缺省 workqueue 会创建 per cpu thread pool 来处理 work. 不过, 也可以通过 workqueue.power\_efficient 这个内核参数来修改 workqueue 的行为:

```c
#ifdef CONFIG_WQ_POWER_EFFICIENT_DEFAULT
static bool wq_power_efficient = true;
#else
static bool wq_power_efficient;
#endif

module_param_named(power_efficient, wq_power_efficient, bool, 0444);
```

如果 wq\_power\_efficient 设定为 true, 那么 WQ\_POWER\_EFFICIENT 的标记的 workqueue 就会强制按照 unbound workqueue 来处理, 即使没有标记 WQ\_UNBOUND.

# 3 分配 workqueue 的内存

```c
if (flags & WQ_UNBOUND)
    tbl_size = nr_node_ids * sizeof(wq->numa_pwq_tbl[0]); －－－only for unbound workqueue

wq = kzalloc(sizeof(*wq) + tbl_size, GFP_KERNEL);

if (flags & WQ_UNBOUND) {
        wq->unbound_attrs = alloc_workqueue_attrs(GFP_KERNEL); －－only for unbound workqueue
    }
```

代码很简单, 与其要解释代码, 不如来解释一些基本概念.

## 3.1 workqueue 和 pool workqueue 的关系

我们先给出一个简化版本的 workqueue\_struct 定义, 如下:

```c
struct workqueue_struct {
    struct list_head    pwqs;
    struct list_head    list;


    struct pool_workqueue __percpu *cpu_pwqs;  －－－－－指向 per cpu 的 pool workqueue
    struct pool_workqueue __rcu *numa_pwq_tbl[]; －－－－指向 per node 的 pool workqueue
};
```

这里涉及 2 个数据结构: workqueue\_struct 和 pool\_workqueue, 为何如此处理呢?我们知道, 在 CMWQ 中, workqueue 和 thread pool 没有严格的一一对应关系了, 因此, 系统中的 workqueue 们共享一组 thread pool, 因此, workqueue 中的成员包括两个类别: global 类型和 per thread pool 类型的, 我们把那些 per thread pool 类型的数据集合起来就形成了 pool\_workqueue 的定义.

挂入 workqueue 的 work 终究需要 worker pool 中的某个 worker thread 来处理, 也就是说, workqueue 要和系统中那些共享的 worker thread pool 进行连接, 这是通过 pool\_workqueue(该数据结构会包含一个指向 worker pool 的指针)的数据结构来管理的. 和这个 workqueue 相关的 pool\_workqueue 被挂入一个链表, 链表头就是 workqueue\_struct 中的 pwqs 成员.

和旧的 workqueue 机制一样, 系统维护了一个所有 workqueue 的 list, list head 定义如下:

```c
static LIST_HEAD(workqueues);
```

workqueue\_struct 中的 list 成员就是挂入这个链表的节点.

workqueue 有两种: unbound workqueue 和 per cpu workqueue. 对于 per cpu 类型, cpu\_pwqs 指向了一组 per cpu 的 pool\_workqueue 数据结构, 用来维护 workqueue 和 per cpu thread pool 之间的关系. 每个 cpu 都有两个 thread pool, normal 和高优先级的线程池, 到底 cpu\_pwqs 指向哪一个 pool\_workqueue(worker thread)是和 workqueue 的 flag 相关, 如果标有 WQ\_HIGHPRI, 那么 cpu\_pwqs 指向高优先级的线程池. unbound workqueue 对应的 pool\_workqueue 和 workqueue 属性相关, 我们在下一节描述.

## 3.2 workqueue attribute

挂入 workqueue 的 work 终究是需要 worker 线程来处理, 针对 worker 线程有下面几个考量点(我们称之 attribute):

(1)该 worker 线程的优先级

(2)该 worker 线程运行在哪一个 CPU 上

(3)如果 worker 线程可以运行在多个 CPU 上, 且这些 CPU 属于不同的 NUMA node, 那么是否在所有的 NUMA node 中都可以获取良好的性能.

对于 per\-CPU 的 workqueue, 2 和 3 不存在问题, 哪个 cpu 上 queue 的 work 就在哪个 cpu 上执行, 由于只能在一个确定的 cpu 上执行, 因此起 NUMA 的 node 也是确定的(一个 CPU 不可能属于两个 NUMA node). 置于优先级, per-CPU 的 workqueue 使用 WQ\_HIGHPRI 来标记. 综上所述, per\-CPU 的 workqueue 不需要单独定义一个 workqueue attribute, 这也是为何在 workqueue\_struct 中只有 unbound\_attrs 这个成员来记录 unbound workqueue 的属性.

unbound workqueue 由于不绑定在具体的 cpu 上, 可以运行在系统中的任何一个 cpu, 直觉上似乎系统中有一个 unbound thread pool 就 OK 了, 不过让一个 thread pool 创建多种属性的 worker 线程是一个好的设计吗?本质上, thread pool 应该创建属性一样的 worker thread. 因此, 我们通过 workqueue 属性来对 unbound workqueue 进行分类, workqueue 属性定义如下:

```c
struct workqueue_attrs {
    int            nice;        /* nice level */
    cpumask_var_t        cpumask;    /* allowed CPUs */
    bool            no_numa;    /* disable NUMA affinity */
};
```

nice 是一个和 thread 优先级相关的属性, nice 越低则优先级越高. cpumask 是该 workqueue 挂入的 work 允许在哪些 cpu 上运行. no\_numa 是一个和 NUMA affinity 相关的设定.

## 3.3 unbound workqueue 和 NUMA 之间的联系

UMA 系统中, 所有的 processor 看到的内存都是一样的, 访问速度也是一样, 无所谓 local or remote, 因此, 内核线程如果要分配内存, 那么也是无所谓, 统一安排即可. 在 NUMA 系统中, 不同的一个或者一组 cpu 看到的 memory 是不一样的, 我们假设 node 0 中有 CPU A 和 B, node 1 中有 CPU C 和 D, 如果运行在 CPU A 上内核线程现在要迁移到 CPU C 上的时候, 悲剧发生了: 该线程在 A CPU 创建并运行的时候, 分配的内存是 node 0 中的 memory, 这些 memory 是 local 的访问速度很快, 当迁移到 CPU C 上的时候, 原来 local memory 变成 remote, 性能大大降低. 因此, unbound workqueue 需要引入 NUMA 的考量点.

NUMA 是内存管理的范畴, 本文不会深入描述, 我们暂且放开 NUMA, 先思考这样的一个问题: 一个确定属性的 unbound workqueue 需要几个线程池?看起来一个就够了, 毕竟 workqueue 的属性已经确定了, 一个线程池创建相同属性的 worker thread 就行了. 但是我们来看一个例子: 假设 workqueue 的 work 是可以在 node 0 中的 CPU A 和 B, 以及 node 1 中 CPU C 和 D 上处理, 如果只有一个 thread pool, 那么就会存在 worker thread 在不同 node 之间的迁移问题. 为了解决这个问题, 实际上 unbound workqueue 实际上是创建了 per node 的 pool_workqueue(thread pool)

当然, 是否使用 per node 的 pool workqueue 用户是可以通过下面的参数进行设定的:

(1)workqueue attribute 中的 no_numa 成员

(2)通过 workqueue.disable\_numa 这个参数, disable 所有 workqueue 的 numa affinity 的支持.

```c
static bool wq_disable_numa;
module_param_named(disable_numa, wq_disable_numa, bool, 0444);
```

# 4 初始化 workqueue 的成员

```c
va_start(args, lock_name);
vsnprintf(wq->name, sizeof(wq->name), fmt, args);－－－－－set workqueue name
va_end(args);

max_active = max_active ?: WQ_DFL_ACTIVE;
max_active = wq_clamp_max_active(max_active, flags, wq->name);
wq->flags = flags;
wq->saved_max_active = max_active;
mutex_init(&wq->mutex);
atomic_set(&wq->nr_pwqs_to_flush, 0);
INIT_LIST_HEAD(&wq->pwqs);
INIT_LIST_HEAD(&wq->flusher_queue);
INIT_LIST_HEAD(&wq->flusher_overflow);
INIT_LIST_HEAD(&wq->maydays);

lockdep_init_map(&wq->lockdep_map, lock_name, key, 0);
INIT_LIST_HEAD(&wq->list);
```

除了 max active, 没有什么要说的, 代码都简单而且直观. 如果用户没有设定 max active(或者说 max active 等于 0), 那么系统会给出一个缺省的设定. 系统定义了两个最大值 WQ\_MAX\_ACTIVE(512)和 WQ\_UNBOUND_MAX\_ACTIVE(和 cpu 数目有关, 最大值是 cpu 数目乘以 4, 当然也不能大于 WQ\_MAX\_ACTIVE), 分别限定 per cpu workqueue 和 unbound workqueue 的最大可以创建的 worker thread 的数目. wq\_clamp\_max\_active 可以将 max active 限制在一个确定的范围内.

# 5 分配 pool workqueue 的内存并建立 workqueue 和 pool workqueue 的关系

这部分的代码主要涉及 alloc\_and\_link\_pwqs 函数, 如下:

```c
static int alloc_and_link_pwqs(struct workqueue_struct *wq)
{
    bool highpri = wq->flags & WQ_HIGHPRI;－－－－normal or high priority?
    int cpu, ret;

    if (!(wq->flags & WQ_UNBOUND)) {－－－－－per cpu workqueue 的处理
        wq->cpu_pwqs = alloc_percpu(struct pool_workqueue);

        for_each_possible_cpu(cpu) {－－－－－逐个 cpu 进行设定
            struct pool_workqueue *pwq =    per_cpu_ptr(wq->cpu_pwqs, cpu);
            struct worker_pool *cpu_pools = per_cpu(cpu_worker_pools, cpu);

            init_pwq(pwq, wq, &cpu_pools[highpri]);
            link_pwq(pwq);－－－－上面两行代码用来建立 workqueue、pool wq 和 thread pool 之间的关系
        }
        return 0;
    } else if (wq->flags & __WQ_ORDERED) {－－－－－ordered unbound workqueue 的处理
        ret = apply_workqueue_attrs(wq, ordered_wq_attrs[highpri]);
        return ret;
    } else {－－－－－unbound workqueue 的处理
        return apply_workqueue_attrs(wq, unbound_std_wq_attrs[highpri]);
    }
}
```

通过 alloc\_percpu 可以为每一个 cpu 分配一个 pool\_workqueue 的 memory. 每个 pool\_workqueue 都有一个对应的 worker thread pool, 对于 per\-CPU workqueue, 它是静态定义的, 如下:

```c
static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS],
                     cpu_worker_pools);
```

init\_pwq 函数初始化 pool\_workqueue, 最重要的是设定其对应的 workqueue 和 worker pool. link\_pwq 主要是将 pool\_workqueue 挂入它所属的 workqueue 的链表中. 对于 unbound workqueue, apply\_workqueue\_attrs 完成分配 pool workqueue 并建立 workqueue 和 pool workqueue 的关系.

# 6 应用新的 attribute 到 workqueue 中

unbound workqueue 有两种, 一种是 normal type, 另外一种是 ordered type, 这种 workqueue 上的 work 是严格按照顺序执行的, 不存在并发问题. ordered unbound workqueue 的行为类似过去的 single thread workqueue. 但是, 无论那种类型的 unbound workqueue 都使用 apply\_workqueue\_attrs 来建立 workqueue、pool wq 和 thread pool 之间的关系.

## 6.1 健康检查

```c
if (WARN_ON(!(wq->flags & WQ_UNBOUND)))
    return -EINVAL;

if (WARN_ON((wq->flags & __WQ_ORDERED) && !list_empty(&wq->pwqs)))
    return -EINVAL;
```

只有 unbound 类型的 workqueue 才有 attribute, 才可以 apply attributes. 对于 ordered 类型的 unbound workqueue, 属于它的 pool workqueue(worker thread pool)只能有一个, 否则无法限制 work 是按照顺序执行.

## 6.2 分配内存并初始化

```c
pwq_tbl = kzalloc(nr_node_ids * sizeof(pwq_tbl[0]), GFP_KERNEL);
new_attrs = alloc_workqueue_attrs(GFP_KERNEL);
tmp_attrs = alloc_workqueue_attrs(GFP_KERNEL);
copy_workqueue_attrs(new_attrs, attrs);
cpumask_and(new_attrs->cpumask, new_attrs->cpumask, cpu_possible_mask);
copy_workqueue_attrs(tmp_attrs, new_attrs);
```

pwq\_tbl 数组用来保存 unbound workqueue 各个 node 的 pool workqueue 的指针, new\_attrs 和 tmp\_attrs 都是一些计算 workqueue attribute 的中间变量, 开始的时候设定为用户传入的 workqueue 的 attribute.

## 6.3 如何为 unbound workqueue 的 pool workqueue 寻找对应的线程池?

具体的代码在 get\_unbound\_pool 函数中. 本节不描述具体的代码, 只说明基本原理, 大家可以自行阅读代码.

per cpu 的 workqueue 的 pool workqueue 对应的线程池也是 per cpu 的, 每个 cpu 有两个线程池(normal 和 high priority), 因此将 pool workqueue 和 thread pool 对应起来是非常简单的事情. 对于 unbound workqueue, 对应关系没有那么直接, 如果属性相同, 多个 unbound workqueue 的 pool workqueue 可能对应一个 thread pool.

系统使用哈希表来保存所有的 unbound worker thread pool, 定义如下:

```c
static DEFINE_HASHTABLE(unbound_pool_hash, UNBOUND_POOL_HASH_ORDER);
```

在创建 unbound workqueue 的时候, pool workqueue 对应的 worker thread pool 需要在这个哈希表中搜索, 如果有相同属性的 worker thread pool 的话, 那么就不需要创建新的线程池, 代码如下:

```c
hash_for_each_possible(unbound_pool_hash, pool, hash_node, hash) {
    if (wqattrs_equal(pool->attrs, attrs)) { －－－－检查属性是否相同
        pool->refcnt++;
        return pool; －－－－－－－在哈希表找到适合的 unbound 线程池
    }
}
```

如果没有相同属性的 thread pool, 那么需要创建一个并挂入哈希表.

## 6.4 给各个 node 分配 pool workqueue 并初始化

在进入代码之前, 先了解一些基础知识. 缺省情况下, 挂入 unbound workqueue 的 works 最好是考虑 NUMA Affinity, 这样可以获取更好的性能. 当然, 实际上用户可以通过 workqueue.disable\_numa 这个内核参数来关闭这个特性, 这时候, 系统需要一个 default pool workqueue(workqueue\_struct 的 dfl\_pwq 成员), 所有的 per node 的 pool workqueue 指针都是执行 default pool workqueue.

workqueue.disable_numa 是 enable 的情况下是否不需要 default pool workqueue 了呢?也不是, 我们举一个简单的例子, 一个系统的构成是这样的: node 0 中有 CPU A 和 B, node 1 中有 CPU C 和 D, node 2 中有 CPU E 和 F, 假设 workqueue 的 attribute 规定 work 只能在 CPU A 和 C 上运行, 那么在 node 0 和 node 1 中创建自己的 pool workqueue 是 ok 的, 毕竟 node 0 中有 CPU A, node 1 中有 CPU C, 该 node 创建的 worker thread 可以在 A 或者 C 上运行. 但是对于 node 2 节点, 没有任何的 CPU 允许处理该 workqueue 的 work, 在这种情况下, 没有必要为 node 2 建立自己的 pool workqueue, 而是使用 default pool workqueue.

OK, 我们来看代码:

```c
dfl_pwq = alloc_unbound_pwq(wq, new_attrs); －－－－－分配 default pool workqueue

for_each_node(node) { －－－－遍历 node
    if (wq_calc_node_cpumask(attrs, node, -1, tmp_attrs->cpumask)) { －－－是否使用 default pool wq
        pwq_tbl[node] = alloc_unbound_pwq(wq, tmp_attrs); －－－该 node 使用自己的 pool wq
    } else {
        dfl_pwq->refcnt++;
        pwq_tbl[node] = dfl_pwq; －－－－该 node 使用 default pool wq
    }
}
```

值得一提的是 wq\_calc\_node\_cpumask 这个函数, 这个函数会根据该 node 的 cpu 情况以及 workqueue attribute 中的 cpumask 成员来更新 tmp\_attrs\->cpumask, 因此, 在 pwq\_tbl\[node\] = alloc\_unbound\_pwq(wq, tmp\_attrs); 这行代码中, 为该 node 分配 pool workqueue 对应的线程池的时候, 去掉了本 node 中不存在的 cpu. 例如 node 0 中有 CPU A 和 B, workqueue 的 attribute 规定 work 只能在 CPU A 和 C 上运行, 那么创建 node 0 上的 pool workqueue 以及对应的 worker thread pool 的时候, 需要删除 CPU C, 也就是说, node 0 上的线程池的属性中的 cpumask 仅仅支持 CPU A 了.

## 6.5 安装

所有的 node 的 pool workqueue 及其 worker thread pool 已经 ready, 需要安装到 workqueue 中了:

```c
for_each_node(node)
        pwq_tbl[node] = numa_pwq_tbl_install(wq, node, pwq_tbl[node]);
    link_pwq(dfl_pwq);
    swap(wq->dfl_pwq, dfl_pwq);
```

代码非常简单, 这里就不细述了.