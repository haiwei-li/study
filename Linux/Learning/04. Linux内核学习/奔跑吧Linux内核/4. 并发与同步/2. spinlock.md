
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [概述](#概述)
- [spinlock 实现](#spinlock-实现)
- [spinlock 变种](#spinlock-变种)
- [3 spinlock 和 raw_spin_lock](#3-spinlock-和-raw_spin_lock)
- [4 总结](#4-总结)

<!-- /code_chunk_output -->


本章思考题

- 为什么 spinlock 的临界区不能睡眠(不考虑 RT-Linux 的情况)?
- Linux 内核中经典 spinlock 的实现有什么缺点?
- 为什么 spinlock 临界区不允许发生抢占?
- Ticket-based 的 spinlock 机制是如何实现的?
- 如果在 spin_lock()和 spin_unlock()的临界区中发全了中断, 并且中断处理程序也恰巧修改了该临界资源, 那么会发生什么后果?该如何避免呢?

# 概述

如果**临界区**只是**一个变量**, 那么**原子变量**可以解决问题, 但是**临界区大多**是一个**数据操作的集合**, 例如先从一个数据结构中移出数据, 对其进行数据解析, 然后再写回到该数据结构或者其他数据结构中, 类 似 "read->modify->write"操作; 再比如临界区是一个链表操作等. 整个执行过程需要保证原子性, 在数据被更新完毕前, 不能有其他内核代码路径访问和改写这些数据. 这个过程使用原子变量显得不合适, 需要**锁机制**来完成, **自旋锁(spinlock**) 是 Linux 内核中**最常见的锁机制**.

spinlock 同一时刻只能被一个内核代码路径持有, 如果有另外一个内核代码路径试图获取一个己经被持有的 spinlock, 那么该**内核代码路径**需要一直**自旋忙等待**, 直到锁持有者释放了该锁. 如果该锁没有被别人持有(或争用, lock contention), 那么可以立即获得该锁.

spinlock 锁的特性如下.

- **忙等待的锁机制**. 操作系统中**锁的机制分为两类(！！！**), 一类是**忙等待**, 另一类是**睡眠等待**. **spinlock**属于前者, 当无法获取 spinlock 锁时会**不断尝试**, 直到获取锁为止.
-  **同一时刻**只能有**一个内核代码路径**可以获得该锁.
- 要求 spinlock 锁**持有者尽快完成临界区的执行任务**. 如果临界区执行时间过长, 在锁外面忙等待的 CPU 比较浪费, 特别是 spinlock**临界区里不能睡眠**.
- spinlock 锁可以在****中断上下文中使用****.

# spinlock 实现

先看 spinlock 数据结构的定义.

```c
[include/linux/spinlock_types.h]
typedef struct spinlock {
	union {
		struct raw_spinlock rlock; //重点
	};
} spinlock_t;

typedef struct raw_spinlock {
	arch_spinlock_t raw_lock; // 重点
} raw_spinlock_t;

[arch/arm/include/asm/spinlock_types.h]
// ARM
typedef struct {
	union {
		u32 slock;
		struct __raw_tickets {
			u16 next;
			u16 owner;
		} tickets;
	};
} arch_spinlock_t;

[arch/x86/include/asm/spinlock_types.h]
// x86
#ifdef CONFIG_QUEUED_SPINLOCKS
#include <asm-generic/qspinlock_types.h>
#else
typedef struct arch_spinlock {
	union {
		__ticketpair_t head_tail;
		struct __raw_tickets {
			__ticket_t head, tail;
		} tickets;
	};
} arch_spinlock_t;
```

spinlock 数据结构定义考虑到了**不同处理器体系结构**的支持和**实时性内核**(RT patches)的要求, 定义了 **raw_spinlock**和**arch_spinlock_t**数据结构, 其中 arch_spinlock_t 数据结构和体系结构有关, 下面给出 ARM32 架构上的实现. 在 Linux 2.6.25 之前, **spinlock 数据结构**就是一个简单的**无符号类型变量**, slock 值为 1 表示锁未被持有, 值为 0 或者负数表示锁被持有. **之前的 spinlock 机制实现比较简洁**, 特别是在**没有锁争用**的情况下, 但是也存在很多问题, 特别是在**很多 CPU 争用同一个 spinlock**时, 会导致**严重的不公平性及性能下降**. 当该锁释放时, 事实上有可能**刚刚释放该锁的 CPU(！！！CPU！！！**)马上又获得了该锁的**使用权**, 或者说在**同一个 NUMA 节点**上的**CPU**都有可能**抢先获取了该锁**, 而没有考虑那些己经在锁外面**等待了很久的 CPU**. 因为刚**刚释放锁的 CPU 的 L1 cache 中存储了该锁(！！！**), 它**比别的 CPU 更快获得锁**, 这对于那些已经等待很久的 CPU 是不公平的. 在**NUMA 处理器**中, **锁争用**的情况会严重影响系统的性能. 有测试表明, 在一个 2 socket 的 8 核处理器中, spinlock 争用情况愈发明显, 有些线程甚至需要尝试 1000000 次才能获取锁. 为此在 LinuX 2.6.25 内核后, **spinlock**实现了一套名为"**FIFO ticket-based**"算法的 spinlock 机制, 本文简称为**排队自旋锁**.

![config](./images/5.png)

ticket-based 的 spinlock 仍然使用原来的数据结构, 但**slock 被拆分成两个部分(联合体实现！！！**), 如图 4.1 所示, **owner**表示**锁持有者**的**等号牌**, **next**表示外面排队**队列**中**末尾者**的**等号牌**. 这类似于排队吃饭的场景, 在用餐高峰时段, 各大饭店人满为患, 顾客来晚了都需要排队. 为了模型简化, 假设某个饭店只有一张饭桌, 刚开市时, next 和 owner 都是 0.

第一个客户 A 来时, 因为**next 和 owner 都是 0**,说明**锁没有人持有**. 此时因为饭馆还没有顾客, 所以客户**A 的等号牌是 0**, 直接进餐, 这时**next\+\+(！！！**).

第二个客户 B 来时, 因为**next 为 1**, **owner 为 0**, 说明**锁被人持有**. 这时服务员给他**1 号的等号牌**, 让他在饭店门口等待, **next++(！！！**).

第三个客户 C 来了, 因为**next 为 2**, **owner 为 0** , 服务员给他**2 号的等号牌**, 让他在饭店门口排队等待, **next++(！！！**).

这时第一个客户**A 吃完买单**了, **owner\+\+(！！！**), **owner 的值变为 1**. 服务员会让**等号牌**和**owner 值相等**的客户就餐, 客户 B 的等号牌是 1 , 所以现在客户 B 就餐. 有新客户来时 next\+\+, 服务员分配等号牌; 客户埋单时 owner\+\+, 服务员叫号, **owner 值**和**等号牌相等**的客户就餐.

```c
[include/linux/spinlock.h]
static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
}

static inline void __raw_spin_lock(raw_spinlock_t *lock)
{
	preempt_disable();   // 关内核抢占
	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
	LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);
}
```

**spin_lock**()函数最终调用__raw_spin_lock()函数来实现. 首先**关闭内核抢占**, 这是**spinlock 锁的实现关键点之一**. 那么为什么**spinlock 临界区不允许发生抢占**呢?

如果**spinlock 临界区**中**允许抢占**, 那么如果**临界区内发生中断**, **中断返回**时会去**检查抢占调度**, 这里有两个问题, 一是**抢占调度相当于持有锁的进程睡眠**, 违背了**spinlock 锁不能睡眠**和**快速执行**完成的设计语义; 二是**抢占调度进程**也有可能会去**申请 spinlock 锁**, 那么会**导致发生死锁**. 关于**中断返回时检查抢占调度**的相关内容可以参考第 5.1.4 节.

如果系统没有打开**CONFIG_LOCKDEP**和**CONFIG_LOCK_STAT**选项, **spin_acquire**()函数其实是一个空函数, 并且**LOCK_CONTENDED**()只是**直接调用 do_raw_spin_lock**()函数.

```c
void do_raw_spin_lock(raw_spinlock_t *lock)
{
	arch_spin_lock(&lock->raw_lock)
}
```

x86 的 arch_spin_lock()函数的实现.

```c
[arch/x86/include/asm/spinlock.h]
static __always_inline void arch_spin_lock(arch_spinlock_t *lock)
{
	register struct __raw_tickets inc = { .tail = TICKET_LOCK_INC };

	inc = xadd(&lock->tickets, inc);
	if (likely(inc.head == inc.tail))
		goto out;

	for (;;) {
		unsigned count = SPIN_THRESHOLD;

		do {
			inc.head = READ_ONCE(lock->tickets.head);
			if (__tickets_equal(inc.head, inc.tail))
				goto clear_slowpath;
			cpu_relax();
		} while (--count);
		__ticket_lock_spinning(lock, inc.tail);
	}
clear_slowpath:
	__ticket_check_and_clear_slowpath(lock, inc.head);
out:
	barrier();	/* make sure nothing creeps before the lock is taken */
}
```

下面来看 ARM 的 `arch_spin_lock()` 函数的实现.

```c
[arch/arm/include/asm/spinlock.h]
static inline void arch_spin_lock(arch_spinlock_t *lock)
{
	unsigned long tmp;
	u32 newval;
	arch_spinlock_t lockval;

	prefetchw(&lock->slock);
	__asm__ __volatile__(
"1:	ldrex	%0, [%3]\n"
"	add	%1, %0, %4\n"
"	strex	%2, %1, [%3]\n"
"	teq	%2, #0\n"
"	bne	1b"
	: "=&r" (lockval), "=&r" (newval), "=&r" (tmp)
	: "r" (&lock->slock), "I" (1 << TICKET_SHIFT)
	: "cc");
    // 位置 1
	while (lockval.tickets.next != lockval.tickets.owner) {
		wfe();
		lockval.tickets.owner = ACCESS_ONCE(lock->tickets.owner);
	}

	smp_mb();
}
```

这是一段 GCC 嵌入式汇编, 与前文中的 atomic_add()函数类似. 首先通过**ldrex 指令**把**lock->slock**的值加载到**变量 lockval**中, lockval 中的**next 域加 1**, 并且**保存到 newval 变量**中, 然后把**newval 值**写入到**lock->slock**中, 也就是**增加锁中 next 域的值**, 即 next\+\+. 此时 lockval 是原有的 lock 的内容.

位置 1, 判断变量**lockval 中的 next 域**和**owner 域是否相等**, 如果**不相等**, 则调用**wfe 指令**让**CPU 进入等待状态**. 当有**其他 CPU 唤醒本 CPU**时, 说明该**spinlock 锁的 owner 域发生了变化**, 即**有人释放了该锁**; 当**新 owner 域**的值和**next 相等**时, 即**owner**等于**该 CPU 持有的等号牌(lockval.next**)时, 说明**该 CPU 成功获取了 spinlock 锁(以 CPU 为单位！！！**), `arch_spin_lock()` 函数返回.

下面来说明 ARM 体系结构中的 wfe 指令. ARM 体系结构中的 WFI(Wait for interrupt)和 WFE(Wait for event)指令都是让 ARM 核进入 standby 睡眠模式. WFI 是直到有 WFI 唤醒事件发生才会唤醒 CPU, WFE 是直到有 WFE 唤醒事件发生, 这两类事件大部分相同, 唯一不同在于 WFE 可以被其他 CPU 上的 SEV 指令唤醒, SEV 指令用于修改 Event 寄存器的指令.

下面来看释放 spinlock 的 `arch_spin_imlock()` 函数的实现.

```c
static inline void arch_spin_unlock(arch_spinlock_t *lock)
{
	smp_mb();
	lock->tickets.owner++;
	dsb_sev();
}
```

**arch_spin_unlock**()函数实现比较简单, 首先调用 smp_mb()**内存屏障指令**, 在 ARM 中 smp_mb()函数也是调用 dmb 指令来保证把**调用该函数之前所有的访问内存指令都执行完成**, 然后给**lock->owner 域加 1(！！！**). 最后调用 dsb_sev()函数, 该函数有两个作用, 一个是**调用 dsb 指令**保证**owner 域己经写入内存**中, 二是执行**SEV 指令**来**唤醒**通过 WFE 指令进入**睡眠状态的 CPU**.

# spinlock 变种

在**驱动代码**编写过程中常常会遇到这样一个问题, 假设某个驱动程序中有一个**链表 a_driver_list**, 在驱动中很多操作都需要**访问和更新该链表**, 例如 open、ioctl 等. 因此**操作链表的地方**就是一个**临界区**, 需要**spinlock 来保护**. 当处于**临界区**时发生了**外部硬件中断**, 此时系统**暂停当前进程的执行**而转去**处理该中断**. 假设**中断处理程序**恰巧也要**操作该链表**, 链表的操作是一个**临界区**, 所以在**操作之前**要调用 **spin_lock**() 函数来**对该链表进行保护**. **中断处理函数**试图去**获取该 spinlock**, 但因为它己经被别人持有了, 于是导致**中断处理函数**进入**忙等待状态**或者 **WFE 睡眠状态**.

在**中断上下文**出现**忙等待或者睡眠状态**是**致命的**, 中断处理程序要求"短"和"快", 锁的持有者因为被中断打断而不能尽快释放锁, 而中断处理程序一直在忙等待锁, 从而导致死锁的发生. Linux 内核的 spinlock 的变种 **spin_lock_irq() 函数**在获取**spinlock**时**关闭本地 CPU 中断**, 可以解决该问题.

```c
[include/linux/spinlock.h]
static inline void spin_lock_irq(spinlock_t *lock)
{
	raw_spin_lock_irq(&lock->rlock);
}

[include/linux/spinlock_api_smp.h]
static inline void __raw_spin_lock_irq(raw_spinlock_t *lock)
{
	local_irq_disable();
	preempt_disable();
	do_raw_spin_lock();
}
```

spin_lock_irq()函数的实现比 spin_lock()函数多了一个**local_irq_disable()函数**, 该函数用于关闭本地处理器中断, 这样在获取 spinlock 锁时可以**确保不会发生中断**, 从而避免发生死锁问题, 因此 spin_lock_irq()主要防止本地中断处理程序和持有锁者之间存在锁的争用. 可能有的读者会有疑问, 既然**关闭了本地 CPU 的中断**, 那么**别的 CPU**依然可以**响应外部中断**, 会不会也有可能死锁?**持有锁者在 CPU0**上, **CPU1**响应了外部中断且中断处理函数也同样试图去获取该锁, 因为**CPU0 上的锁持有者**也在继续**执行**, 所以它很快会离开临界区释放了锁, 这样 CPU1 上的中断处理函数可以很快获得该锁.

在上述场景中, 如果**CPU0**在**临界区**中发生了**进程切换**, 会是什么情况?注意**进入 spinlock 之前**己经显式地**调用 preempt_disable()关闭了抢占**, 因此**内核不会主动发生抢占**. 但令人担心的是, 驱动编写者**主动调用睡眠函数**, 从而**发生了调度**.

使用 spinlock 的重要原则是: **拥有 spinlock 锁**的**临界区代码**必须是**原子执行**, **不能休眠和主动调度(！！！**). 但在实际工程中, 驱动代码编写者却常常容易犯错误. 例如调用分配内存函数 kmalloc()时, 就有可能因为系统空闲内存不足而睡眠等待, 除非显式地使用 GFP_ATOMIC 分配掩码.

**spin_lock_irqsave**()函数会**保存本地 CPU 当前的 irq 状态**并且**关闭本地 CPU 中断**, 然后获取 spinlock 锁. **local_irq_save**()函数在**关闭本地 CPU 中断前**把**CPU 当前的中断状态保存到 flags 变量**中; 在调用**local_irq_restore**()函数时把**flags 值恢复到相关寄存器**中, 例如 ARM 的 CPSR 寄存器中, 这样做的目的是防止破坏掉中断响应的状态.

spinlock 还有另外一个常用的变种**spin_lock_bh**()函数, 用于处理**进程和延迟处理机制**导致的**并发访问的互斥问题**.

# 3 spinlock 和 raw_spin_lock

有的代码中使用了 spin_lock(), 而有的代码使用 raw_spin_lock(), 并且发现 spin_lock()直接调用 raw_spin_lock(), 读者可能会有困惑.

这要从 Linux 内核的实时补丁 RT-patch 说起, **实时补丁**旨在提升**Linux 内核的实时性**,它允许在**spinlock 锁的临界区**内**被抢占**, 且**临界区内允许进程睡眠等待**, 这样会导致**spinlock 语义被修改**. 当时内核中大约有 10000 多处使用了 spinlock, 直接修改 spinlock 的工作量巨大, 但是可以修改那些真正不允许抢占和休眠的地方, 大概有 100 多处, 因此改为使用 raw_spin_lock. spinlock 和 raw_spin_lock 的区别在于:

- 在**绝对不允许被抢占和睡眠的临界区**, 应该使用**raw_spin_lock**, 否则使用**spinlock**.

因此对于**没有打上 RT-patch**的 Linux 内核来说, spin_lock()直接调用 raw_spin_lock();对于打上了**RT-patch 的 Linux 内核**, spinlock 变成**可抢占和睡眠的锁(！！！**), 这一点需要特别注意.

# 4 总结

**spinlock**主要针对**数据操作集合的临界区**, 临界区是**一个变量**, **原子变量**可以解决. 抢到锁的**进程不能睡眠**, 并要**快速执行**(这是 spinlock 的**设计语义**).

"FIFO ticket-based 算法":

锁由 slock(和\<next, owner\>组成 union)构成

- 获取锁: CPU 先领 ticket(当前 next 值), 然后锁的 next 加 1, owner 等于 ticket, CPU 获得锁, 返回, 否则循环忙等待
- 释放锁: 锁的 owner 加 1

获取锁接口(这里只提这两个):

- spin_lock(): 关内核抢占, 循环抢锁, 但来中断(中断会抢占所有)可能导致死锁
- spin_lock_irq(): 关本地中断, 关内核抢占(内核不会主动抢占), 循环抢锁

spin_lock()和 raw_spin_lock():

- 在绝对不允许被抢占和睡眠的临界区, 使用 raw_spin_lock
- Linux 实时补丁 spin_lock()允许**临界区抢占**和**进程睡眠**, 否则 spin_lock()直接调用 raw_spin_lock()

spin_lock 特点:

- 忙等待, 不断尝试
- **同一时刻只能有一个获得**
- spinlock 临界区尽快完成, 不能睡眠
- 可以在中断上下文使用