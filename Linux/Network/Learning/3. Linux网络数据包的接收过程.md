
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 网卡到内存](#1-网卡到内存)
- [2. 内核的网络模块](#2-内核的网络模块)
- [3. 协议栈](#3-协议栈)
  - [3.1. IP 层](#31-ip-层)
  - [3.2. UDP 层](#32-udp-层)
- [4. socket](#4-socket)
- [5. 总结语](#5-总结语)
- [6. 参考](#6-参考)

<!-- /code_chunk_output -->

https://segmentfault.com/a/1190000008836467

介绍在 Linux 系统中, 数据包是如何一步一步从网卡传到进程手中的. 

强烈建议阅读后面参考里的两篇文章, 里面介绍的更详细. 

本文只讨论以太网的物理网卡, 不涉及虚拟设备, 并且以一个 UDP 包的接收过程作为示例.

>本示例里列出的函数调用关系来自于 kernel 3.13.0

# 1. 网卡到内存

网卡需要有**驱动**才能工作, 驱动是加载到**内核中的模块**, 负责衔接**网卡**和**内核**的**网络模块**, 驱动在**加载**的时候将自己**注册进网络模块**, 当相应的**网卡**收到**数据包**时, **网络模块**会调用相应的驱动程序处理数据. 

下图展示了数据包(packet)如何进入内存, 并被内核的网络模块开始处理: 

```
                   +-----+
                   |     |                            Memroy
+--------+   1     |     |  2  DMA     +--------+--------+--------+--------+
| Packet |-------->| NIC |------------>| Packet | Packet | Packet | ...... |
+--------+         |     |             +--------+--------+--------+--------+
                   |     |<--------+
                   +-----+         |
                      |            +---------------+
                      |                            |
                    3 | Raise IRQ                  | Disable IRQ
                      |                          5 |
                      |                            |
                      ↓                            |
                   +-----+                   +------------+
                   |     |  Run IRQ handler  |            |
                   | CPU |------------------>| NIC Driver |
                   |     |       4           |            |
                   +-----+                   +------------+
                                                   |
                                                6  | Raise soft IRQ
                                                   |
                                                   ↓
```

- 1:  **数据包**从**外面的网络**进入**物理网卡**. 如果**目的地址不是该网卡(！！！**), 且该**网卡没有开启混杂模式(！！！**), 该包会**被网卡丢弃**. 

- 2:  网卡(驱动)将数据包通过**DMA 的方式(！！！**)写入到**指定的内存地址**, 该地址由**网卡驱动分配并初始化(！！！**). 

    注: **老的网卡**可能**不支持 DMA**, 不过新的网卡一般都支持. 

- 3:  网卡通过**硬件中断(IRQ**)通知**CPU**, 告诉它有数据来了

- 4:  CPU 根据**中断表**, 调用已经注册的**中断函数**, 这个中断函数会调到**驱动程序(NIC Driver**)中相应的函数

- 5:  **驱动**先**禁用网卡的中断(不是 CPU 中断！！！**), 表示驱动程序已经知道内存中有数据了, 告诉网卡下次再收到数据包**直接写内存(！！！**)就可以了, **不要再通知 CPU(驱动程序自己的行为, 驱动收到网络数据！！！**)了, 这样可以提高效率, **避免 CPU 不停的被中断**. 

- 6:  启动**软中断**. 这步结束后, **硬件中断**处理函数就**结束返回**了. 由于**硬中断处理程序**执行的过程中**不能被中断**, 所以如果它执行时间过长, 会导致 CPU 没法响应其它硬件的中断, 于是内核引入软中断, 这样可以将硬中断处理函数中耗时的部分移到软中断处理函数里面来慢慢处理. 

# 2. 内核的网络模块

软中断会触发**内核网络模块(！！！**)中的**软中断处理函数(！！！**), 后续流程如下

```
                                                     +-----+
                                             17      |     |
                                        +----------->| NIC |
                                        |            |     |
                                        |Enable IRQ  +-----+
                                        |
                                        |
                                  +------------+                                      Memroy
                                  |            |        Read           +--------+--------+--------+--------+
                 +--------------->| NIC Driver |<--------------------- | Packet | Packet | Packet | ...... |
                 |                |            |          9            +--------+--------+--------+--------+
                 |                +------------+
                 |                      |    |        skb
            Poll | 8      Raise softIRQ | 6  +-----------------+
                 |                      |             10       |
                 |                      ↓                      ↓
         +---------------+  Call  +-----------+        +------------------+        +--------------------+  12  +---------------------+
         | net_rx_action |<-------| ksoftirqd |        | napi_gro_receive |------->| enqueue_to_backlog |----->| CPU input_pkt_queue |
         +---------------+   7    +-----------+        +------------------+   11   +--------------------+      +---------------------+
                                                               |                                                      | 13
                                                            14 |        + - - - - - - - - - - - - - - - - - - - - - - +
                                                               ↓        ↓
                                                    +--------------------------+    15      +------------------------+
                                                    | __netif_receive_skb_core |----------->| packet taps(AF_PACKET) |
                                                    +--------------------------+            +------------------------+
                                                               |
                                                               | 16
                                                               ↓
                                                      +-----------------+
                                                      | protocol layers |
                                                      +-----------------+
```

- 7:  内核中的**ksoftirqd 进程**专门负责**软中断**的处理, 当它收到软中断后, 就会调用相应软中断所对应的处理函数, 对于上面第 6 步中是网卡驱动模块抛出的软中断, ksoftirqd 会调用网络模块的**net\_rx\_action**函数

- 8:  net\_rx\_action 调用**网卡驱动**里的**poll 函数**来一个一个的**处理数据包**

- 9:  在 pool 函数中, 驱动会**一个接一个(！！！**)的**读取网卡写到内存中的数据包(！！！**), 内存中**数据包的格式只有驱动(！！！**)知道

- 10:  **驱动程序**将**内存中的数据包**转换成**内核网络模块**能识别的**skb 格式**, 然后调用 napi\_gro\_receive 函数

- 11:  napi\_gro\_receive 会处理 GRO 相关的内容, 也就是**将可以合并的数据包进行合并**, 这样就**只需要调用一次协议栈**. 然后判断是否开启了 RPS, 如果开启了, 将会调用 enqueue\_to\_backlog

- 12:  在 enqueue\_to\_backlog 函数中, 会将**数据包**放入 CPU 的**softnet\_data 结构体**的**input\_pkt\_queue**中, 然后返回, 如果 input\_pkt\_queue 满了的话, 该数据包将会被丢弃, queue 的大小可以通过 net.core.netdev\_max\_backlog 来配置

- 13:  CPU 会接着在自己的**软中断上下文**中处理自己 input\_pkt\_queue 里的网络数据(调用\_\_netif\_receive\_skb\_core)

- 14:  如果没开启 RPS, napi\_gro\_receive 会直接调用\_\_netif\_receive\_skb\_core

- 15:  看是不是有 AF\_PACKET 类型的**socket(也就是我们常说的原始套接字**), 如果有的话, 拷贝一份数据给它. **tcpdump 抓包就是抓的这里的包！！！**. 

- 16:  **调用协议栈相应的函数**, 将**数据包**交给**协议栈(！！！**)处理. 

- 17:  待内存中的所有数据包被处理完成后(即**poll 函数执行完成**), 启用网卡的硬中断, 这样下次网卡再收到数据的时候就会通知 CPU

>enqueue\_to\_backlog 函数也会被 netif\_rx 函数调用, 而 netif\_rx 正是 lo 设备发送数据包时调用的函数

# 3. 协议栈

## 3.1. IP 层

由于是 UDP 包, 所以第一步会进入 IP 层, 然后一级一级的函数往下调: 

```
          |
          |
          ↓         promiscuous mode &&
      +--------+    PACKET_OTHERHOST (set by driver)   +-----------------+
      | ip_rcv |-------------------------------------->| drop this packet|
      +--------+                                       +-----------------+
          |
          |
          ↓
+---------------------+
| NF_INET_PRE_ROUTING |
+---------------------+
          |
          |
          ↓
      +---------+
      |         | enabled ip forword  +------------+        +----------------+
      | routing |-------------------->| ip_forward |------->| NF_INET_FORWARD |
      |         |                     +------------+        +----------------+
      +---------+                                                   |
          |                                                         |
          | destination IP is local                                 ↓
          ↓                                                 +---------------+
 +------------------+                                       | dst_output_sk |
 | ip_local_deliver |                                       +---------------+
 +------------------+
          |
          |
          ↓
 +------------------+
 | NF_INET_LOCAL_IN |
 +------------------+
          |
          |
          ↓
    +-----------+
    | UDP layer |
    +-----------+
```

- ip\_rcv:  **ip\_rcv**函数是**IP 模块的入口函数**, 在该函数里面, 第一件事就是将**垃圾数据包**(**目的 mac 地址不是当前网卡！！！**, 但由于**网卡设置了混杂模式而被接收进来！！！**)直接丢掉, 然后调用注册在 NF\_INET\_PRE\_ROUTING 上的函数

- NF\_INET\_PRE\_ROUTING:  netfilter 放在协议栈中的钩子, 可以通过 iptables 来注入一些数据包处理函数, 用来修改或者丢弃数据包, 如果数据包没被丢弃, 将继续往下走

- routing:  进行路由, 如果是**目的 IP 不是本地 IP**, 且**没有开启 ip forward 功能**, 那么数据包将被丢弃, 如果开启了 ip forward 功能, 那将进入 ip\_forward 函数

- ip\_forward:  ip\_forward 会先调用 netfilter 注册的 NF\_INET\_FORWARD 相关函数, 如果数据包没有被丢弃, 那么将继续往后调用 dst\_output\_sk 函数

- dst\_output\_sk:  该函数会调用 IP 层的相应函数将该数据包发送出去, 同下一篇要介绍的数据包发送流程的后半部分一样. 

- ip\_local\_deliver: 如果上面 routing 的时候发现目的 IP 是本地 IP, 那么将会调用该函数, 在该函数中, 会先调用 NF\_INET\_LOCAL\_IN 相关的钩子程序, 如果通过, 数据包将会向下发送到 UDP 层

## 3.2. UDP 层

```
          |
          |
          ↓
      +---------+            +-----------------------+
      | udp_rcv |----------->| __udp4_lib_lookup_skb |
      +---------+            +-----------------------+
          |
          |
          ↓
 +--------------------+      +-----------+
 | sock_queue_rcv_skb |----->| sk_filter |
 +--------------------+      +-----------+
          |
          |
          ↓
 +------------------+
 | __skb_queue_tail |
 +------------------+
          |
          |
          ↓
  +---------------+
  | sk_data_ready |
  +---------------+
```

- udp\_rcv: udp\_rcv 函数是 UDP 模块的入口函数, 它里面会调用其它的函数, 主要是做一些必要的检查, 其中一个重要的调用是\_\_udp4\_lib\_lookup\_skb, 该函数会根据**目的 IP 和端口**找对应的**socket**, 如果没有找到相应的 socket, 那么该数据包将会被丢弃, 否则继续

- sock\_queue\_rcv\_skb:  主要干了两件事, 一是检查这个**socket**的**receive buffer**是不是满了, 如果满了的话, 丢弃该数据包, 然后就是调用 sk\_filter 看这个包是否是满足条件的包, 如果当前 socket 上设置了 filter, 且该包不满足条件的话, 这个数据包也将被丢弃(在 Linux 里面, 每个 socket 上都可以像 tcpdump 里面一样定义 filter, 不满足条件的数据包将会被丢弃)

- \_\_skb\_queue\_tail: 将**数据包**放入**socket 接收队列的末尾**

- sk\_data\_ready: 通知 socket 数据包已经准备好

>调用完 sk\_data\_ready 之后, 一个数据包处理完成, 等待应用层程序来读取, 上面所有函数的执行过程都在**软中断的上下文(协议栈操作在软中断上下文！！！**)中. 

# 4. socket

**应用层**一般有**两种方式接收数据**, 一种是 recvfrom 函数阻塞在那里等着数据来, 这种情况下当 socket 收到通知后, recvfrom 就会被唤醒, 然后读取接收队列的数据; 另一种是通过**epoll 或者 select 监听相应的 socket**, 当收到通知后, 再**调用 recvfrom 函数**去读取接收队列的数据. 两种情况都能正常的接收到相应的数据包. 

# 5. 总结语

了解数据包的接收流程有助于帮助我们搞清楚我们可以在哪些地方监控和修改数据包, 哪些情况下数据包可能被丢弃, 为我们处理网络问题提供了一些参考, 同时了解 netfilter 中相应钩子的位置, 对于了解 iptables 的用法有一定的帮助, 同时也会帮助我们后续更好的理解 Linux 下的网络虚拟设备. 

总结: 

1. 数据从物理网线/无线方式到达网卡后, 网卡驱动会通过 DMA 操作(网卡一般自带了 DMA 控制器, 没有使用主板上通用的)将网卡的数据拷贝到物理内存(网卡驱动初始化分配的, 所以这是一段内核态内存), 然后网卡触发硬件中断, 驱动先禁用网卡的中断(不是 CPU 中断), 网卡下次再收到数据包直接写内存, 这样避免 CPU 不停的被中断. 最后数据处理主要在下半部, 软中断中.

2. 驱动程序一个一个处理内存中数据包, 将内存中的数据包(之前只有驱动能识别)转换成内核网络模块能识别的 skb 格式, 然后整合等操作, 将其转发给协议栈(协议栈动作也是在软中断上下文), 等到内存中数据处理完后开启网卡中断

3. IP 层将目的 mac 非当前网卡的数据包丢弃(由于网卡的混杂模式接收的), 非本地 IP 且没有 IP forward 则丢弃; UDP 层根据目的 IP 和端口找对应的 socket, socket 的 receive buffer 没满并且没被过滤则将数据放到队列, 然后通过 socket; 应用层会将数据拷贝到**自己的用户空间(用户态内存空间！！！**)

在接下来的几篇文章中, 将会介绍 Linux 下的网络虚拟设备和 iptables. 

# 6. 参考

- [Monitoring and Tuning the Linux Networking Stack: Receiving Data](https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/)
- [Illustrated Guide to Monitoring and Tuning the Linux Networking Stack: Receiving Data](https://blog.packagecloud.io/eng/2016/10/11/monitoring-tuning-linux-networking-stack-receiving-data-illustrated/)
- [NAPI](https://wiki.linuxfoundation.org/networking/napi)