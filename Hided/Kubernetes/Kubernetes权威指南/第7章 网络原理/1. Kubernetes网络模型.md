
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 网络模型基础原则](#1-网络模型基础原则)
- [2. IP-Per-Pod模型](#2-ip-per-pod模型)
  - [2.1. Pod之间访问使用对方Pod的实际地址](#21-pod之间访问使用对方pod的实际地址)
  - [2.2. 同一个Pod内不同容器共享同一个网络命名空间(同一个Linux网络协议栈)](#22-同一个pod内不同容器共享同一个网络命名空间同一个linux网络协议栈)
  - [2.3. 与Docker原生动态端口映射方式对比](#23-与docker原生动态端口映射方式对比)
- [3. Kubernetes集群网络实现要求](#3-kubernetes集群网络实现要求)

<!-- /code_chunk_output -->

# 1. 网络模型基础原则

Docker的传统网络模型在应用至日趋复杂的实际业务场景时必将导致复杂性的几何级数上升, Kubernetes**网络模型**设计的一个**基础原则**是: **每个Pod！！！** 都拥有一个**独立的IP地址！！！**, 并**假定所有Pod**都在一个可以**直接连通的！！！**、**扁平的网络空间！！！** 中. 

所以不管它们**是否运行在同一个Node(宿主机！！！**)中, 都要求它们可以**直接通过对方的IP进行访问**. 

设计这个**原则的原因**是, 用户**不需要额外考虑！！！** 如何建立**Pod之间！！！的连接**, 也**不需要考虑**如何将**容器端口**映射到**主机端口！！！** 等问题. 

# 2. IP-Per-Pod模型

实际上, 在Kubernetes的世界里, **IP！！！** 是以**Pod为单位！！！** 进行**分配！！！**的, 而**非容器**. 

**一个Pod内部**的**所有容器！！！共享一个网络堆栈**(相当于一个**网络命名空间！！！**, 它们的**IP地址**、**网络设备**、**配置**等都是**共享的**). 

按照这个网络原则抽象出来的为**每个Pod**都设置**一个IP地址**的模型也被称作**IP\-per\-Pod模型**. 

## 2.1. Pod之间访问使用对方Pod的实际地址

由于Kubernetes的网络模型假设**Pod之间访问！！！** 时使用的是**对方Pod的实际地址！！！**, 所以**一个Pod！！！内部的应用程序**看到的**自己**的**IP地址**和**端口**与集群内**其他Pod看到的一样**. 

它们都是**Pod实际分配的IP地址**. 将**IP地址**和**端口**在**Pod内部**和**外部**都**保持一致**, 也就**不需要使用NAT来进行地址转换**了. 

Kubernetes的网络之所以这么设计, 主要原因就是可以**兼容过去的应用**. 当然, 我们使用Linux命令"**ip addr show**"也能看到这些地址, 和程序看到的没有什么区别. 所以这种IP\-per\-Pod的方案很好地利用了现有的各种域名解析和发现机制. 

## 2.2. 同一个Pod内不同容器共享同一个网络命名空间(同一个Linux网络协议栈)

为每个Pod都设置一个IP地址的模型还有**另外一层含义**, 那就是**同一个Pod内！！！** 的**不同容器！！！** 会**共享同一个网络命名空间！！！**, 也就是**同一个Linux网络协议栈**. 

这就意味着**同一个Pod内**的**容器**可以通过**localhost来连接对方的端口**. 

这种关系和**同一个VM**内的**进程之间的关系**是一样的, 看起来**Pod内容器之间的隔离性减小**了, 而且**Pod内不同容器之间的端口是共享**的, 就没有所谓的**私有端口**的概念了. 

如果**你的应用**必须要使用一些**特定的端口范围**, 那么你也可以**为这些应用单独创建一些Pod！！！**. 反之, 对那些没有特殊需要的应用, 由于Pod内的容器是共享部分资源的, 所以可以通过共享资源互相通信, 这显然更加容易和高效. 针对这些应用, 虽然损失了可接受范围内的部分隔离性, 却也是值得的. 

## 2.3. 与Docker原生动态端口映射方式对比

**IP\-per\-Pod模式**和**Docker原生！！！的通过动态端口映射方式**实现的**多节点访问模式！！！** 有什么区别呢?

主要区别是**后者的动态端口映射会引入端口管理的复杂性**, 而且访问者看到的IP地址和端口与服务提供者**实际绑定的不同**(因为**NAT**的缘故, 它们都被映射成**新的地址或端口**了), 这也会引起**应用配置的复杂化**. 

同时, **标准的DNS等名字解析服务也不适用**了, 甚至**服务注册**和**发现机制**都将迎来挑战, 因为在**端口映射**情况下, 服务**自身很难知道**自己对外暴露的**真实的服务IP和端口**, **外部应用**也无法通过**服务所在容器的私有IP地址和端口**来访问服务. 

总的来说, IP\-per\-Pod模型是一个简单的兼容性较好的模型. 从该模型的**网络的端口分配**、**域名解析**、**服务发现**、**负载均衡**、**应用配置**和**迁移**等角度来看, **Pod**都能够被看作**一台独立的虚拟机或物理机！！！**. 

# 3. Kubernetes集群网络实现要求

按照这个网络抽象原则, Kubernetes对网络有什么前提和要求呢?

Kubernetes对**集群网络**有如下要求. 

(1)**所有容器(Pod**)都可以在**不用NAT的方式！！！** 下同**别的容器通信**. 

(2)**所有节点**都可以在**不用NAT的方式！！！** 下同**所有容器通信**, 反之亦然. 

(3)**容器自己使用的IP**也是**其他容器**或**节点**直接看到的地址. . 换句话讲, 所有Pod对象都位于同一平面网络中, 而且可以使用Pod自身的地址直接通信. 

这些基本要求意味着并不是只要**两台机器都运行Docker**, Kubernetes就可以工作了. 具体的集群网络实现必须满足上述基本要求, **原生的Docker网络**目前还**不能很好地支持这些要求**. 

实际上, 这些对网络模型的要求并没有降低整个网络系统的复杂度. 如果你的程序原来在VM上运行, 而那些VM拥有独立IP, 并且它们之间可以直接透明地通信, 那么Kubernetes的网络模型就和VM使用的网络模型一样. 所以使用这种模型可以很容易地将已有的应用程序从VM或者物理机迁移到容器上. 

当然, 谷歌设计Kubernetes的一个**主要运行基础**就是其**公有云GCE**, GCE默认支持这些网络要求. 另外, 常见的其他公有云服务商如亚马逊等, 其公有云环境也支持这些网络要求. 

由于部署私有云的场景也非常普遍, 所以在私有云中运行Kubernetes+Docker集群之前, 需要自己搭建出**符合Kubernetes要求的网络环境**. 有很多**开源组件**可以帮助我们**打通Docker容器和容器之间**的网络, 实现满足Kubernetes要求的网络模型. 当然, 每种方案都有适合的场景, 我们要根据自己的实际需要进行选择. 在后面的章节中会对常见的开源方案进行介绍. 

Kubernetes的网络**依赖于Docker**, Docker的网络又离不开**Linux操作系统内核特性**的支持, 所以我们有必要先深入了解Docker背后的网络原理和基础知识. 接下来一起深入学习必要的Linux网络知识. 
