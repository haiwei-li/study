
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [0 概述](#0-概述)
- [1 安装kubeadm和相关工具](#1-安装kubeadm和相关工具)
- [2 kubeadm config](#2-kubeadm-config)
- [3 下载Kubernetes的相关镜像](#3-下载kubernetes的相关镜像)
- [4 安装Master](#4-安装master)
- [5 安装Node, 加入集群](#5-安装node-加入集群)
- [6 安装网络插件](#6-安装网络插件)
- [7 安装Kubernetes集群是否安装完成](#7-安装kubernetes集群是否安装完成)
- [8 kube\-proxy开启ipvs](#8-kube-proxy开启ipvs)
- [9 从集群中移除Node](#9-从集群中移除node)

<!-- /code_chunk_output -->

# 0 概述

最简单方法是使用`yum install kubernetes`命令安装Kubernetes集群, 但仍需**修改各组件的启动参数**, 才能完成对Kubernetes集群的配置, 整个过程比较复杂, 也容易出错. 

Kubernetes从1.4版本开始引入了**命令行工具kubeadm**, 致力于简化集群的安装过程, 并解决Kubernetes集群的高可用问题. 

最近发布的[Kubernetes 1.15](https://www.kubernetes.org.cn/tags/kubernetes1-15)中, [kubeadm](https://www.kubernetes.org.cn/tags/kubeadm)对HA集群的配置已经达到beta可用, 说明kubeadm快在生产环境中可用

安装etcd并启动, 同时加入开机启动列表中. 通过命令

```
# yum install etcd -y
# systemctl enable etcd.service
# systemctl start etcd.service
```

通过`etcdctl cluster\-health`, 可验证etcd是否正确启动

```
# etcdctl cluster-health
```

# 1 安装kubeadm和相关工具

在所有节点上

先配置yum源, 官方yum源地址为`https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64`. 

可以使用国内的一个yum源, 比如腾讯的, 地址为`http://mirrors.tencent.com/kubernetes/yum/repos/kubernetes-el7-x86_64/`, 配置如下

阿里云的baseurl是http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/

```
[root@localhost ~]# vim /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes Repo
baseurl=http://mirrors.tencent.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0

[root@localhost ~]# yum makecache
```

测试地址是否可用.

```
curl http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
```

安装kubeadm和相关工具:

```
[root@localhost ~]# yum install -y kubelet kubeadm kubectl --disableexcludes=kebernetes
```

- **kubeadm**: **启动 k8s 集群的命令工具**
- **kubelet**: 集群**容器内**的命令工具
- **kubectl**: **操作集群**的命令工具

其中会安装cri\-tools, 这是CRI(Container Runtime Interface)容器运行时接口的命令行工具

启动docker服务和kubelet服务, 并设为开机自动启动:

```
# systemctl enable docker && systemctl start docker
# systemctl enable kubelet && systemctl start kubelet
```

注: 这时候的kubelet服务不是active状态, 是正常的

运行kubelet –help可以看到原来kubelet的绝大多数命令行flag参数都被DEPRECATED了

而官方推荐我们使用\–config指定配置文件, 并在配置文件中指定原来这些flag所配置的内容. 具体内容可以查看这里[Set Kubelet parameters via a config file](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/). 这也是Kubernetes为了支持动态Kubelet配置(Dynamic Kubelet Configuration)才这么做的, 参考[Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/). 

kubelet的配置文件必须是json或yaml格式, 具体可[查看这里](https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go). 

修改kubelet 启动时的 cgroup driver, 使得和 docker 的一致

修改/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf

```
Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd"

ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CGROUP_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
```

```
systemctl daemon-reload
systemctl restart kubelet
```

Kubernetes 1.8开始要求关闭系统的Swap, 如果不关闭, 默认配置下kubelet将无法启动.  关闭系统的Swap方法如下:

```
# 临时关闭
# swapoff -a
```

编辑/etc/fstab, 注释掉包含swap的那一行即可, 重启后可永久关闭, 使用free -m确认swap已经关闭. swappiness参数调整, 修改/etc/sysctl.d/k8s.conf添加下面一行: 

```
vm.swappiness=0
```

执行sysctl \-p /etc/sysctl.d/k8s.conf使修改生效. 

当然, 可以修改kubelet的配置去掉这个限制. 使用kubelet的启动参数\–fail\-swap\-on=false去掉必须关闭Swap的限制, 修改/etc/sysconfig/kubelet, 加入: 

```
KUBELET_EXTRA_ARGS=--fail-swap-on=false
```

# 2 kubeadm config

kubeadm提供了配置文件功能用于**复杂定制**. 

同时, kubeadm将配置文件以ConfigMap的形式保存到集群之中, 便于后续的查询和升级工作. 

kubeadm config子命令提供了对这一组功能的支持: 

- kubeadm config upload from\-file: 由配置文件上传到集群中生成ConfigMap. 
- kubeadm config upload from\-flags: 由配置参数生成ConfigMap. 
- kubeadm config view: 查看当前集群中的配置值. 
- kubeadm config print init\-defaults: 输出kubeadm init默认参数文件的内容. 
- kubeadm config print join\-defaults: 输出kubeadm join默认参数文件的内容. 
- kubeadm config migrate: 在新旧版本之间进行配置转换. 
- kubeadm config images list: 列出所需的镜像列表. 
- kubeadm config images pull: 拉取镜像到本地. 

执行`kubeadm config print init\-defaults`, 可获取默认的初始化参数文件:

```
# kubeadm config print init-defaults > init.default.yaml
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: localhost.localdomain
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.14.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```

从默认的配置中可以看到, 可以使用imageRepository定制在集群初始化时拉取k8s所需镜像的地址. 

可修改为合适的配置. 例如, 镜像仓库的地址, 以及Pod的地址范围, 可使用如下配置:

```
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 172.16.100.139
  bindPort: 6443
nodeRegistration:
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.15.0
networking:
  podSubnet: 192.168.0.0/16
```

>使用kubeadm默认配置初始化的集群, 会在master节点打上node\-role.kubernetes.io/master:NoSchedule的污点, 阻止master节点接受调度运行工作负载. 这里测试环境只有两个节点, 所以将这个taint修改为node\-role.kubernetes.io/master:PreferNoSchedule. 

将上面内容保存为kubeadm.yaml备用

# 3 下载Kubernetes的相关镜像

为了从国内的镜像托管站点获得镜像加速支持, 建议修改Docker的配置文件, 增加Registry Mirror参数, 将镜像配置写入配置参数中, 

例如/etc/docker/daemon.json

```
{
    "registry-mirrors": ["https://registry.docker-cn.com"]
}
```

然后重启Docker服务. 

使用config images pull子命令下载所需镜像, 例如: 

```
[root@master01 ~]# kubeadm config images pull --config=kubeadm.yaml
[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.15.0
[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.15.0
[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.15.0
[config/images] Pulled k8s.gcr.io/kube-proxy:v1.15.0
[config/images] Pulled k8s.gcr.io/pause:3.1
[config/images] Pulled k8s.gcr.io/etcd:3.3.10
[config/images] Pulled k8s.gcr.io/coredns:1.3.1
```

注: `k8s.gcr.io`会被重定向到`gcr.io/google-containers`, 下面image的url是一样的

```
k8s.gcr.io/pause-amd64:3.1
gcr.io/google_containers/pause-amd64:3.1
```

如果国外镜像访问不行, 使用国内镜像加速

```
$ docker pull gcr.io/google_containers/hyperkube-amd64:v1.9.2

$ docker pull gcr.azk8s.cn/google_containers/hyperkube-amd64:v1.9.2
```

# 4 安装Master

执行kubeadm init命令即可一键安装Kubernetes的Master. 

```
[root@master01 ~]# kubeadm init --config=kubeadm.yaml --ignore-preflight-errors=all
......
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.16.100.139:6443 --token cfnvuv.lae6eb8jkqkg09ko \
    --discovery-token-ca-cert-hash sha256:f64e6a9257be7fd89539a0181febd40670273bfaaf9853942f16a15861036d82
```

这里的ignore\-preflight\-errors会忽略所有preflight错误提示, 如果是未关闭swap使用命令

```
# kubeadm init --config=kubeadm.yaml --ignore-preflight-errors=Swap
```

如果失败了, 执行下面命令后再重试.

```
[root@master01 ~]# kubeadm reset
# rm -rf /var/lib/cni/ $HOME/.kube/config /var/lib/etcd
```

根据init输出的内容基本上可以看出手动初始化安装一个Kubernetes集群所需要的关键步骤.  其中有以下关键内容: 

- \[kubelet\-start] 生成kubelet的配置文件"/var/lib/kubelet/config.yaml"
- \[certs]生成相关的各种证书
- \[kubeconfig]生成相关的kubeconfig文件
- \[control\-plane]使用/etc/kubernetes/manifests目录中的yaml文件创建apiserver、controller-manager、scheduler的静态pod
- \[bootstraptoken]生成token记录下来, 后边使用kubeadm join往集群中添加节点时会用到
- 下面的命令是配置常规用户如何使用kubectl访问集群: 

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

- 最后给出了将节点加入集群的命令

```
kubeadm join 172.16.100.139:6443 --token cfnvuv.lae6eb8jkqkg09ko \
    --discovery-token-ca-cert-hash sha256:f64e6a9257be7fd89539a0181febd40670273bfaaf9853942f16a15861036d82
```

```
To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.16.100.141:6443 --token i1y8nh.z9bguxavcwrobu86 \
    --discovery-token-ca-cert-hash sha256:1e9884c8ab9a47cb788422e34c1933d23f7aa34cae467bc6fb83b0d56ef9f511
```

按照提示, 复制配置文件到用户的home目录下:

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

这样就在Master上安装了Kubernetes, 但在集群内还是没有可用的工作Node, 并缺乏对容器网络的配置. 

这里需要注意kubeadm init命令执行完成后的最后几行提示信息, 其中包含加入节点的指令(kubeadm join)和所需的Token. 

验证ConfigMap

```
[root@master01 ~]# kubectl get -n kube-system configmap
NAME                                 DATA   AGE
coredns                              1      7m42s
extension-apiserver-authentication   6      7m46s
kube-proxy                           2      7m42s
kubeadm-config                       2      7m44s
kubelet-config-1.15                  1      7m44s
```

可以看到其中生成了名为kubeadm\-config的ConfigMap对象. 

查看一下集群状态, 确认个组件都处于healthy状态: 

```
[root@master01 ~]# kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health":"true"}
```

集群初始化如果遇到问题, 可以使用下面的命令进行清理: 

```
kubeadm reset
ifconfig cni0 down
ip link delete cni0
ifconfig flannel.1 down
ip link delete flannel.1
rm -rf /var/lib/cni/
```

# 5 安装Node, 加入集群

对于新节点的添加, 系统准备和Kubernetes yum源的配置过程是一致的, 在Node主机上执行下面的安装过程. 

(1) 安装kubeadm和相关工具

```
[root@localhost ~]# yum install -y kubelet kubeadm kubectl --disableexcludes=kebernetes
```

启动docker服务和kubelet服务, 并设为开机自动启动:

```
# systemctl enable docker && systemctl start docker
# systemctl enable kubelet && systemctl start kubelet
```

(2) 为kubeadm命令生成配置文件

```
[root@node01 ~]# kubeadm config print join-defaults > join.yaml
```

其中, apiServerEndpoint的值来自Master服务器的地址, token和tlsBootstrapToken的值就来自于使用kubeadm init安装Master的最后一行提示信息. 

生成不过期的token

```
# kubeadm token create --ttl 0 --print-join-command
kubeadm join 172.16.100.141:6443 --token 228rn3.vwxxc5o80u8y8dj2     --discovery-token-ca-cert-hash sha256:1e9884c8ab9a47cb788422e34c1933d23f7aa34cae467bc6fb83b0d56ef9f511
```

修改结果

```
[root@node01 ~]# cat join.yaml
apiVersion: kubeadm.k8s.io/v1beta2
caCertPath: /etc/kubernetes/pki/ca.crt
discovery:
  bootstrapToken:
    apiServerEndpoint: 172.16.100.139:6443
    token: cfnvuv.lae6eb8jkqkg09ko
    unsafeSkipCAVerification: true
  timeout: 5m0s
  tlsBootstrapToken: cfnvuv.lae6eb8jkqkg09ko
kind: JoinConfiguration
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: node01
  taints: null
```

(3) 执行kubeadm join命令, 将本Node加入集群:

```
[root@node01 ~]# kubeadm join --config=join.yaml
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.15" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Activating the kubelet service
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

kubeadm在Master上也安装了kubelet, 在默认情况下并不参与工作负载. 如果希望安装一个单机All\-In\-One的Kubernetes环境, 则可以执行下面的命令(删除Node的Label"node\-role.kubernetes.io/master"), 让Master成为一个Node: 

```
# kubectl taint nodes --all node-role.kubernetes.io/master-
```

# 6 安装网络插件

执行kubectl get nodes命令, 会发现Kubernetes提示Master为NotReady状态, 这是因为还没有安装CNI网络插件

```
[root@master01 ~]# kubectl get nodes
NAME       STATUS     ROLES    AGE   VERSION
master01   NotReady   master   41m   v1.15.2
node01     NotReady   <none>   84s   v1.15.2
```

下面根据kubeadm的提示安装CNI网络插件. 对于CNI网络插件, 可以有许多选择, 请参考 https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network 的说明. 

例如, 选择weave插件, 执行下面的命令即可一键完成安装: 

```
[root@master01 ~]# kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
serviceaccount/weave-net created
clusterrole.rbac.authorization.k8s.io/weave-net created
clusterrolebinding.rbac.authorization.k8s.io/weave-net created
role.rbac.authorization.k8s.io/weave-net created
rolebinding.rbac.authorization.k8s.io/weave-net created
daemonset.extensions/weave-net created
```

# 7 安装Kubernetes集群是否安装完成

执行下面的命令, 验证Kubernetes集群的相关Pod是否都正常创建并运行: 

```
[root@master01 ~]# kubectl get pods --all-namespaces
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-5c98db65d4-k5gdt           1/1     Running   0          63m
kube-system   coredns-5c98db65d4-t6fnr           1/1     Running   0          63m
kube-system   etcd-master01                      1/1     Running   0          62m
kube-system   kube-apiserver-master01            1/1     Running   0          62m
kube-system   kube-controller-manager-master01   1/1     Running   0          62m
kube-system   kube-proxy-6xg9p                   1/1     Running   0          63m
kube-system   kube-proxy-mf9h2                   1/1     Running   0          23m
kube-system   kube-scheduler-master01            1/1     Running   0          63m
kube-system   weave-net-2ql4b                    2/2     Running   0          74s
kube-system   weave-net-f4gb8                    2/2     Running   0          74s
```

如果发现有状态错误的Pod, 则可以执行kubectl \-\-namespace=kube\-system describe pod \<pod\_name>来查看错误原因, 常见的错误原因是镜像没有下载完成. 

至此, 通过kubeadm工具就实现了Kubernetes集群的快速搭建. 如果安装失败, 则可以执行kubeadm reset命令将主机恢复原状, 重新执行kubeadm init命令, 再次进行安装. 

# 8 kube\-proxy开启ipvs

修改ConfigMap的kube\-system/kube\-proxy中的config.conf, mode: "ipvs"

之后重启各个节点上的kube-proxy pod: 

```
# kubectl get pod -n kube-system | grep kube-proxy | awk '{system("kubectl delete pod "$1" -n kube-system")}'
# kubectl get pod -n kube-system | grep kube-proxy
kube-proxy-pjz6z                   1/1     Running            0          7s
kube-proxy-rnrlx                   1/1     Running            0          9s

[root@master01 ~]# kubectl logs kube-proxy-pjz6z -n kube-system
I0816 03:42:15.298157       1 server_others.go:170] Using ipvs Proxier.
W0816 03:42:15.298551       1 proxier.go:401] IPVS scheduler not specified, use rr by default
I0816 03:42:15.299255       1 server.go:534] Version: v1.15.0
I0816 03:42:15.309150       1 conntrack.go:52] Setting nf_conntrack_max to 131072
I0816 03:42:15.310572       1 config.go:187] Starting service config controller
I0816 03:42:15.310591       1 controller_utils.go:1029] Waiting for caches to sync for service config controller
I0816 03:42:15.311470       1 config.go:96] Starting endpoints config controller
I0816 03:42:15.311587       1 controller_utils.go:1029] Waiting for caches to sync for endpoints config controller
I0816 03:42:15.411687       1 controller_utils.go:1036] Caches are synced for service config controller
I0816 03:42:15.411897       1 controller_utils.go:1036] Caches are synced for endpoints config controller
```

日志中打印出了Using ipvs Proxier, 说明ipvs模式已经开启. 

# 9 从集群中移除Node

如果需要从集群中移除node01这个Node执行下面的命令: 

在master节点上执行: 

```
kubectl drain node01 --delete-local-data --force --ignore-daemonsets
kubectl delete node node01
```

在node01上执行: 

```
kubeadm reset
ifconfig cni0 down
ip link delete cni0
ifconfig flannel.1 down
ip link delete flannel.1
rm -rf /var/lib/cni/
```

