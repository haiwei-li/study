
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [Deployment或RC: 全自动调度](#deployment或rc全自动调度)
- [NodeSelector: 定向调度](#nodeselector-定向调度)
- [3 NodeAffinity: Node亲和性调度](#3-nodeaffinitynode亲和性调度)
- [4 PodAffinity: Pod亲和与互斥调度策略](#4-podaffinitypod亲和与互斥调度策略)

<!-- /code_chunk_output -->

在Kubernetes平台上, 我们**很少会直接创建一个Pod**, 在大多数情况下会通过**RC**、**Deployment**、**DaemonSet**、**Job**等**控制器**完成对一组Pod副本的创建、调度及全生命周期的自动控制任务. 

在**最早的Kubernetes版本**里是**没有这么多Pod副本控制器**的, **只有一个Pod副本控制器RC**(Replication Controller), 这个控制器是这样设计实现的: **RC独立于所控制的Pod**, 并通过**Label标签**这个松耦合关联关系**控制目标Pod实例的创建和销毁**, 随着Kubernetes的发展, RC也出现了新的继任者——**Deployment**, 用于更加自动地完成Pod副本的部署、版本更新、回滚等功能. 

严谨地说, **RC的继任者**其实并**不是Deployment**, 而是**ReplicaSet**, 因为 ReplicaSet进一步**增强了 RC标签选择器的灵活性**. 之前**RC的标签选择器只能选择一个标签**, 而ReplicaSet拥有**集合式的标签选择器**, 可以选择**多个Pod标签**, 如下所示: 

```yaml
selector:
  matchLabels:
    tier: frontend
  matchExpressions:
    - {key: tier, operator: In, values: [frontend]}
```

**与RC不同**, **ReplicaSet**被设计成能**控制多个不同标签的Pod副本**. 一种**常见的应用场景**是, **应用MyApp**目前发布了**v1与v2两个版本**, 用户希望MyApp的**Pod副本数保持为3个**, 可以**同时包含v1和v2版本的Pod**, 就可以用**ReplicaSet来实现这种控制**, 写法如下: 

```yaml
selector:
  matchLabels:
    version: v2
  matchExpressions:
    - {key: version, operator: In, values: [v1, v2]}
```

其实, Kubernetes的**滚动升级**就是巧妙运用ReplicaSet的这个特性来实现的, 同时, **Deployment**也是通过**ReplicaSet**来实现**Pod副本自动控制功能**的. 

我们**不应该！！！直接使用底层的ReplicaSet**来控制Pod副本, 而应该使用**管理ReplicaSet**的**Deployment对象！！！** 来控制副本, 这是来自官方的建议. 

在大多数情况下, 我们希望**Deployment**创建的**Pod副本**被成功调度到集群中的**任何一个可用节点**, 而不关心具体会调度到哪个节点. 但是, 在真实的生产环境中的确也存在一种需求: 希望**某种Pod的副本**全部在指定的**一个或者一些节点上运行**, 比如希望将MySQL数据库调度到一个具有SSD磁盘的目标节点上, 此时**Pod模板**中的**NodeSelector属性**就开始发挥作用了, 上述MySQL定向调度案例的实现方式可分为以下两步. 

(1)把具有SSD磁盘的**Node**都打上**自定义标签**"**disk=ssd**". 

(2)在**Pod模板**中设定**NodeSelector**的值为"**disk: ssd**". 

如此一来, Kubernetes在**调度Pod副本**的时候, 就会**先**按照**Node的标签**过滤出**合适的目标节点**, 然后选择**一个最佳节点**进行调度. 

上述逻辑看起来既简单又完美, 但在真实的生产环境中可能面临以下令人尴尬的问题. 

(1)如果**NodeSelector**选择的**Label不存在**或者**不符合条件**, 比如这些目标节点此时宕机或者资源不足, 该怎么办?

(2)如果要选择**多种合适**的目标节点, 比如**SSD磁盘的节点**或者**超高速硬盘的节点**, 该怎么办?Kubernates引入了**NodeAffinity(节点亲和性设置**)来解决该需求. 

在真实的生产环境中还存在如下所述的**特殊需求**. 

(1)**不同Pod**之间的**亲和性(Affinity**). 比如MySQL数据库与Redis中间件不能被调度到同一个目标节点上, 或者两种不同的Pod必须被调度到同一个Node上, 以实现本地文件共享或本地网络通信等特殊需求, 这就是PodAffinity要解决的问题. 

(2)**有状态集群的调度**. 对于ZooKeeper、Elasticsearch、MongoDB、Kafka等有状态集群, 虽然集群中的每个Worker节点看起来都是相同的, 但每个Worker节点都必须有明确的、不变的唯一ID(主机名或IP地址), 这些节点的启动和停止次序通常有严格的顺序. 此外, 由于集群需要持久化保存状态数据, 所以集群中的Worker节点对应的Pod不管在哪个Node上恢复, 都需要挂载原来的Volume, 因此这些Pod还需要捆绑具体的PV. 针对这种复杂的需求, Kubernetes提供了StatefulSet这种特殊的副本控制器来解决问题, 在Kubernetes 1.9版本发布后, StatefulSet才可用于正式生产环境中. 

(3)在**每个Node**上**调度**并且**仅仅创建一个Pod副本**. 这种调度通常用于系统监控相关的Pod, 比如主机上的日志采集、主机性能采集等进程需要被部署到集群中的每个节点, 并且只能部署一个副本, 这就是DaemonSet这种特殊Pod副本控制器所解决的问题. 

(4)对于**批处理作业**, 需要创建**多个Pod副本**来协同工作, 当这些Pod副本都完成自己的任务时, 整个批处理作业就结束了. 这种Pod运行且仅运行一次的特殊调度, 用常规的RC或者Deployment都无法解决, 所以Kubernates引入了新的Pod调度控制器Job来解决问题, 并继续延伸了定时作业的调度控制器CronJob. 

与单独的Pod实例不同, 由RC、ReplicaSet、Deployment、DaemonSet等控制器创建的Pod副本实例都是归属于这些控制器的, 这就产生了一个问题: 控制器被删除后, 归属于控制器的Pod副本该何去何从?在Kubernates 1.9之前, 在RC等对象被删除后, 它们所创建的Pod副本都不会被删除; 在Kubernates 1.9以后, 这些Pod副本会被一并删除. 如果不希望这样做, 则可以通过kubectl命令的\-\-cascade=false参数来取消这一默认特性: 

```
# kubectl delete replicaset my-repset --cascade=false
```

# Deployment或RC: 全自动调度

**Deployment**或**RC**的主要功能之一就是**自动部署**一个容器应用的**多份副本**, 以及持续监控副本的数量, 在集群内始终维持用户指定的副本数量. 

下面是一个Deployment配置的例子, 使用这个配置文件可以创建一个ReplicaSet, 这个ReplicaSet会创建3个Nginx应用的Pod: 

```yaml
# nginx-deployment.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80 
```

运行kubectl create命令创建这个Deployment: 

```
# kubectl create -f nginx-deployment.yaml
deployment "nginx-deployment" created
```

查看Deployment的状态:

```
# kubectl get deployment
NAME              DESIRED   CURRENT   UP-TO-DATE  AVAILABLE AGE
nginx-deployment  3         3         3           3         18s
```

该状态说明Deployment已创建好所有3个副本, 并且所有副本都是最新的可用的. 

通过运行kubectl get rs和kubectl get pods可以查看已创建的ReplicaSet(RS)和Pod的信息. 

```
#kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-4087004473   3         3         3       53s

# kubectl get pods
NAME                                READY   STATUS  RESTARTS    AGE
nginx-deployment-4087004473-9jqqs   1/1     Running 0           1m
nginx-deployment-4087004473-cq0cf   1/1     Running 0           1m
nginx-deployment-4087004473-vxn56   1/1     Running 0           1m
```

从调度策略上来说, 这3个Nginx Pod由系统全自动完成调度. 它们各自最终运行在哪个节点上, 完全由Master的Scheduler经过一系列算法计算得出, 用户无法干预调度过程和结果. 

除了使用系统自动调度算法完成一组Pod的部署, Kubernetes也提供了多种丰富的调度策略, 用户只需在Pod的定义中使用NodeSelector、NodeAffinity、PodAffinity、Pod驱逐等更加细粒度的调度策略设置, 就能完成对Pod的精准调度. 下面对这些策略进行说明. 

# NodeSelector: 定向调度

Kubernetes **Master**上的**Scheduler**服务(**kube\-scheduler进程**)负责实现**Pod的调度**, 整个调度过程通过执行一系列复杂的算法, 最终为每个Pod都计算出一个最佳的目标节点, 这一过程是自动完成的, 通常我们**无法**知道**Pod最终会被调度到哪个节点**上. 

在实际情况下, 也可能需要将Pod调度到指定的一些Node上, 可以通过**Node的标签(Label**)和**Pod的nodeSelector属性**相匹配, 来达到上述目的. 

(1)首先通过**kubectl label命令**给**目标Node打上一些标签**: 

```
# kubectl label nodes <node-name> <label-key>=<label-value>
```

这里, 我们为k8s\-node\-1节点打上一个zone=north标签, 表明它是"北方"的一个节点: 

```
# kubectl label nodes k8s-node-1 zone=north
NAME        LABELS                                        STATUS
k8s-node-1  kubernetes.io/hostname=k8s-node-1,zone=north  Ready
```

上述命令行操作也可以通过**修改资源定义文件**的方式, 并执行**kubectl replace \-f xxx.yaml命令**来完成. 

2)然后, 在Pod的定义中加上nodeSelector的设置, 以redis\-master\-controller.yaml为例: 

```yaml
apiVersion: v1
kind: ReplicationController 
metadata:
  name: redis-master
  labels:
    name: redis-master 
spec:
  replicas: 1
  selector:
    name: redis-master
  template:
    metadata:
      labels:
        name: redis-master
    spec:
      containers:
      - name: master
        image: kubeguide/redis-master
        ports:
        - containerPort: 6379
      nodeSelector:   # 重点1
        zone: north   # 重点2
```

运行kubectl create \-f命令创建Pod, scheduler就会将该Pod调度到拥有zone=north标签的Node上. 

使用kubectl get pods \-o wide命令可以**验证Pod所在的Node**: 

```
# kubectl get pods -o wide
NAME                 READY   STATUS   RESTARTS    AGE   NODE
redis-master-f0rqj   1/1     Running  0           19s   k8s-node-1
```

如果我们给多个Node都定义了相同的标签(例如zone=north), 则scheduler会根据调度算法从这组Node中挑选一个可用的Node进行Pod调度. 

通过基于Node标签的调度方式, 我们可以把集群中具有不同特点的Node都贴上不同的标签, 例如"role=frontend""role=backend""role=database"等标签, 在部署应用时就可以根据应用的需求设置NodeSelector来进行指定Node范围的调度. 

需要注意的是, 如果我们指定了Pod的nodeSelector条件, 且在集群中不存在包含相应标签的Node, 则即使在集群中还有其他可供使用的Node, 这个Pod也无法被成功调度. 

除了用户可以自行给Node添加标签, Kubernetes也会给Node预定义一些标签, 包括: 

* kubernetes.io/hostname

* beta.kubernetes.io/os(从1.14版本开始更新为稳定版, 到1.18版本删除)

* beta.kubernetes.io/arch(从1.14版本开始更新为稳定版, 到1.18版本删除)

* kubernetes.io/os(从1.14版本开始启用)

* kubernetes.io/arch(从1.14版本开始启用)

用户也可以使用这些系统标签进行Pod的定向调度. 

NodeSelector通过标签的方式, 简单实现了限制Pod所在节点的方法. 亲和性调度机制则极大扩展了Pod的调度能力, 主要的增强功能如下. 

* 更具表达力(不仅仅是"符合全部"的简单情况). 

* 可以使用软限制、优先采用等限制方式, 代替之前的硬限制, 这样调度器在无法满足优先需求的情况下, 会退而求其次, 继续运行该Pod. 

* 可以依据节点上正在运行的其他Pod的标签来进行限制, 而非节点本身的标签. 这样就可以定义一种规则来描述Pod之间的亲和或互斥关系. 

亲和性调度功能包括节点亲和性(NodeAffinity)和Pod亲和性(PodAffinity)两个维度的设置. 节点亲和性与NodeSelector类似, 增强了上述前两点优势; Pod的亲和与互斥限制则通过Pod标签而不是节点标签来实现, 也就是上面第4点内容所陈述的方式, 同时具有前两点提到的优点. 

NodeSelector将会继续使用, 随着节点亲和性越来越能够表达nodeSelector的功能, 最终NodeSelector会被废弃. 

# 3 NodeAffinity: Node亲和性调度

NodeAffinity意为Node亲和性的调度策略, 是用于替换NodeSelector的全新调度策略. 目前有两种节点亲和性表达. 

* RequiredDuringSchedulingIgnoredDuringExecution: 必须满足指定的规则才可以调度Pod到Node上(功能与nodeSelector很像, 但是使用的是不同的语法), 相当于硬限制. 

* PreferredDuringSchedulingIgnoredDuringExecution: 强调优先满足指定规则, 调度器会尝试调度Pod到Node上, 但并不强求, 相当于软限制. 多个优先级规则还可以设置权重(weight)值, 以定义执行的先后顺序. 

IgnoredDuringExecution的意思是: 如果一个Pod所在的节点在Pod运行期间标签发生了变更, 不再符合该Pod的节点亲和性需求, 则系统将忽略Node上Label的变化, 该Pod能继续在该节点运行. 

下面的例子设置了NodeAffinity调度的如下规则. 

* requiredDuringSchedulingIgnoredDuringExecution要求只运行在amd64的节点上(beta.kubernetes.io/arch In amd64). 

* preferredDuringSchedulingIgnoredDuringExecution的要求是尽量运行在磁盘类型为ssd(disk-type In ssd)的节点上. 

代码如下: 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:               # 1
    nodeAffinity:         # 2
      requiredDuringSchedulingIgnoredDuringExecution:   # 3
        nodeSelectorTerms:
        - matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
            - amd64
      preferredDuringSchedulingIgnoredDuringExecution:  # 4
      - weight: 1
        preference:
          matchExpressions:
          - key: disk-type
            operator: In
            values:
            - ssd
  containers:
  - name: with-node-affinity
    image: gcr.io/google_containers/pause:2.0
```

从上面的配置中可以看到In操作符, NodeAffinity语法支持的操作符包括In、NotIn、Exists、DoesNotExist、Gt、Lt. 虽然没有节点排斥功能, 但是用NotIn和DoesNotExist就可以实现排斥的功能了. 

NodeAffinity规则设置的注意事项如下. 

* 如果同时定义了nodeSelector和nodeAffinity, 那么必须两个条件都得到满足, Pod才能最终运行在指定的Node上. 

* 如果nodeAffinity指定了多个nodeSelectorTerms, 那么其中一个能够匹配成功即可. 

* 如果在nodeSelectorTerms中有多个matchExpressions, 则一个节点必须满足所有matchExpressions才能运行该Pod. 

# 4 PodAffinity: Pod亲和与互斥调度策略

Pod间的亲和与互斥从Kubernetes 1.4版本开始引入. 这一功能让用户从另一个角度来限制Pod所能运行的节点: 根据在节点上正在运行的Pod的标签而不是节点的标签进行判断和调度, 要求对节点和Pod两个条件进行匹配. 这种规则可以描述为: 如果在具有标签X的Node上运行了一个或者多个符合条件Y的Pod, 那么Pod应该(如果是互斥的情况, 那么就变成拒绝)运行在这个Node上. 