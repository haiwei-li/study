
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->



<!-- /code_chunk_output -->


我们在前面已经了解到, 为了支持**集群**的**水平扩展**、**高可用性**, Kubernetes抽象出了Service的概念. **Service**是**对一组Pod的抽象**, 它会根据**访问策略**(如负载均衡策略)来**访问这组Pod**. 

Kubernetes在**创建服务**时会**为服务分配一个虚拟的IP地址**, **客户端**通过访问**这个虚拟的IP地址**来**访问服务**, 服务则负责将请求**转发到后端的Pod**上. 这不就是一个反向代理吗?没错, 这就是**一个反向代理**. 但是, 它和普通的反向代理有一些不同: 首先, 它的**IP地址是虚拟的**, 想**从外面访问还需要一些技巧**; 其次, 它的**部署和启停**是由Kubernetes**统一自动管理**的. 

在很多情况下, **Service只是一个概念**, 而真正**将Service的作用落实**的是它背后的**kube\-proxy服务进程**. 只有理解了kube\-proxy的原理和机制, 我们才能真正理解Service背后的实现逻辑. 

在Kubernetes集群的每个Node上都会运行一个**kube\-proxy服务进程**, 我们可以把**这个进程**看作**Service**的**透明代理**兼**负载均衡器**, 其**核心功能**是将到**某个Service的访问请求**转发到**后端的多个Pod实例**上. 

此外, **Service的Cluster IP！！！** 与**NodePort！！！** 等概念是**kube\-proxy服务！！！**通过**iptables！！！** 的**NAT转换！！！** 实现的, **kube\-proxy**在运行过程中**动态创建**与Service相关的**iptables规则**, **这些规则**实现了将**访问服务(Cluster IP或NodePort)的请求**负载分发到**后端Pod**的功能. 

由于**iptables机制**针对的是**本地的kube\-proxy端口**, 所以在**每个Node**上都要**运行kube\-proxy组件**, 这样一来, 在**Kubernetes集群内部**, 我们可以在**任意Node**上发起**对Service的访问请求**. 综上所述, 由于kube\-proxy的作用, 在Service的调用过程中客户端无须关心后端有几个Pod, 中间过程的通信、负载均衡及故障恢复都是透明的. 

**起初**, kube\-proxy进程是一个**真实的TCP/UDP代理**, 类似HA Proxy, 负责从Service到Pod的**访问流量的转发**, 这种模式被称为**userspace(用户空间代理)模式**. 如图5.13所示, 当**某个Pod**以**Cluster IP**方式**访问某个Service**的时候, 这个流量会被**Pod所在本机！！！的iptables**转发到**本机的kube\-proxy进程**, 然后由**kube\-proxy**建立起**到后端Pod的TCP/UDP连接！！！**, 随后将请求转发到某个后端Pod上, 并在这个过程中实现负载均衡功能. 

图5.13　Service的负载均衡转发规则:

![2019\-09\-01\-21\-20\-38.png](./images/2019\-09\-01\-21\-20\-38.png)

关于Cluster IP与Node Port的实现原理, 以及kube\-proxy与API Server的交互过程, 图5.14给出了较为详细的说明, 由于这是**最古老的kube\-proxy的实现方式**, 所以不再赘述. 

图5.14　kube\-proxy工作原理示意:

![2019\-09\-01\-21\-21\-36.png](./images/2019\-09\-01\-21\-21\-36.png)

如图5.15所示, Kubernetes从**1.2版本**开始, 将**iptables**作为**kube\-proxy**的**默认模式**. 

**iptables模式**下的kube\-proxy**不再起到Proxy！！！** 的作用, 其**核心功能**: 通过**API Server**的**Watch接口**实时跟踪**Service**与**Endpoint**的**变更信息**, 并**更新对应的iptables规则**, Client的请求流量则通过**iptables的NAT机制！！！** "直接路由"到**目标Pod**. 

图5.15　应用程序编程访问API Server:

![2019\-09\-01\-21\-22\-06.png](./images/2019\-09\-01\-21\-22\-06.png)

根据Kubernetes的网络模型, 一个Node上的Pod与其他Node上的Pod应该能够直接建立双向的TCP/IP通信通道, 所以如果直接修改iptables规则, 则也可以实现kube\-proxy的功能, 只不过后者更加高端, 因为是全自动模式的. 与第1代的userspace模式相比, iptables模式完全工作在内核态, 不用再经过用户态的kube\-proxy中转, 因而性能更强. 

iptables模式虽然实现起来简单, 但存在无法避免的缺陷: 在集群中的Service和Pod大量增加以后, iptables中的规则会急速膨胀, 导致性能显著下降, 在某些极端情况下甚至会出现规则丢失的情况, 并且这种故障难以重现与排查, 于是Kubernetes从1.8版本开始引入第3代的IPVS(IP Virtual Server)模式, 如图5.16所示. IPVS在Kubernetes 1.11中升级为GA稳定版. 

图5.16　应用程序编程访问API Server:

![2019\-09\-01\-21\-24\-15.png](./images/2019\-09\-01\-21\-24\-15.png)

iptables与IPVS虽然都是基于Netfilter实现的, 但因为定位不同, 二者有着本质的差别: iptables是为防火墙而设计的; IPVS则专门用于高性能负载均衡, 并使用更高效的数据结构(Hash表), 允许几乎无限的规模扩张, 因此被kube\-proxy采纳为第三代模式. 

与iptables相比, IPVS拥有以下明显优势: 

- 为大型集群提供了更好的可扩展性和性能; 
- 支持比iptables更复杂的复制均衡算法(最小负载、最少连接、加权等); 
- 支持服务器健康检查和连接重试等功能; 
- 可以动态修改ipset的集合, 即使iptables的规则正在使用这个集合. 

由于IPVS无法提供包过滤、airpin\-masquerade tricks(地址伪装)、SNAT等功能, 因此在某些场景(如NodePort的实现)下还要与iptables搭配使用. 在IPVS模式下, kube\-proxy又做了重要的升级, 即使用iptables的扩展ipset, 而不是直接调用iptables来生成规则链. 

iptables规则链是一个线性的数据结构, ipset则引入了带索引的数据结构, 因此当规则很多时, 也可以很高效地查找和匹配. 我们可以将ipset简单理解为一个IP(段)的集合, 这个集合的内容可以是IP地址、IP网段、端口等, iptables可以直接添加规则对这个"可变的集合"进行操作, 这样做的好处在于可以大大减少iptables规则的数量, 从而减少性能损耗. 

假设要禁止上万个IP访问我们的服务器, 则用iptables的话, 就需要一条一条地添加规则, 会在iptables中生成大量的规则; 但是用ipset的话, 只需将相关的IP地址(网段)加入ipset集合中即可, 这样只需设置少量的iptables规则即可实现目标. 

kube\-proxy针对Service和Pod创建的一些主要的iptables规则如下. 

- KUBE\-CLUSTER\-IP: 在masquerade\-all=true或clusterCIDR指定的情况下对Service Cluster IP地址进行伪装, 以解决数据包欺骗问题. 
- KUBE\-EXTERNAL\-IP: 将数据包伪装成Service的外部IP地址. 
- KUBE\-LOAD\-BALANCER、KUBE\-LOAD\-BALANCER\-LOCAL: 伪装Load Balancer 类型的Service流量. 
- KUBE\-NODE\-PORT\-TCP、KUBE\-NODE\-PORT\-LOCAL\-TCP、KUBE\-NODE\-PORTUDP、KUBE\-NODE\-PORT\-LOCAL\-UDP: 伪装NodePort类型的Service流量. 
